[["index.html", "Matrix Algebra for Educational Scientists Foreword Acknowledgments Colophon", " Matrix Algebra for Educational Scientists Michael Rodriguez &amp; Andrew Zieffler 2021-05-27 Foreword Spring 2021. The contents of this book constitute information from several set of notes from a variety of QME courses, including from an old course called EPsy 8269. We are making this book available as a resource for anyone who wants to use it. We will be adding and revising the content for awhile. Feel free to offer criticism, suggestion, and feedback. You can either open an issue on the book’s github page or send us an email directly. Andrew Zieffler &amp; Michael Rodriguez zief0002@umn.edu Acknowledgments Many thanks to James Terwilliger, whose initial notes gave rise to some of this material. Also, thank you to all the students in our courses who have been through previous iterations of this material. Your feedback has been invaluable, and you are the world’s greatest copy editors. Colophon The book is typeset using Karla for the body font, Lora for the headings and Sue Ellen Francisco for the title. The color palette was generated using coolors.co. Icon and note ideas and prototypes by Desirée De Leon. Some of the book style and CSS code were inspired by: De Leon, D., &amp; Hill, A. (2019). A handbook for teaching and learning with R and RStudio. "],["intro.html", "Chapter 1 Introduction 1.1 Prerequisites", " Chapter 1 Introduction Having a basic understanding of the vocabulary, notation, and ideas of matrix algebra, is important for all educational scientists who use quantitative methods in their work. The statistical and psychometric models underlying many quantitative methodologies employed in educational research rely on matrix algebra. Subsequently, educational scientists use the language and notation of matrix algebra to communicate in the scientific literature. Moreover, matrix algebra forms the bedrock of statistical computation. Having fundamental knowledge of matrix algebra can often help an educational scientist troubleshoot problems that arise in their own work, and devise solutions for those issues. For quantitative methodologists, it is important to have a much deeper understanding of matrix algebra, as it is foundational to the computational estimation and optimization used in methodological work. Statistical programming, formulating the mathematics of quantitative methods, and even back-of-the-napkin calculations are all made easier (and more efficient) through matrix algebra. 1.1 Prerequisites ADD PREREQUISITES "],["datastructures.html", "Chapter 2 Data Structures 2.1 Scalars 2.2 Vectors 2.3 Matrices 2.4 Tensors 2.5 A Word about Notation 2.6 Exercises", " Chapter 2 Data Structures In this chapter you will be introduced to four common data structures that form the building blocks of matrix algebra: scalars, vectors, matrices, and tensors. You will also be introduced to some of the vocabulary that we use to describe these structures. In future chapters, we will examine these structures in more detail and learn how to mathematically manipulate and operate on these structures. 2.1 Scalars A scalar is a single real number. You have likely had a lot of previous experience with scalars, as they are emphasized in much of the mathematics taught in high schools in the United States. Here are three examples of scalars: \\[ 1 \\qquad \\sqrt{2} \\qquad -7 \\] Scalar arithmetic is the arithmetic operations (addition, subtraction, multiplication, and division) we perform using real numbers. For example, \\[ \\begin{split} 2 + 3 = 5 \\\\[1ex] 21 \\div 3 = 7 \\end{split} \\] Notice that scalar arithmetic also produces a scalar. For example the scalar addition in the example, \\(2+3\\) produces the scalar \\(5\\). 2.2 Vectors A vector is a specifically ordered one-dimensional array of values. Here are three examples of vectors: \\[ \\begin{bmatrix} -1 \\\\ 3 \\\\ \\sqrt{7} \\\\2 \\end{bmatrix} \\qquad \\begin{bmatrix} 5 &amp; 4 \\end{bmatrix} \\qquad \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{bmatrix} \\] A vector may be written as a row or a column, and are respectively referred to as row vectors or column vectors. Each value in a vector is called an element or component. Vectors are also typically described in terms of the number of elements they have. For example, the following is a two-element row vector: \\[ \\begin{bmatrix} 5 &amp; 4 \\end{bmatrix} \\] Here, a is a four-element column vector: \\[ \\mathbf{a} = \\begin{bmatrix} 5 \\\\ 4 \\\\ 7 \\\\2 \\end{bmatrix} \\] 2.3 Matrices A matrix is specifically ordered two-dimensional array of values. Here are three examples of matrices: \\[ \\begin{bmatrix} -1 &amp; 5\\\\ 3 &amp; 2 \\\\ \\sqrt{7} &amp; -4 \\\\2 &amp; 2 \\end{bmatrix} \\qquad \\begin{bmatrix} 5 &amp; 4 \\\\ 1 &amp; 6 \\end{bmatrix} \\qquad \\begin{bmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\\\ a_{3,1} &amp; a_{32} \\end{bmatrix} \\] Matrices have both rows and columns, and are typically described by the number of rows and columns they have. For example, the matrix B (below) has 3 rows and 2 columns: \\[ \\underset{3\\times 2}{\\mathbf{B}} = \\begin{bmatrix} 5 &amp; 1 \\\\ 7 &amp; 3 \\\\ -2 &amp; -1 \\end{bmatrix} \\] We say that B is a “3 by 2” matrix. The number of rows and columns are referred to as the dimensions or order of the matrix, and, for matrix B, is denoted as \\(3\\times 2\\). The dimension is often appended to the bottom of the matrix (e.g., \\(\\underset{3\\times 2}{\\mathbf{B}}\\)). The elements within the matrix are indexed by their row number and column number, respectively. For example, \\(\\mathbf{B}_{1,2} = 1\\) since the element in the first row and second column is 1. The subscripts on each element indicate the row and column positions of the element.1 More generally, we define matrix A, which has n rows and k columns as: \\[ \\underset{n\\times k}{\\mathbf{A}} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\ldots &amp; a_{1k} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\ldots &amp; a_{2k} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; \\ldots &amp; a_{3k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\ldots &amp; a_{nk} \\end{bmatrix} \\] where element \\(a_{ij}\\) is in the \\(i^{\\mathrm{th}}\\) row and \\(j^{\\mathrm{th}}\\) column of A. 2.4 Tensors Tensors, generally speaking, are the generalization of the matrix to three or more dimensions. Here is an example of a tensor: \\[ \\begin{bmatrix} \\begin{bmatrix} 5 &amp; 4 \\end{bmatrix} &amp; \\begin{bmatrix} 1 &amp; 6 \\end{bmatrix} \\\\ \\begin{bmatrix} 2 &amp; 3 \\end{bmatrix} &amp; \\begin{bmatrix} 0 &amp; 7 \\end{bmatrix} \\end{bmatrix} \\] Technically, this definition is not quite true, but for our purposes it will be adequate to think of a tensor as a structure for data in N dimensions. This book will primarily deal with scalars, vectors, and matrices, but tensors do come up in statistical work. For example, image data are often represented as tensors; with each pixel in a two-dimensional image having multiple values associated with it to represent the color information for the pixel. In longitudinal data analysis, the variance–covariance matrix of the responses for multiple subjects is also represented as a tensor. Bi et al. (2021) and McCullagh (2018) are good resources to learn more about working with and operating on tensors. 2.5 A Word about Notation Authors and textbooks use a wide variety of notation to represent scalars, vectors, and matrices. For example, vectors might be denoted using a lower-case underlined letter (\\(\\underline{a}\\)), a lower-case bolded letter (a), or a lower-case letter with an overset arrow (\\(\\vec{a}\\)). In this book, we will try to use a consistent notation to denote each of these structures. Scalars will be denoted with an italicized lower-case letter (e.g., \\(a\\)) or a non-bolded lower-case Greek letter (e.g., \\(\\lambda\\)). Vectors will be denoted using a bold-faced lower-case letter (e.g., a. Matrices will be denoted using a bold-faced upper-case letter (e.g., A) or a bold-faced upper-case Greek letter (e.g., \\(\\boldsymbol\\Phi\\)). 2.6 Exercises Identify each of the following as a scalar, row vector, column vector, matrix, or tensor. \\((a_1 + a_2 + a_3 + a_4)\\) Show/Hide Solution Scalar \\(\\begin{bmatrix} 520 \\\\ 640 \\\\ 780\\end{bmatrix}\\) Show/Hide Solution Column vector 13 Show/Hide Solution Scalar \\(\\begin{bmatrix} 115 &amp; 129 &amp; 92 &amp; 89\\end{bmatrix}\\) Show/Hide Solution Row vector \\(\\begin{bmatrix}5 &amp; 11 \\\\ -3 &amp; 0\\end{bmatrix}\\) Show/Hide Solution Matrix \\(\\begin{bmatrix} 0 &amp; 0 &amp; 0\\end{bmatrix}\\) Show/Hide Solution Row vector How many elements are in each of the following vectors? \\(\\begin{bmatrix} 115 &amp; 129 &amp; 92 &amp; 89\\end{bmatrix}\\) Show/Hide Solution 4 elements \\(\\begin{bmatrix}5 \\\\ 11 \\\\ -3 \\end{bmatrix}\\) Show/Hide Solution 3 elements \\(\\begin{bmatrix} a_1 &amp; a_2 &amp; a_3 &amp; \\ldots &amp; a_k\\end{bmatrix}\\) Show/Hide Solution k elements What is the order (dimensions) of each of the following matrices? \\(\\begin{bmatrix}5 &amp; 11 \\\\ -3 &amp; 0 \\\\ 2 &amp; -1\\end{bmatrix}\\) Show/Hide Solution \\(3\\times2\\) \\(\\begin{bmatrix}a_{11} &amp; a_{12} &amp; a_{13} &amp; \\ldots &amp; a_{1k} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\ldots &amp; a_{2k} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; \\ldots &amp; a_{3k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\ldots &amp; a_{nk}\\end{bmatrix}\\) Show/Hide Solution \\(n\\times k\\) Many authors do not include the comma between the row and column parts of the subscript (e.g., \\(\\mathbf{B}_{1,2} = \\mathbf{B}_{12}\\)).↩︎ "],["vec.html", "Chapter 3 Vectors 3.1 What is a Vector? 3.2 General Form of a Vector 3.3 The Geometry of Vectors 3.4 Vector Properties 3.5 Vector Equality 3.6 Special Vectors 3.7 Exercises", " Chapter 3 Vectors In this chapter you will learn more about the vector data structure. You will be introduced to some of the vocabulary that we use to describe vectors. You will also be introduced to the geometry of vectors. It is important to recognize that although the illustration of vector geometry is restricted to 2-dimensional space, all of these ideas can be extended to n-dimensions. 3.1 What is a Vector? A vector is a specifically ordered one-dimensional array of values written as a row or column. Vectors are typically described by the number of elements they have. Here, for example, a is a four-element column vector: \\[ \\mathbf{a} = \\begin{bmatrix} 5 \\\\ 4 \\\\ 7 \\\\ 2 \\end{bmatrix} \\] We might also say that a is a column vector in 4-dimensions. In statistical or psychometric applications, vectors are how we typically represent and structure data collected on a particular attribute. For example, a might represent test scores for \\(n=4\\) students. Each element in this vector would correspond to a student’s test score. In this case, \\(a_1=5\\) would be the test score for the first student recorded in the vector, \\(a_2=4\\) would be the test score for the second student recorded in the vector, etc. Computationally, we can also work with vectors using R. We use the matrix() function to create a column or row vector in R. We include the elements of the vector in a c() function and provide this to the data= argument. We also include either the argument ncol=1 (column vector) or nrow=1 (row vector). Below we create vector a. We can also use the length() function to count the number of elements in a vector. # Create column vector a a = matrix(data = c(5, 4, 7, 2), ncol = 1) a [,1] [1,] 5 [2,] 4 [3,] 7 [4,] 2 # Create row vector b b = matrix(data = c(1, 0, 0), nrow = 1) b [,1] [,2] [,3] [1,] 1 0 0 # Count number of elements in a length(a) [1] 4 # Count number of elements in b length(b) [1] 3 3.1.1 Transposition One important vector operation is transposition. Transposition is an operation in which we replace the ith element of a column vector as the ith element of a row vector, and vice-versa. In other words, we are converting a column vector into a row vector, or, conversely, converting a row vector into a column vector. Notationally, we use a superscripted prime or a superscripted intercalate symbol (looks like a “T”) to denote a vector’s transpose. For example, transposing vector a, which we defined earlier: \\[ \\mathbf{a} = \\begin{bmatrix} 5 \\\\ 4 \\\\ 7 \\\\ 2 \\end{bmatrix} \\qquad \\mathbf{a}^{\\prime} = \\mathbf{a}^{\\intercal} = \\begin{bmatrix} 5 &amp; 4 &amp; 7 &amp; 2 \\end{bmatrix} \\] In R, the t() function will compute the transpose of a vector. Here we use t() to compute the transpose of the earlier defined vectors a and b. # Transpose of a t(a) [,1] [,2] [,3] [,4] [1,] 5 4 7 2 # Transpose of b t(b) [,1] [1,] 1 [2,] 0 [3,] 0 Throughout the remainder of the book, row vectors will be denoted using a transpose. So, for example, a will indicate a column vector, and \\(\\mathbf{a}^{\\intercal}\\) will indicate a row vector. 3.2 General Form of a Vector Now that we have introduced transposition, we can formalize the general notation for both column and row vectors. The general form of a column vector with n elements is: \\[ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\] While, the general form of a row vector with n elements is: \\[ \\mathbf{x}^{\\intercal} = \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 &amp; \\ldots &amp; x_n \\end{bmatrix} \\] Vectors can also be thought of as a special case of a matrix in one of the dimensions is equal to 1. For example x could be considered a \\(n \\times 1\\) matrix, and \\(\\mathbf{x}^{\\intercal}\\) could be considered a \\(1 \\times n\\) matrix. 3.3 The Geometry of Vectors A vector can be represented geometrically by using a coordinate system where each element in the vector corresponds to a distance along one of the reference axes defining the coordinate system. Consider the column vector a: \\[ \\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\] This vector has a distance of 2 on the first reference axis (R1) and a distance of 3 on the second reference axis (R2). Figure 3.1: Plot showing vector a (in red) in the R1–R2 dimensional space. In Figure 3.1 the reference coordinate system (R1–R2) define a two-dimensional space. Vector a had two elements and thus resides in a two-dimensional space. In general, an n-dimensional vector resides in an n-dimensional space. The reference axes always intersect at the origin of the space (i.e., the reference point \\((0, 0)\\)). Here the reference axes are at right angles to one another, although this is not a requirement. When the reference axes are at right angles to each other, the reference system is referred to as an orthogonal coordinate system. 3.4 Vector Properties Geometrically, a vector is displayed as a line segment with an arrowhead at one end of the segment. The end of the vector with the arrowhead is called the head or terminus, and the other end of the vector is called the tail or origin. The head of the vector indicates the direction of the vector. All vectors have a direction. They also have a length. These two properties completely define a vector within a given reference system. Note that location of the vector in the reference system is not a property of the vector. For example, the red vector and the blue vector below are identical—they have the same direction and length! Figure 3.2: Plot showing two identical vectors (in red and blue) in the R1–R2 dimensional space. They are identical because they have the same length and direction. Location in the reference system is a convenience, not a property of the vector. Location is a convenience in that we can re-locate any vector within the reference system to make it easier to work with. 3.4.1 Vector Length The length of a vector (also referred to as the vector’s magnitude or norm) is the distance from the vector’s tip to tail. In an orthogonal reference system, if we locate the tail of the vector at the origin, multiple right triangles with the vector as hypotenuse can be formed with the reference axes. The Pythagorean Theorem can then be applied to determine the length of a vector. Figure 3.3: Plot showing vector a (in red) in the R1–R2 dimensional space. This vector is the hypotenuse of a right triangle with legs of length 2 and 3. In Figure 3.3, vector a is the hypotenuse of a right triangle with legs of length 2 and 3. We can use the Pythagorean Theorem to find the length of the vector: \\[ \\begin{split} C^2 &amp;= A^2 + B^2 \\\\[2ex] &amp;= 2^2 + 3 ^2 \\\\[2ex] &amp;= 13 \\\\[2ex] C &amp;= \\sqrt{13} \\approx 3.61 \\end{split} \\] Note that the sum of the squared elements is equal to the square of the length of the vector, and to determine the length of a (denoted as \\(\\lvert\\lvert \\mathbf{a} \\rvert\\rvert\\)), we take the square root of that sum. For our two-dimensional vector, this is \\[ \\lvert\\lvert \\mathbf{a} \\rvert\\rvert = \\sqrt{a_1^2 + a_2^2} \\] We can generalize this to finding the length of a vector a with n-dimensions: \\[ \\lvert\\lvert \\mathbf{a} \\rvert\\rvert = \\sqrt{a_1^2 + a_2^2 + a_3^2 + \\ldots + a_n^2} \\] Computationally, to compute the length of a vector, we chain together several computations that (1) square the elements of in the vector, (2) sum those squares together, and (3) compute the square root of this sum. Below we create a 2-element column vector a and compute its length. # Create vector a a = matrix(data = c(2, 3), ncol = 1) a [,1] [1,] 2 [2,] 3 # Compute length of vector a sqrt(sum(a ^ 2)) [1] 3.605551 3.4.2 Vector Direction A vector’s direction is often expressed as the measure of the angle between the horizontal reference axis and the vector.2 Figure 3.4 shows this angle (denoted \\(\\theta\\)) for vector a. Figure 3.4: Plot showing vector a (in red) in the R1–R2 dimensional space. The direction of this vector is the measure of the angle (\\(\\theta\\)) between the horizontal reference axis and the vector. To determine this angle, we need to use a trigonometric function. For example, here we can use the fact that the cosine of an angle \\(\\theta\\) is defined as the length of the side of the right triangle which is adjacent to \\(\\theta\\) divided by the length of the hypotenuse of the triangle.3 \\[ \\cos (\\theta) = \\frac{\\lvert\\lvert\\mathrm{Adjacent~Side}\\rvert\\rvert}{\\lvert\\lvert\\mathrm{Hypotenuse}\\rvert\\rvert} \\] In our example, \\[ \\cos (\\theta) = \\frac{2}{\\sqrt{13}} \\] To find \\(\\theta\\), we need to calculate the arc-cosine of \\(\\frac{2}{\\sqrt{13}}\\). We can compute the arc-cosine in R using the acos() function. This function will compute the value of \\(\\theta\\) in radians. We can also convert this to degrees by multiplying by \\(\\frac{180}{\\pi}\\) (where \\(\\pi\\) is the mathematical constant approximated as 3.14). # Find theta using arccosine (radians) acos(2 / sqrt(13)) [1] 0.9827937 # Find theta using arccosine (degrees) acos(2 / sqrt(13)) * 180 / pi [1] 56.30993 We always measure the angle counter-clockwise to the reference axis. So if we measure the direction associated with the vector \\(\\begin{bmatrix}-2 \\\\ -2\\end{bmatrix}\\), the direction is more than \\(\\pi\\) radians (or \\(180^\\circ\\)). Figure 3.5: Plot showing vector (-2, -2) (in red) in the R1–R2 dimensional space. The direction of this vector is the measure of the angle (\\(\\theta\\)) between the vector and the horizontal reference axis measured in the counter-clockwise direction from the axis. There are several ways to compute this angle. Remember that when we use the trigonometric functions, we are computing the angle in the right triangle formed by the vector and the reference axes (see Figure 3.6). Figure 3.6: Plot showing vector (-2, -2) (in red) in the R1–R2 dimensional space. The direction of this vector is the measure of the angle (\\(\\theta\\)) between the vector and the horizontal reference axis measured in the counter-clockwise direction from the axis. Here we have split this angle up into two parts; \\(\\alpha\\) can be computed using a trigonometric function, and \\(\\beta = \\pi\\) radians. To find \\(\\theta\\) we need to find \\(\\alpha\\) using trigonometric functions and then add it to \\(\\beta\\). For example, \\[ \\begin{split} \\tan (\\alpha) &amp;= \\frac{\\lvert\\lvert\\mathrm{Opposite~Side}\\rvert\\rvert}{\\lvert\\lvert\\mathrm{Adjacent~Side}\\rvert\\rvert} \\\\[2ex] &amp;= \\frac{2}{2} \\\\[2ex] &amp;= 1 \\end{split} \\] Computing the arc-tangent with the atan() function we find that \\(\\alpha=0.785\\) radians. Adding this to the \\(\\beta\\) value which is \\(\\pi\\) radians, we get the vector’s direction of 3.927 radians (or \\(225^\\circ\\)). # Compute alpha (in radians) atan(1) [1] 0.7853982 # Compute theta = alpha + beta (in radians) atan(1) + pi [1] 3.926991 # Compute theta = alpha + beta (in degrees) (atan(1) + pi) * 180 / pi [1] 225 Beyond two dimensions, direction in more complicated, involving more than one angle. For example, reporting the direction of a vector in three dimensions requires two angles. One common method is to give (1) the angle between the first reference axis (e.g., x-axis) and the projection of the vector onto the plane defined by the first and second reference axes (e.g., the xy-plane), and (2) the angle between the vector and the third reference axis (e.g., the z-axis). Suffice it to say that we need additional angles to fully define the direction of a vector in higher dimensions. 3.5 Vector Equality Two vectors are said to be equal if they satisfy two conditions: Both vectors have the same dimensions (i.e., they have the same number of elements and both are row or column vectors); Corresponding elements from each vector must be equal. Consider the following vectors: \\[ \\mathbf{a} = \\begin{bmatrix} 5 \\\\ 4 \\\\ 7 \\\\ 2 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 5 \\\\ 4 \\\\ 2 \\\\ 7 \\end{bmatrix} \\qquad \\mathbf{c}^{\\intercal} = \\begin{bmatrix} 5 &amp; 4 &amp; 7 &amp; 2 \\end{bmatrix} \\] Within this set of vectors, \\(\\mathbf{a} \\neq \\mathbf{b}\\), since not all of the corresponding elements are equal. \\(\\mathbf{a} \\neq \\mathbf{c}^{\\intercal}\\), since a’s dimensions are \\(4 \\times 1\\) (column vector) and \\(\\mathbf{c}^{\\intercal}\\)’s dimensions are \\(1 \\times 4\\) (row vector). \\(\\mathbf{a} = \\mathbf{c}\\) since both vectors a and c have the same dimensions (\\(4 \\times 1\\)) and all corresponding elements are equal. Geometrically, two vectors are equal if they have the same length and direction. (Remember, location in the reference coordinate system is not a property of the vectors, so two equal vectors might be in different locations.) 3.6 Special Vectors There are several vectors that are special in that they have unique properties. 3.6.1 Zero Vector One special vector is the zero vector. A zero vector (denoted 0) is a vector in which every element is 0. For example, the following vector is a zero vector: \\[ \\mathbf{0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] The length of a zero vector is 0. (Convince yourself of this!) Furthermore, since geometrically the head and tail of a zero vector coincide (are at the same location in the reference coordinate system) they have an undefinable direction. 3.6.2 Ones Vector Another special vector is referred to as a ones vector. A ones vector is a vector in which every elements is 1. For example, the following vector 1 is a ones vector: \\[ \\mathbf{1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\] The length of a ones vector is \\(\\sqrt{n}\\). (Convince yourself of this!) The direction of a ones vector is \\(45^\\circ\\) or 0.785 radians since the each element in the vector is equal. For example, locating the tail of a 2-dimensional ones vector at the origin, it would lie on the \\(R1=R2\\) line. Figure 3.7 shows a 2-dimensional ones vector with its tail located at the origin. Figure 3.7: Plot showing the ones vector (in red) in the R1–R2 dimensional space. The \\(R1=R2\\) line is also displayed. 3.6.3 Unit Vector Consider the n-element column vector u: \\[ \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ \\vdots \\\\ u_n \\end{bmatrix} \\] u is a unit vector if the length of the vector is 1. For example, each of the following vectors are unit vectors: \\[ \\mathbf{u}_1 = \\begin{bmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{bmatrix} \\qquad \\mathbf{u}_2 = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{u}_3 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Although the length of a unit vector must be 1, the direction is not specified. Thus there are an infinite number of unit vectors. Figure 3.8 shows several unit vectors with their tail located at the origin. Figure 3.8: Plot showing several unit vectors in the R1–R2 dimensional space. All vector tails have been located at the origin. 3.6.4 Elementary Vectors An elementary vector is a vector that has one element that is equal to one and the remainder of its elements equal to 0. For example, each of the 3-element column vectors below are elementary vectors. \\[ \\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] For an n-dimensional vector there are always n elementary vectors. Moreover, every elementary vector is also a unit vector. Geometrically, each elementary vector lies on one of the reference axes. Figure 3.9 shows the two elementary vectors (with their tail located at the origin) in our 2-dimensional reference coordinate system. Figure 3.9: Plot showing the two elementary vectors in the R1–R2 dimensional space. All vector tails have been located at the origin. 3.7 Exercises Create a 2-dimensional orthogonal reference system. Add the following vectors to that system. \\(\\mathbf{x} = \\begin{bmatrix}2 \\\\ -1 \\end{bmatrix}\\) Show/Hide Solution \\(\\mathbf{y} = \\begin{bmatrix}-3 \\\\ 2 \\end{bmatrix}\\) Show/Hide Solution A unit-vector z in which all the elements are equal. Show/Hide Solution \\[ \\mathbf{z} = \\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{bmatrix} \\] All elementary vectors. Show/Hide Solution \\[ \\mathbf{e}_1 = \\begin{bmatrix} 0 \\\\ 1\\end{bmatrix} \\quad \\mathbf{e}_2 = \\begin{bmatrix} 1 \\\\ 0\\end{bmatrix} \\] Compute the following: \\(\\lvert\\lvert\\mathbf{x}\\rvert\\rvert\\) Show/Hide Solution C \\[ \\begin{split} \\lvert\\lvert\\mathbf{x}\\rvert\\rvert &amp;= \\sqrt{(2)^2 + (-1)^2} \\\\[2ex] &amp;= \\sqrt{5} \\end{split} \\] \\(\\lvert\\lvert\\mathbf{y}\\rvert\\rvert\\) Show/Hide Solution C \\[ \\begin{split} \\lvert\\lvert\\mathbf{y}\\rvert\\rvert &amp;= \\sqrt{(-3)^2 + (2)^2} \\\\[2ex] &amp;= \\sqrt{13} \\end{split} \\] \\(\\lvert\\lvert\\mathbf{z}\\rvert\\rvert\\) Show/Hide Solution C \\[ \\begin{split} \\lvert\\lvert\\mathbf{z}\\rvert\\rvert &amp;= \\sqrt{\\bigg(\\frac{1}{\\sqrt{2}}\\bigg)^2 + \\bigg(\\frac{1}{\\sqrt{2}}\\bigg)^2} \\\\[2ex] &amp;= \\sqrt{\\frac{1}{2} + \\frac{1}{2}} \\\\[2ex] &amp;= \\sqrt{1} \\\\[2ex] &amp;= 1 \\end{split} \\] Remember, a unit-vector has a length of 1. The direction based on the angle (in radians) from the horizontal reference axis for x. Show/Hide Solution C \\[ \\begin{split} \\theta &amp;= 2\\pi - \\arccos(\\frac{2}{\\sqrt{5}}) \\\\[2ex] &amp;= 5.8195 \\mathrm{~radians} \\end{split} \\] The direction based on the angle (in degrees) from the horizontal reference axis for y. Show/Hide Solution C \\[ \\begin{split} \\theta &amp;= \\bigg(\\frac{\\pi}{2} + \\arctan(\\frac{3}{2})\\bigg) \\times \\frac{180}{\\pi} \\\\[2ex] &amp;= 146.31 \\mathrm{~degrees} \\end{split} \\] The direction based on the angle (in radians) from the horizontal reference axis for z. Show/Hide Solution C \\[ \\begin{split} \\theta &amp;= \\arctan\\bigg(\\frac{\\frac{1}{\\sqrt{2}}}{\\frac{1}{\\sqrt{2}}}\\bigg) \\\\[2ex] &amp;= \\arctan(1) \\\\[2ex] &amp;= 0.786 \\mathrm{~radians} \\end{split} \\] Indicate whether the vectors are equal or not equal. \\(\\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix} \\overset{?}{=} \\begin{bmatrix} 6 \\\\ 5 \\end{bmatrix}\\) Show/Hide Solution Not equal \\(\\begin{bmatrix} 55 \\\\ 66 \\\\ 48 \\end{bmatrix} \\overset{?}{=} \\begin{bmatrix} 55 &amp; 66 &amp; 48 \\end{bmatrix}\\) Show/Hide Solution Not equal Assume a and b are equal. Find the values for \\(a_1\\), \\(a_3\\), and \\(b_2\\). \\(\\mathbf{a} = \\begin{bmatrix} a_1 \\\\ 0 \\\\ a_3 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 9 \\\\ b_2 \\\\ 1 \\end{bmatrix}\\) Show/Hide Solution \\(a_1=9\\), \\(a_3=1\\), and \\(b_2=0\\) While it is common to express the direction as the measure of the angle between the horizontal reference axis, it could also be expressed as the measure of the angle between the vector and any of the reference axes. In more than two-dimensions, the direction would need to include multiple angles to specify the direction; you would need \\(n-1\\) angles to specify an n-dimensional vector.↩︎ You could also use the definition for sine or tangent to compute \\(\\theta\\).↩︎ "],["vecop.html", "Chapter 4 Vector Operations 4.1 Vector Addition and Subtraction 4.2 Vector–Scalar Multiplication 4.3 Vector–Vector Multiplication: Dot Product 4.4 Vector Operations Using R 4.5 Exercises", " Chapter 4 Vector Operations In this chapter you will learn about some of the common arithmetic operations (addition, subtraction, and multiplication) that can be performed with vectors. As in the previous chapter, you will also be introduced to the geometry of these vector operations in 2-dimensional space. 4.1 Vector Addition and Subtraction Vectors can be added together if they have the same dimensions (i.e., they have the same number of elements and are both row or column vectors). To add two vectors, each of length n, together, we sum the corresponding elements in the vectors. For example, consider the column vectors a and b, where, \\[ \\mathbf{a} = \\begin{bmatrix}5 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} \\] Since both vectors have dimensions \\(2 \\times 1\\), we can compute the sum of these vectors as: \\[ \\begin{split} \\mathbf{a} + \\mathbf{b} &amp;= \\begin{bmatrix}5 \\\\ 2\\end{bmatrix} + \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix}5 + 1 \\\\ 2 + 3\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix}6 \\\\ 5\\end{bmatrix} \\end{split} \\] Note that the resulting vector is a column vector with two elements, the same as the vectors we summed together. In general, if we sum two n-dimensional column vectors, x and y, the resulting vector will also be an n-dimensional column vector: \\[ \\begin{split} \\mathbf{x} + \\mathbf{y} &amp;= \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\\end{bmatrix} + \\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix}x_1 + y_1 \\\\ x_2 + y_2 \\\\ x_3 + y_3 \\\\ \\vdots \\\\ x_n + y_n\\end{bmatrix} \\end{split} \\] This process is similar for summing two n-dimensional row vectors, except the resulting vector will be an n-dimensional row vector. 4.1.1 Geometry of Adding Vectors Geometrically, adding two vectors, say a and b, is equivalent to drawing vector b so that its tail is placed at the head of vector a. Then the sum is the new vector that originates at vector a’s tail and terminates at vector b’s head. Figure 4.1 geometrically shows the sum of the following vectors: \\[ \\mathbf{a} = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} \\qquad \\mathbf{a}+\\mathbf{b} = \\begin{bmatrix} 6 \\\\ 5 \\end{bmatrix} \\] Figure 4.1: Plot showing the sum of vector a (in red) and vector b (in blue) in the R1–R2 dimensional space. For convenience we have located the tail of vector a at the origin. The vector that corresponds to the sum of a and b is shown in purple. 4.1.2 Vector Subtraction To subtract vector b from vector a, we subtract the corresponding elements in vector b from those in vector a. For example, working with our previously defined vectors, \\[ \\begin{split} \\mathbf{a} - \\mathbf{b} &amp; = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} 5-1 \\\\ 2 - 3 \\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} 4 \\\\ -1 \\end{bmatrix} \\end{split} \\] Since subtraction is equivalent to adding the inverse, subtracting the elements of b from a is equivalent to adding the inverted elements of b to the elements of a (where “inverting the elements” means switching the sign on each element). For example, \\[ \\begin{split} \\mathbf{a} - \\mathbf{b} &amp; = \\mathbf{a} + -\\mathbf{b} \\\\[2ex] &amp;= \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} -1 \\\\ -3 \\end{bmatrix} \\end{split} \\] Figure 4.2 geometrically shows the operation of vector subtraction by adding the inverse of b to a. Figure 4.2: Plot showing the sum of vector a (in red) and the inverted vector b (in blue) in the R1–R2 dimensional space. For convenience we have located the tail of vector a at the origin. The vector that corresponds to the difference of a and b is shown in purple. 4.1.3 Properties of Vector Addition and Subtraction In vector addition, since each corresponding element is added, vector addition satisfies both the commutative and associative properties. That is, \\[ \\mathbf{a} + \\mathbf{b} = \\mathbf{b} + \\mathbf{a} \\] and \\[ \\begin{split} \\mathbf{a} + (\\mathbf{b} + \\mathbf{c}) &amp;= (\\mathbf{a} + \\mathbf{b}) + \\mathbf{c} \\\\[2ex] &amp;= \\mathbf{a} + \\mathbf{b} + \\mathbf{c} \\end{split} \\] Convince yourself these two properties are satisfied using the following vectors. \\[ \\mathbf{a} = \\begin{bmatrix}5 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} \\qquad \\mathbf{c} = \\begin{bmatrix}2 \\\\ 0\\end{bmatrix} \\] Since we can re-write vector subtraction as vector addition, these same conditions and properties also apply for vector subtraction. 4.2 Vector–Scalar Multiplication When a vector is multiplied by a scalar, each element of the vector is multiplied by the value of the scalar. Consider the following vector a and scalar \\(\\lambda\\), \\[ \\mathbf{a} = \\begin{bmatrix}2 \\\\ 3 \\\\ 0 \\\\ -1\\end{bmatrix} \\qquad \\lambda=3 \\] Multiplying a by \\(\\lambda\\) gives: \\[ \\begin{split} \\lambda\\mathbf{a} &amp;= 3\\begin{bmatrix}2 \\\\ 3 \\\\ 0 \\\\ -1\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix}3 \\times 2 \\\\ 3 \\times 3 \\\\ 3 \\times 0 \\\\ 3 \\times -1\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix}6 \\\\ 9 \\\\ 0 \\\\ -3\\end{bmatrix} \\end{split} \\] This same method applies to row vectors. Scalars may be fractions, negative numbers, or unknowns. For example, if the scalar \\(\\gamma\\) is an unknown scalar, then \\[ \\gamma\\mathbf{a} = \\begin{bmatrix}2\\gamma \\\\ 3\\gamma \\\\ 0 \\\\ -1\\gamma\\end{bmatrix} \\] 4.2.1 Division by a Scalar Technically, division is undefined for vectors. However, if we are dividing by a scalar, we can multiply the vector by the scalar’s reciprocal. For example, using our previously defined vector a and scalar \\(\\lambda\\), \\[ \\mathbf{a} \\div \\lambda \\quad \\mathrm{does~not~exist} \\] But, we can multiply by the reciprocal of \\(\\lambda\\), namely \\(\\dfrac{1}{\\lambda}\\). For example, using the values from our previous example: \\[ \\begin{split} \\mathbf{a} = \\begin{bmatrix}2 \\\\ 3 \\\\ 0 \\\\ -1\\end{bmatrix} \\quad &amp;\\mathrm{and} \\quad \\lambda = 3 \\\\[2ex] \\frac{1}{\\lambda}\\mathbf{a} &amp;= \\frac{1}{3}\\begin{bmatrix}2 \\\\ 3 \\\\ 0 \\\\ -1\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} \\frac{2}{3} \\\\ 1 \\\\ 0 \\\\ -\\frac{1}{3}\\end{bmatrix} \\end{split} \\] 4.2.2 Geometry of Vector–Scalar Multiplication Consider the product \\(\\lambda \\mathbf{a}\\) where, \\[ \\lambda = 2 \\quad \\mathrm{and} \\quad \\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\] The product is \\[ \\begin{bmatrix} 4 \\\\ 6 \\end{bmatrix} \\] To show what happens geometrically when we multiply a vector by a scalar, let’s examine a plot of the original vector and the product. Figure 4.3: LEFT: Plot showing vector a in the R1–R2 dimensional space. RIGHT: Plot showing vector 2(a) in the R1–R2 dimensional space. For convenience we have located the tail of both vectors at the origin. Multiplying vector a by 2 doubled its length. However the direction of the new vector is the same as the original. In general, multiplying a vector by a positive scalar changes the length of a vector but not the direction. If it is multiplied by a negative scalar, not only does the length of the resulting vector change, but its direction is \\(180^\\circ\\) from the original. For example, Figure 4.4 shows vector a and the vector \\(\\lambda \\mathbf{a}\\) where \\(\\lambda = -0.5\\). Figure 4.4: LEFT: Plot showing vector a in the R1–R2 dimensional space. RIGHT: Plot showing vector -0.5(a) in the R1–R2 dimensional space. For convenience we have located the tail of both vectors at the origin. The product vector is half the length of the original and is pointed in the complete opposite direction. 4.3 Vector–Vector Multiplication: Dot Product For two column vectors, a and b each having n elements, the dot product (i.e., scalar product) is defined as: \\[ \\mathbf{a} \\bullet \\mathbf{b} = \\sum_{i=1}^n a_ib_i \\] In other words, the dot product is calculated by multiplying together the corresponding elements of each vector, and summing those products. Consider the vectors, \\[ \\mathbf{a} = \\begin{bmatrix}5 \\\\ 4 \\\\ 7 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}1 \\\\ 0 \\\\ -1 \\\\ 2\\end{bmatrix} \\] The dot product, or \\(\\mathbf{a} \\bullet \\mathbf{b}\\), is calculated as: \\[ \\begin{split} \\mathbf{a} \\bullet \\mathbf{b} &amp;= 5(1) + 4(0) + 7(-1) + 2(2) \\\\[2ex] &amp;= 2 \\end{split} \\] Remember that the result of a dot product is a scalar. 4.3.1 Re-visiting Vector Length Recall that to find the length of a vector a with n-dimensions: \\[ \\lvert\\lvert \\mathbf{a} \\rvert\\rvert = \\sqrt{a_1^2 + a_2^2 + a_3^2 + \\ldots + a_n^2} \\] The sum under the square root is equivalent to computing the dot product of a with itself (i.e., \\(\\mathbf{a} \\bullet \\mathbf{a}\\)). Therefore, the length of an n-dimensional vector a can be found by computing the square root of the dot product between a and itself: \\[ \\lvert\\lvert \\mathbf{a} \\rvert\\rvert = \\sqrt{\\mathbf{a} \\bullet \\mathbf{a}} \\] 4.3.2 Dot Products Using the Special Vectors In the previous chapter we introduced several special vectors, including zero vectors, ones vectors, and elementary vectors. It is useful to explore what happens when one of these special vectors is used to calculate an dot product. Consider finding the dot product between an n-dimensional vector a and an n-dimensional zero vector 0: \\[ \\begin{split} \\mathbf{a} \\bullet \\mathbf{0} &amp;= \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\bullet \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\\\[2ex] &amp;= a_1(0) + a_2(0) + a_3(0) + \\ldots + a_n(0) \\\\[2ex] &amp;= 0 \\end{split} \\] The dot product is 0. Now, consider finding the dot product between an n-dimensional vector a and an n-dimensional ones vector 1: \\[ \\begin{split} \\mathbf{a} \\bullet \\mathbf{1} &amp;= \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\bullet \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\\\[2ex] &amp;= a_1(1) + a_2(1) + a_3(1) + \\ldots + a_n(1) \\\\[2ex] &amp;= \\sum_{i=1}^n a_i \\end{split} \\] Notice that the dot product here is simply the sum of the elements in a. Because of this property, ones vectors are also sometimes referred to as sum vectors. Next, consider finding the dot product between an n-dimensional vector a and an n-dimensional elementary vector, say \\(\\mathbf{e}_1\\) in which the first element is 1 and the rest are 0: \\[ \\begin{split} \\mathbf{a} \\bullet \\mathbf{e}_1 &amp;= \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\bullet \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\\\[2ex] &amp;= a_1(1) + a_2(0) + a_3(0) + \\ldots + a_n(0) \\\\[2ex] &amp;= a_1 \\end{split} \\] Notice that the dot product here is simply the same as the first element in a. In general, the dot product between an n-element column vector, a, and an n-element elementary vector, \\(\\mathbf{e}_i\\) with element \\(i=1\\), is: \\[ \\mathbf{a} \\bullet \\mathbf{e}_i = a_i \\] 4.4 Vector Operations Using R We can also carry out these vector operations using R. Below we show how to carry out the operations of vector addition and vector subtraction. # Create vectors a = matrix(data = c(5, 2), ncol = 1) a [,1] [1,] 5 [2,] 2 b = matrix(data = c(1, 3), ncol = 1) b [,1] [1,] 1 [2,] 3 # Vector addition a + b [,1] [1,] 6 [2,] 5 # Vector subtraction a - b [,1] [1,] 4 [2,] -1 Multiplication of a vector by a scalar is carried out using the * operator. This operator carries out element-wise multiplication; in this case it multiplies each element in the vector by the given scalar. # Create vectors a = matrix(data = c(2, 3, 0, -1), ncol = 1) a [,1] [1,] 2 [2,] 3 [3,] 0 [4,] -1 # Multiplication by 3 3 * a [,1] [1,] 6 [2,] 9 [3,] 0 [4,] -3 # Division by 3 (this multiplies each element by the reciprical) a / 3 [,1] [1,] 0.6666667 [2,] 1.0000000 [3,] 0.0000000 [4,] -0.3333333 Lastly, although there is no dot product operator, we can mimic this function by using the elemement-wise multiplication operator to find the product of the corresponding elements of two vectors, and then use the sum() function to add those products together. # Create vectors a = matrix(data = c(5, 4, 7, 2), ncol = 1) a [,1] [1,] 5 [2,] 4 [3,] 7 [4,] 2 b = matrix(data = c(1, 0, -1, 2), ncol = 1) b [,1] [1,] 1 [2,] 0 [3,] -1 [4,] 2 # Element-wise multiplication a * b [,1] [1,] 5 [2,] 0 [3,] -7 [4,] 4 # Compute dot product sum(a * b) [1] 2 # Compute length of a using dot product sqrt(sum(a * a)) [1] 9.69536 4.5 Exercises Consider the following vectors: \\[ \\mathbf{a} = \\begin{bmatrix}1 \\\\ 2 \\\\ 4 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}2 \\\\ 1 \\\\ -1\\end{bmatrix} \\qquad \\mathbf{c} = \\begin{bmatrix}5 \\\\ 2 \\\\ 3 \\end{bmatrix} \\qquad \\mathbf{d} = \\begin{bmatrix}2 \\\\ 1 \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{e} = \\begin{bmatrix}1 \\\\ 3 \\\\ 2 \\end{bmatrix} \\] Find \\(\\mathbf{a}\\bullet\\mathbf{b}\\). Show/Hide Solution \\[ 1(2) + 2(1) + 4(-1) = 0 \\]$ Find \\(\\mathbf{c}\\bullet\\mathbf{c}\\). Show/Hide Solution \\[ 5(5) + 2(2) + 3(3) = 38 \\] Create a 2-dimensional orthogonal reference system. Show the following vector operations on that system. \\(\\mathbf{x}+\\mathbf{y}\\) where \\(\\mathbf{x} = \\begin{bmatrix}2 \\\\ -1 \\end{bmatrix}\\) and \\(\\mathbf{y} = \\begin{bmatrix}-3 \\\\ 2 \\end{bmatrix}\\) Show/Hide Solution \\[ \\begin{split} \\mathbf{x} + \\mathbf{y} &amp;= \\begin{bmatrix} 2 \\\\ -1\\end{bmatrix} + \\begin{bmatrix} -3 \\\\ 2\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} -1 \\\\ 1\\end{bmatrix} \\end{split} \\] \\(3\\mathbf{z}\\) where z is a unit vector in which all the elements are equal. Show/Hide Solution Students’ scores on two exams ranged from 32 to 98 out of a possible 100 points on both exams. The scores for six students are shown below for Exam 1 (x) and Exam 2 (y). \\[ \\mathbf{x} = \\begin{bmatrix}56 \\\\ 64 \\\\ 32 \\\\ 88 \\\\ 90 \\\\ 79\\end{bmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix}50 \\\\ 69 \\\\ 51 \\\\ 98 \\\\ 87 \\\\ 70\\end{bmatrix} \\] Determine the total score for each student by finding \\(\\mathbf{z} = \\mathbf{x} + \\mathbf{y}\\). Show/Hide Solution \\[ \\mathbf{z} = \\begin{bmatrix}56 + 50 \\\\ 64 + 69 \\\\ 32 + 51 \\\\ 88 + 98 \\\\ 90 + 87 \\\\ 79 + 70\\end{bmatrix} = \\begin{bmatrix}106 \\\\ 133 \\\\ 83 \\\\ 186 \\\\ 177 \\\\ 149\\end{bmatrix} \\] Determine the mean score for each student by finding \\(\\frac{1}{2} \\mathbf{z}\\). Show/Hide Solution \\[ \\frac{1}{2}\\mathbf{z} = \\frac{1}{2}\\begin{bmatrix}106 \\\\ 133 \\\\ 83 \\\\ 186 \\\\ 177 \\\\ 149\\end{bmatrix}= \\begin{bmatrix}53.0 \\\\ 66.5 \\\\ 41.5 \\\\ 93.0 \\\\ 88.5 \\\\ 74.5\\end{bmatrix} \\] If the students took a third exam, and the scores for the same six students were presented in vector w, how would you write the algebraic expression (using vector notation) to obtain the students’ mean scores for all three exams? Show/Hide Solution \\[ \\frac{1}{3}(\\mathbf{x} + \\mathbf{y} + \\mathbf{w}) \\] If the mean score on Exam 3 was 60, write the algebraic expression (using vector notation) to find the six students’ mean deviation scores on Exam 3. Show/Hide Solution \\[ \\mathbf{w} - \\begin{bmatrix}60 \\\\ 60 \\\\ 60 \\\\ 60 \\\\ 60 \\\\ 60\\end{bmatrix} \\] "],["vector-geometry-angles-projection-and-decomposition.html", "Chapter 5 Vector Geometry: Angles, Projection, and Decomposition 5.1 Angle Between Vectors 5.2 Orthogonal Projection 5.3 Orthogonal Decomposition", " Chapter 5 Vector Geometry: Angles, Projection, and Decomposition In this chapter you will learn some about some additional ideas in the geometry of vectors. Again, while the illustration of these concepts is restricted to 2-dimensional space, all of these ideas can be extended to n-dimensions. 5.1 Angle Between Vectors It can be quite useful to determine the angle between two vectors. For example, what is the angle between vector a and b where, \\[ \\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\quad \\mathrm{and} \\quad \\mathbf{b}= \\begin{bmatrix} -4 \\\\ 1 \\end{bmatrix} \\] Figure 5.1 shows both vectors displayed in the same two-dimensional reference coordinate system. Figure 5.1: Plot showing two vector a (in red) and b (in blue) in the R1–R2 dimensional space. The angle between them is denoted as \\(\\theta\\). For convenience we have located the tail of both vectors at the origin. The angle between these vectors is denoted as \\(\\theta\\), and can be found using the following: \\[ \\cos (\\theta) = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\] That is, the cosine of the angle between the two vectors is equal to the dot product of the vectors divided by the product of their lengths. To find the angle (\\(\\theta\\)), we can compute the arc-cosine of this ratio. In our example, \\[ \\begin{split} \\cos (\\theta) &amp;= \\frac{-5}{\\sqrt{13}\\times\\sqrt{17}} \\\\[2ex] &amp;= -0.336 \\\\[4ex] \\arccos(1.914) &amp;= 1.914 \\end{split} \\] and \\(\\theta=109.65^{\\circ}\\). Below is the R syntax to compute this angle. # Create vectors a = matrix(data = c(2, 3), ncol = 1) b = matrix(data = c(-4, 1), ncol = 1) # Compute dot product between a and b a_dot_b = sum(a * b) # Compute vector lengths l_a = sqrt(sum(a * a)) l_b = sqrt(sum(b * b)) # Compute theta (in radians) acos(a_dot_b / (l_a * l_b)) [1] 1.91382 # Compute theta (in degrees) acos(a_dot_b / (l_a * l_b)) * 180 / pi [1] 109.6538 Manipulating the formula to compute the angle between two vectors provides a common formula to determine the dot product between two vectors. \\[ \\begin{split} \\cos (\\theta) &amp;= \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\\\[2ex] \\mathbf{a}\\bullet\\mathbf{b} &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert \\cos(\\theta) \\end{split} \\] That is the dot product between vectors a and b is equal to the product of their magnitudes and the cosine of the angle between them. 5.1.1 Orthogonal Vectors Two vectors a and b are orthogonal when the angle between them is \\(90^\\circ\\). Since the cosine of a \\(90^\\circ\\) angle is 0, if a and b are orthogonal, then \\[ 0 = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\] For example, consider the following two elementary vectors \\[ \\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\] The dot product between these two vectors is 0, which implies that the cosine of the angle between them must also be 0, indicating that \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) are orthogonal. 5.1.2 Collinear Vectors Two vectors a and b are collinear when the angle between them is \\(0^\\circ\\). Since the cosine of a \\(0^\\circ\\) angle is 1, if a and b are collinear, then \\[ 1 = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\] For example, consider the following two vectors \\[ \\mathbf{a} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix} \\] \\[ \\begin{split} \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} &amp;= \\frac{14}{\\sqrt{5}\\times\\sqrt{45}} \\\\[2ex] &amp;= \\frac{14}{\\sqrt{225}} \\\\[2ex] &amp;= \\frac{14}{14} \\\\[2ex] &amp;= 1 \\end{split} \\] This implies that a and b are collinear. Two vectors are collinear when one can be written as a linear combination of the other. In our example, \\[ \\mathbf{b} = 3\\mathbf{a} \\] Geometrically, collinear vectors are parallel to one another (remember location in the reference space is a convenience). 5.2 Orthogonal Projection Orthogonal projection of vector a on vector b occurs by dropping a perpendicular line from the terminus of a to intersect with x2. Figure 5.2: Orthogonal projection of vector a (vermillion) onto vector b (black). Note that the projection creates a 90-degree angle with vector b. The result of the projection is the vector p (blue). The result is a vector p which is collinear with b but has a different length. To compute the length of p, we make use of the fact that the projection creates a right triangle having a hypotenuse of a and an adjacent leg of p to the angle \\(\\theta\\). Then, \\[ \\begin{split} \\cos(\\theta) &amp;= \\frac{\\lvert\\lvert\\mathbf{p}\\rvert\\rvert}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert} \\\\[2em] \\lvert\\lvert\\mathbf{p}\\rvert\\rvert &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\cos(\\theta) \\end{split} \\] Since \\(\\theta\\) is the angle between a and b, \\[ \\begin{split} \\lvert\\lvert\\mathbf{p}\\rvert\\rvert &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\cos(\\theta) \\\\[2em] &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert}\\\\[2em] &amp;= \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\end{split} \\] That is, the magnitude of the projection p, is the ratio of the dot product between vectors a and b to the magnitude of b. Consider the following two vectors: \\[ \\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} \\] Projecting a onto b, geometrically, Figure 5.3: Orthogonal projection of vector a (in red) onto vector b (in blue). The result of the projection is the vector p (in black). We can find the magnitude of p using our formula: \\[ \\begin{split} \\lvert\\lvert\\mathbf{p}\\rvert\\rvert &amp;= \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\\\[2ex] &amp;= \\frac{1}{\\sqrt{5}} \\\\[2ex] &amp;= 0.447 \\end{split} \\] # Create vectors a = c(2, 3) b = c(2, -1) # Compute dot product of a and b a_dot_b = sum(a * b) # Compute length of b l_b = sqrt(sum(b * b)) # Compute length of p l_p = a_dot_b / l_b l_p [1] 0.4472136 Henceforth, we will denote the projection of vector a onto vector b as: \\[ \\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}} \\] 5.3 Orthogonal Decomposition Orthogonal projection of a vector results in a geometric decomposition of the vector into two additive components. Figure 5.4 illustrates the decomposition of vector a into two additive components, \\(\\mathbf{p}_1\\) and \\(\\mathbf{p}_2\\). That is, \\[ \\mathbf{a} = \\mathbf{p}_1 + \\mathbf{p}_2 \\] Figure 5.4: Two orthogonal projections of vector a (vermillion). The first orthogonal projection is from vector a onto vector b (horizontal black) and the secondorthogonal projection is from vector a is onto vector o (vertical black). The result of the projections are the vectors \\(\\mathbf{p}_1\\) (blue horizontal) and \\(\\mathbf{p}_2\\) (blue vertical). The vector \\(\\mathbf{p}_1\\) is the same orthogonal projection from the earlier example, namely \\(\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}}\\). The vector \\(\\mathbf{p}_1\\) is a second projection, of a onto a vector o (\\(\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\)). The vector o is, by definition, orthogonal to b. The lengths of the two projections correspond to the lengths of the sides of the right triangle where the hypotenuse is a. Namely4, \\[ \\begin{split} \\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}}\\rvert\\rvert &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\cos(\\theta) \\\\[2em] \\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\rvert\\rvert &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\sin(\\theta) \\end{split} \\] As an example, we can decompose a Using our two previous example vectors: \\[ \\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} \\] We previously determined that \\(\\lvert\\lvert\\mathbf{a}\\rvert\\rvert=\\sqrt{13}\\), \\(\\lvert\\lvert\\mathbf{b}\\rvert\\rvert=\\sqrt{5}\\), and \\(\\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}}\\rvert\\rvert = 0.447\\). Recall, \\[ \\cos(\\theta) = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\] This implies that \\(\\cos(\\theta)=\\frac{1}{\\sqrt{65}}\\) and, taking the arc-cosine, that \\(\\theta = 82.87^\\circ\\) (or 1.45 radians). Using this value, we can compute the magnitude of the second projection as: \\[ \\begin{split} \\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\rvert\\rvert &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\sin(\\theta) \\\\[2em] &amp;= \\sqrt{13} \\times \\sin(82.87^\\circ) \\\\[2em] &amp;= 3.58 \\end{split} \\] # Compute length of a l_a = sqrt(sum(a * a)) # Compute theta (in radians) theta = acos(a_dot_b / (l_a * l_b)) theta [1] 1.446441 # Compute length of projection of a onto o l_p2 = l_a * sin(theta) l_p2 [1] 3.577709 We can use the Pythagorean theorem to verify the computation of the two projections’ lengths. Since the square of the hypotenuse of a right triangle is the sum of the squares of the sides, \\[ \\begin{split} \\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}}\\rvert\\rvert^2 +\\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\rvert\\rvert^2 &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert^2 \\\\[2em] 3.58^2 + 0.447^2 &amp;= (\\sqrt{13})^2 \\\\[2em] 13 &amp;= 13 \\end{split} \\] # Compute sum of the squared projection lengths l_p^2 + l_p2^2 [1] 13 # Compute length of a squared l_a^2 [1] 13 # Check values using Pythagorean Theorem l_a^2 == l_p^2 + l_p2^2 [1] TRUE The final system is shown in Figure 5.5. Figure 5.5: Orthogonal projection of vector a (in red) onto vector b (in blue). The result of the projection is the vector \\(\\mathbf{p}_1\\) (in black). A second projection is the vector \\(\\mathbf{p}_2\\) (in black) on to the vector o, which is orthogonal to b. There are other ways to compute the length of \\(\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\). For example, since o is orthogonal to b, the angle between o and a is \\(\\phi=90-\\theta\\). Then \\(\\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\rvert\\rvert=\\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\cos(\\phi)\\).↩︎ "],["statistical-appplication-of-vectors.html", "Chapter 6 Statistical Appplication of Vectors 6.1 Deviation Scores and the Standard Deviation 6.2 Vector Correlation and Separation 6.3 Orthogonal Decomposition and Bivariate Regression", " Chapter 6 Statistical Appplication of Vectors In this chapter, we will provide examples of how vectors define the underlying geometry of various statistical summaries (standard deviation and correlation coefficient), including linear models. We will provide an example using a single predictor, but again, the ideas can be extended to models that include multiple predictors. 6.1 Deviation Scores and the Standard Deviation Consider the following vector scores (x) and the vector of the mean deviation scores (\\(\\mathbf{d_x}\\)): \\[ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\\end{bmatrix} \\qquad \\mathbf{d_x} = \\begin{bmatrix} x_1-\\bar{x} \\\\ x_2-\\bar{x} \\\\ x_3-\\bar{x} \\\\ \\vdots \\\\ x_n-\\bar{x}\\end{bmatrix} \\] Remember that the length of a vector is the square root of the dot product of the vector with itself. If we were to compute the length of the deviation score vector, the length would be the square root of the sum of squared deviations: \\[ \\sqrt{\\mathbf{d_x}\\bullet \\mathbf{d_x}} = \\sqrt{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + (x_3 - \\bar{x})^2 + \\ldots + (x_n - \\bar{x})^2} \\] If we divided this result by \\(\\sqrt{n}\\), this would be equivalent to the standard deviation of the scores in x (\\(s_\\mathbf{x}\\)). \\[ s_\\mathbf{x} = \\frac{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert}{\\sqrt{n}} \\] So there is a direct relation between the length of a deviation vector and the standard deviation, namely, \\[ \\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert = \\sqrt{n} (s_\\mathbf{x}) \\] Consider the following vectors of data representing SAT scores (x) and GPA values (y) for \\(n=10\\) student: \\[ \\mathbf{x} = \\begin{bmatrix}760 \\\\ 710 \\\\ 680 \\\\ 730 \\\\ 420 \\\\ 410 \\\\ 620 \\\\ 630 \\\\ 720 \\\\ 670\\end{bmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix} 3.8 \\\\ 2.4 \\\\ 2.6 \\\\ 3.1 \\\\ 1.9 \\\\ 1.7 \\\\ 2.5 \\\\ 2.4 \\\\ 3.5 \\\\ 3.1\\end{bmatrix} \\] We compute the mean scores for each vector as \\(\\bar{x}=635\\) and \\(\\bar{y}=2.7\\), respectively. By subtracting the mean (which is a scalar) from each vector of data, we can create deviation vectors: \\[ \\mathbf{d_x} = \\mathbf{x} - 635 = \\begin{bmatrix}125 \\\\ 75 \\\\ 45 \\\\ 95 \\\\ -215 \\\\ -225 \\\\ -15 \\\\ -5 \\\\ 85 \\\\ 35\\end{bmatrix} \\qquad \\mathbf{d_y} =\\mathbf{y}-2.7 = \\begin{bmatrix}1.1 \\\\ -0.3 \\\\ -0.1 \\\\ 0.4 \\\\ -0.8 \\\\ -1.0 \\\\ -0.2 \\\\ -0.3 \\\\ 0.8 \\\\ 0.4\\end{bmatrix} \\] The length of the deviation vectors are computed as \\(\\sqrt{\\mathbf{d}\\bullet\\mathbf{d}}\\), namely, \\[ \\begin{split} \\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert &amp;= \\sqrt{\\mathbf{d_x}\\bullet\\mathbf{d_x}} \\\\[2em] \\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert &amp;= \\sqrt{\\mathbf{d_y}\\bullet\\mathbf{d_y}} \\end{split} \\] And using the values in the deviation vectors, \\[ \\begin{split} \\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert &amp;= \\sqrt{137850} = 371.28 \\\\[2em] \\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert &amp;= \\sqrt{4.04} = 2.01 \\end{split} \\] Finally, we can compute the standard deviations for x and y by dividing these lengths by \\(\\sqrt{n}\\). \\[ \\begin{split} s_\\mathbf{x} &amp;= \\frac{371.28}{\\sqrt{10}} = 117.41 \\\\[2em] s_\\mathbf{y} &amp;= \\frac{4.04}{\\sqrt{10}} = 0.64 \\end{split} \\] # Original vectors x = matrix(data = c(760, 710, 680, 730, 420, 410, 620, 630, 720, 670), ncol = 1) y = matrix(data = c(3.8, 2.4, 2.6, 3.1, 1.9, 1.7, 2.5, 2.4, 3.5, 3.1), ncol = 1) # Compute deviation vectors d_x = x - mean(x) d_x [,1] [1,] 125 [2,] 75 [3,] 45 [4,] 95 [5,] -215 [6,] -225 [7,] -15 [8,] -5 [9,] 85 [10,] 35 d_y = y - mean(y) d_y [,1] [1,] 1.1 [2,] -0.3 [3,] -0.1 [4,] 0.4 [5,] -0.8 [6,] -1.0 [7,] -0.2 [8,] -0.3 [9,] 0.8 [10,] 0.4 # Compute lengths of x deviation vector l_x = sqrt(sum(d_x * d_x)) l_x [1] 371.2816 # Compute lengths of y deviation vector l_y = sqrt(sum(d_y * d_y)) l_y [1] 2.009975 # Compute sd of x s_x = l_x / sqrt(10) s_x [1] 117.4095 # Compute sd of y s_y = l_y / sqrt(10) s_y [1] 0.6356099 Note that if we are using \\(s_\\mathbf{x}\\) as an estimate for the population parameter \\(\\sigma_\\mathbf{x}\\), that is x is a sample of student scores, then we need to divide the length of vector x by \\(n-1\\) rather than \\(n\\). In that case, \\[ \\begin{split} \\hat\\sigma_\\mathbf{x} &amp;= \\frac{371.28}{\\sqrt{9}} = 123.76 \\\\[2em] \\hat\\sigma_\\mathbf{y} &amp;= \\frac{4.04}{\\sqrt{9}} = 0.67 \\end{split} \\] This is the denominator that is used in the sd() function in R. # Compute sd of x sigma_x = l_x / sqrt(9) sigma_x [1] 123.7605 # Compute sd of y sigma_y = l_y / sqrt(9) sigma_y [1] 0.6699917 # Check results sd(x) [1] 123.7605 sd(y) [1] 0.6699917 6.2 Vector Correlation and Separation The correlation between two vectors can also be expressed in terms of deviation scores: \\[ \\begin{split} r_{xy} &amp;= \\frac{\\mathrm{Cov}_{\\mathbf{xy}}}{s_\\mathbf{x} s_\\mathbf{y}} \\\\[2em] &amp;= \\frac{\\frac{\\sum (\\mathbf{x}-\\bar{x})(\\mathbf{y}-\\bar{y})}{n}}{s_\\mathbf{x} s_\\mathbf{y}} \\end{split} \\] The expression \\(\\sum (x-\\bar{x})(y-\\bar{y})\\) is equivalent to \\(\\mathbf{x}\\bullet\\mathbf{y}\\). Furthermore, we can re-write the standard deviations using our previous relationship with length of the deviation vectors. This implies, \\[ \\begin{split} r_{xy} &amp;= \\frac{\\frac{\\mathbf{d_x}\\bullet\\mathbf{d_y}}{n}}{\\frac{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert}{\\sqrt{n}}\\frac{\\lvert\\lvert\\mathbf{d_y}\\rvert\\rvert}{\\sqrt{n}}} \\\\[2em] &amp;= \\frac{\\frac{\\mathbf{d_x}\\bullet\\mathbf{d_y}}{n}}{\\frac{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert~\\lvert\\lvert\\mathbf{d_y}\\rvert\\rvert}{n}} \\\\[2em] &amp;= \\frac{\\mathbf{d_x}\\bullet\\mathbf{d_y}}{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert~\\lvert\\lvert\\mathbf{d_y}\\rvert\\rvert} \\end{split} \\] That is, the correlation between x and y is equal to the ratio between the dot product of the deviation vectors and the product of their lengths.5 In our example, # Compute correlation sum(d_x * d_y) / (l_x * l_y) [1] 0.8468822 # Check with correlation function cor(x, y) [,1] [1,] 0.8468822 Recall that the angle between two vectors a and b, denoted \\(\\theta_{\\mathbf{ab}}\\), is given by, \\[ \\cos (\\theta_{\\mathbf{ab}}) = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\] Thus to compute the angle between the two deviation vectors (\\(\\theta\\)), this is: \\[ \\cos (\\theta) = \\frac{\\mathbf{d_x}\\bullet\\mathbf{d_y}}{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{d_y}\\rvert\\rvert} \\] This is the same as the formula for the correlation! In other words, \\[ r_{\\mathbf{xy}} = \\cos (\\theta) \\] The correlation between two vectors x and y is equal to the cosine of the angle between their deviation vectors. Using this equality, we can find the angle between the deviation vectors for our example SAT scores and GPA values. Since \\(r_{\\mathbf{xy}}=0.847\\), that implies \\(\\theta\\approx32^\\circ\\). # Compute theta (in radians) acos(0.847) [1] 0.5604801 # Compute theta (in degrees) acos(0.847) * 180 / pi [1] 32.11314 This correlation corresponds to an angle of approximately \\(32^\\circ\\) of separation between the deviation score vectors for SAT scores and GPA values in the 10-dimensional space defined by our 10 students. We can use the fact that \\(r_{\\mathbf{xy}} = \\cos (\\theta)\\) to make some connections between the value of the correlation coefficient and the angle between the deviation vectors. Perfect positive correlation (\\(r_{\\mathbf{xy}}=1\\)) indicates that deviation vectors are collinear. In this case \\(\\cos(\\theta)=1\\) which implies that \\(\\theta=0^\\circ\\). Perfect negative correlation (\\(r_{\\mathbf{xy}}=-1\\)) indicates that deviation vectors are in opposite directions. In this case \\(\\cos(\\theta)=-1\\) which implies that \\(\\theta=180^\\circ\\). Perfect lack of correlation (\\(r_{\\mathbf{xy}}=0\\)) indicates that deviation vectors are orthogonal. In this case \\(\\cos(\\theta)=0\\) which implies that \\(\\theta=90^\\circ\\). 6.3 Orthogonal Decomposition and Bivariate Regression Recall that the simple regression model seeks to explain variation in an outcome variable Y using a predictor variable X. Mathematically, the model fitted is expressed as: \\[ \\mathbf{y} = b_0 + b_1(\\mathbf{x}) + \\mathbf{e} \\] where y is a vector of fitted values, \\(b_0\\) and \\(b_1\\) are scalars produced from the OLS estimation, x is a vector of the predictor values, and e is a vector of residuals. Recall that when we have mean centered the outcome and predictor, the intercept (\\(b_0\\)) drops out of this equation. we can express this as: \\[ \\mathbf{y} - \\bar{y} = b_1(\\mathbf{x} - \\bar{x}) + \\mathbf{e} \\] The outcome and predictors are now expressed as deviation vectors, say \\[ \\mathbf{d}_\\mathbf{y} = b_1(\\mathbf{d}_\\mathbf{x} ) + \\mathbf{e} \\] where \\(\\hat{\\mathbf{d}}_\\mathbf{y}=b_1(\\mathbf{d}_\\mathbf{x} )\\). That is, we can partition the deviation vector of y into two components: \\[ \\mathbf{d}_\\mathbf{y} = \\hat{\\mathbf{d}}_\\mathbf{y} + \\mathbf{e} \\] Ordinary least squares (OLS) estimation determines the value of \\(b_1\\) by minimizing the sum of squared residuals, that is, it minimizes the quantity \\(\\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2\\). Geometrically, minimizing \\(\\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2\\) is determining the orthogonal projection of \\(\\mathbf{d}_\\mathbf{y}\\) onto \\(\\mathbf{d}_\\mathbf{x}\\).6 This projection is the first component of the partitioning of the deviation vector described previously, \\(\\hat{\\mathbf{d}}_\\mathbf{y}=b_1(\\mathbf{d}_\\mathbf{x})\\), which is collinear with \\(\\mathbf{d}_\\mathbf{x}\\) since \\(b_1(\\mathbf{d}_\\mathbf{x})\\) is a scalar multiple of \\(\\mathbf{d}_\\mathbf{x}\\) This is shown in Figure 6.1. Figure 6.1: The two orthogonal projections from the deviation vector of y form the basis for the model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the e vector. The ‘triangle’ formed by the vectors \\(\\hat{\\mathbf{d}}_\\mathbf{y}\\), e, and \\(\\mathbf{d}_\\mathbf{y}\\) is referred to as the model triangle. Figure 6.2 shows the model triangle. (Remember e can be moved to the right-side of the triangle since location is not a vector property.) Figure 6.2: The model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the e vector. The geometry of this triangle is the same as the geometry visualizing the sum of these vectors, namely \\[ \\mathbf{d}_\\mathbf{y} = \\hat{\\mathbf{d}}_\\mathbf{y} + \\mathbf{e} \\] Namely that the vectors that create the legs of the model triangle can be added together to create the \\(\\mathbf{d}_\\mathbf{y}\\) hypotenuse vector. The e vector is also the second orthogonal projection vector, \\(\\mathbf{e} = \\mathbf{p}_{\\mathbf{d}_\\mathbf{y}\\perp\\mathbf{o}}\\). By definition, this means that the vector of residuals (e) is orthogonal to the vector of fitted values (\\(\\hat{\\mathbf{d}}_\\mathbf{y}\\)), which means, the correlation between those two vectors is zero. \\[ r_{\\mathbf{e},\\hat{\\mathbf{d}}_\\mathbf{y}} = 0 \\] It also means that the model triangle is a right triangle, whose side lengths are governed by the Pythagorean Theorem. \\[ \\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert^2 = \\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert^2 + \\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2 \\] Expressing these lengths using the deviations we get \\[ \\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 + \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\] This is the partitioning that describes the ANOVA decomposition: \\[ \\mathrm{Total~SS} = \\mathrm{Model~SS} + \\mathrm{Residual~SS} \\] Each of these sum of squares is the squared length of one of the vectors in the model triangle. \\[ \\begin{split} \\mathrm{Total~SS} &amp;= \\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert^2 \\\\[2em] \\mathrm{Model~SS} &amp;= \\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert^2 \\\\[2em] \\mathrm{Residual~SS} &amp;= \\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2 \\end{split} \\] Relatedly, the lengths of the vectors making up the model triangle are the square roots of the these sum of square terms: \\[ \\begin{split} \\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert &amp;= \\sqrt{\\mathrm{Total~SS}} \\\\[2em] \\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert &amp;= \\sqrt{\\mathrm{Model~SS}} \\\\[2em] \\lvert\\lvert\\mathbf{e}\\rvert\\rvert &amp;= \\sqrt{\\mathrm{Residual~SS}} \\end{split} \\] Figure 6.3: The side lengths of the model triangle (yellow) correspond to the square roots of the sum of square terms used in the ANOVA decomposition. Lastly, since the model-level \\(R^2\\) value is defined as \\(R^2 = \\frac{\\mathrm{Model~SS}}{\\mathrm{Total~SS}}\\) and there is only a single predictor in the model then, \\[ \\begin{split} r_{\\mathbf{xy}} &amp;= \\sqrt{R^2} \\\\[2em] &amp;= \\sqrt{\\frac{\\mathrm{Model~SS}}{\\mathrm{Total~SS}}} \\\\[2em] &amp;= \\frac{\\sqrt{\\mathrm{Model~SS}}}{\\sqrt{\\mathrm{Total~SS}}} \\\\[2em] &amp;= \\frac{\\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert}{\\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert} \\end{split} \\] That is, the correlation coefficient between x and y is equivalent to the ratio of the lengths between the orthogonal projection vector collinear with the deviation vector of x and the deviation vector of y. Note this is also exactly how we compute cosine of \\(\\theta\\). 6.3.1 Back to the SAT and GPA Example Here we return to our GPA and SAT data to provide an example of these computations. Suppose we want to predict SAT (y) from GPA (x), employing deviation scores. We begin by computing the length of both deviation vectors: \\[ \\begin{split} \\lvert\\lvert\\mathbf{d}_\\mathbf{x}\\rvert\\rvert &amp;= \\sqrt{137850} = 371.28\\\\[2em] \\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert &amp;= \\sqrt{4.04} = 2.01 \\end{split} \\] # Compute length of deviation vector d_x sqrt(sum(l_x * l_x)) [1] 371.2816 # Compute length of deviation vector d_y sqrt(sum(l_y * l_y)) [1] 2.009975 We can also compute the correlation coefficient between SAT scores and GPAs by finding the cosine of the angle between the two vectors, \\(r = 0.847\\). Taking the arc-cosine, we find that \\(\\theta = 32.12^\\circ\\). # Compute correlation r = sum(d_x * d_y) / (l_x * l_y) r [1] 0.8468822 Figure 6.4: The model triangle (yellow) for our regression of GPA (y) onto SAT scores (x). We can now use the definition of cosine and sine to compute the lengths of the two orthogonal projections from \\(\\mathbf{d}_\\mathbf{y}\\). The length of the projection onto \\(\\mathbf{d}_\\mathbf{x}\\) is calculated as: \\[ \\begin{split} \\cos(\\theta) &amp;= \\frac{\\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert}{\\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert} \\\\[2em] 0.847 &amp;= \\frac{\\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert}{2.01} \\\\[2em] \\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert &amp;= 1.70 \\end{split} \\] The length of the second projection onto o is calculated as: \\[ \\begin{split} \\sin(\\theta) &amp;= \\frac{\\lvert\\lvert\\mathbf{e}\\rvert\\rvert}{\\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert} \\\\[2em] 0.531 &amp;= \\frac{\\lvert\\lvert\\mathbf{e}\\rvert\\rvert}{2.01} \\\\[2em] \\lvert\\lvert\\mathbf{e}\\rvert\\rvert &amp;= 1.07 \\end{split} \\] # Compute length of projection on d_x r * l_y [1] 1.702212 # Compute length of projection on o sin(acos(r)) * l_y [1] 1.068866 Figure 6.5 shows the model triangle for the regression of GPA (y) onto SAT scores (x) with all of the computed side lengths. Figure 6.5: The model triangle (yellow) for our regression of GPA (y) onto SAT scores (x). Finally, we can use these lengths, along with the length of \\(\\mathbf{d}_\\mathbf{y}\\) to write out the ANOVA decomposition; the partitioning of the sum of squares. \\[ \\begin{split} \\mathrm{Total~SS} &amp;= \\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert^2 = 2.01^2 = 4.04\\\\[2em] \\mathrm{Model~SS} &amp;= \\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert^2 = 1.70^2 = 2.90\\\\[2em] \\mathrm{Residual~SS} &amp;= \\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2 = 1.07^2 = 1.14 \\end{split} \\] These sums of squares are additive within rounding: \\[ \\begin{split} \\mathrm{Total~SS} &amp;= \\mathrm{Model~SS} + \\mathrm{Residual~SS} \\\\[2em] 4.04 &amp;= 2.90 + 1.14 \\end{split} \\] Using these values we can also compute the model-level \\(R^2\\). \\[ R^2 = \\frac{\\mathrm{Model~SS}}{\\mathrm{Total~SS}} = \\frac{2.90}{4.04} = 0.718 \\] That is, differences in SAT scores explain 71.8% of the variation in GPAs. Most formulas for the correlation will use \\(n-1\\) rather than \\(n\\), but here it doesn’t matter as the \\(n\\)s drop out when we reduce this.↩︎ Remember from geometry that the shortest distance from a point (at the end of the \\(\\mathbf{d}_\\mathbf{y}\\)) to a line (spanned by \\(\\mathbf{d}_\\mathbf{x}\\)) is the perpendicular line segment between them.↩︎ "],["matrices-1.html", "Chapter 7 Matrices 7.1 Matrix Equality 7.2 Exercises", " Chapter 7 Matrices A matrix is a rectangular array of elements arranged in rows and columns. We typically denote matrices using a bold-faced, upper-case letter. For example, consider the matrix B which has 3 rows and 2 columns: \\[ \\mathbf{B} = \\begin{bmatrix} 5 &amp; 1 \\\\ 7 &amp; 3 \\\\ -2 &amp; -1 \\end{bmatrix} \\] In psychometric and statistical applications, the data we work with typically have this type of rectangular arrangement. For example, the data in 7.1, which includes measures of 5 variables for 100 students, is arranged into the familiar case-by-variable rectangular ‘data matrix.’7 Table 7.1: Example set of education data. The data are rectangular, having rows (cases) and columns (variables). ID SAT GPA IQ School 1 560 3.0 112 Public 2 780 3.9 143 Public 3 620 2.9 124 Private \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) 100 600 2.7 129 Public 7.0.1 Dimensions of a Matrix We define matrices in terms of their dimensions or order; the number of rows and columns within the matrix. The dimensions of the matrix B is \\(3\\times 2\\); it has three rows and two columns. Whereas the dimensions of the data matrix is \\(100\\times 5\\). We often denote the matrix’s dimension by appending it to the bottom of the matrix’s name, \\[ \\underset{3\\times 2}{\\mathbf{B}} = \\begin{bmatrix} 5 &amp; 1 \\\\ 7 &amp; 3 \\\\ -2 &amp; -1 \\end{bmatrix} \\] 7.0.2 Indexing Individual Elements The elements within the matrix are indexed by their row number and column number. For example, \\(\\mathbf{B}_{1,2} = 1\\) since the element in the first row and second column is 1. The subscripts on each element indicate the row and column positions of the element. Vectors are a special case of a matrix, where a row vector is a \\(1\\times n\\) matrix, and a column vector is a \\(n \\times 1\\) matrix. Note that although a vector can always be described as a matrix, a matrix cannot always be described as a vector. In general, we define matrix A, which has n rows and k columns as: \\[ \\underset{n\\times k}{\\mathbf{A}} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\ldots &amp; a_{1k} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\ldots &amp; a_{2k} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; \\ldots &amp; a_{3k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\ldots &amp; a_{nk} \\end{bmatrix} \\] where element \\(a_{ij}\\) is in the \\(i^{\\mathrm{th}}\\) row and \\(j^{\\mathrm{th}}\\) column of A. 7.0.3 Geometry of Matrices A matrix is an k-dimensional representation where k is the number of columns in the matrix. For example, consider the matrix M: \\[ \\mathbf{M} = \\begin{bmatrix} 1 &amp; 3 \\\\ 2 &amp; -2 \\end{bmatrix} \\] Since M has two columns, this matrix represents a 2-dimensional structure (i.e., a plane). Each column in the matrix corresponds to a vector in the coordinate system that helps define that structure. M is defined by the space encompassed by the vectors \\((1, 2)\\) and \\((3, -2)\\). Figure 7.1 shows matrix M plotted in the 2-dimensional coordinate space defined by R1 and R2. Figure 7.1: Matrix M (yellow) in the 2-dimensional coordinate system defined by R1 and R2. Because most matrices have much higher dimensions, we typically do not visualize them geometrically. For additional detail on the geometry of matrices, see YouTube content creator 3Blue1Brown’s excellent playlist, Essence of Linear Algebra. 7.0.4 Matrices in R To enter a matrix in R, we use the matrix() function, similar to creating a vector. The only difference is that the nrow= or ncol= argument will include a value other than 1. The elements of the matrix given in the data= argument, by default, will be filled-in by columns. For example to create matrix B from the earlier example, we can use the following syntax. # Create B by filling in columns B = matrix( data = c(5, 7, -2, 1, 3, -1), nrow = 3, ncol = 2 ) # Display B B [,1] [,2] [1,] 5 1 [2,] 7 3 [3,] -2 -1 The byrow=TRUE argument will fill the elements by rows rather than columns. # Create B by filling in rows B = matrix( data = c(5, 1, 7, 3, -2, -1), byrow = TRUE, nrow = 3 ) # Display B B [,1] [,2] [1,] 5 1 [2,] 7 3 [3,] -2 -1 The dim() function can be used to return the dimensions of a matrix. # Get matrix dimensions dim(B) [1] 3 2 Lastly, we can index elements of a matrix by specifying the row and column number of the matrix object for the element we want in square brackets. These values are separated by a comma. For example, to index the element in B that is in the 2nd row and 1st column, we use the following syntax: # Index the element in the 2nd row, 1st column B[2, 1] [1] 7 7.1 Matrix Equality Two matrices are said to be equal if they satisfy two conditions: They have the same dimensions, and All corresponding elements are equal. Consider the following matrices: \\[ \\mathbf{A} = \\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix} 112 &amp; 134 \\\\86 &amp; 94 \\\\ 0 &amp; 0 \\end{bmatrix} \\qquad \\mathbf{C} = \\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} \\] Within this set of matrices, \\(\\mathbf{A} \\neq \\mathbf{B}\\), since they do not have the same dimensions. A is a \\(2\\times3\\) matrix and B is a \\(3\\times2\\) matrix. Similarly, \\(\\mathbf{A} \\neq \\mathbf{B}\\), they also do not have the same dimensions. \\(\\mathbf{A} = \\mathbf{C}\\) since both matrices have the same dimensions and all corresponding elements are equal. 7.2 Exercises Consider the following matrices: \\[ \\mathbf{A} = \\begin{bmatrix}5 &amp; 6 &amp; 4&amp; 5 &amp; 9 \\\\21 &amp; 23 &amp; 24 &amp; 22 &amp; 20 \\end{bmatrix} \\qquad \\mathbf{b}^\\intercal = \\begin{bmatrix}0 &amp; 1 &amp; 0 &amp; 0 \\end{bmatrix} \\qquad \\mathbf{Y} = \\begin{bmatrix}2 &amp; 3 &amp; 1 \\\\5 &amp; 6 &amp; 8\\\\9 &amp; 4 &amp; 7 \\end{bmatrix} \\] Use notation to describe the dimensions of A. Show/Hide Solution \\[ \\underset{2\\times5}{\\mathbf{A}} \\] Use notation to describe the dimensions of b. Show/Hide Solution \\[ \\underset{4\\times1}{\\mathbf{b}} \\] Use notation to describe the dimensions of Y. Show/Hide Solution \\[ \\underset{3\\times3}{\\mathbf{Y}} \\] The matrix X contains data from four students’ of four different course exams. \\[ \\mathbf{X} = \\begin{bmatrix}32 &amp; 54 &amp; 56 &amp; 21 \\\\42 &amp; 23 &amp; 52 &amp; 35 \\\\ 16 &amp; 41 &amp; 54 &amp; 56 \\\\ 58 &amp; 52 &amp; 31 &amp; 24 \\end{bmatrix} \\] Which student obtained a 35 and on which test? Report the row and column using subscript notation. Show/Hide Solution \\[ X_{2,4}=35 \\] The 2nd student received a 35 on the 4th exam. Using subscript notation, indicate the 4th student’s score on the 1st exam. Show/Hide Solution \\[ X_{4,1}=58 \\] In computation, a matrix is a very particular type of data structure in which every column has the same type of data (e.g., every column is numeric, or every column is a character variable). A data frame is a more generalized computational structure that accommodates multiple types of columns. In some computational languages this structure is referred to as a data matrix.↩︎ "],["matrix-operations.html", "Chapter 8 Matrix Operations 8.1 Matrix Addition and Subtraction 8.2 Scalar–Matrix Multiplication 8.3 Matrix Multiplication 8.4 Exercises", " Chapter 8 Matrix Operations In this chapter you will learn about some of the common arithmetic operations (addition, subtraction, and multiplication) that can be performed with matrices. 8.1 Matrix Addition and Subtraction Two or more matrices can be added or subtracted if they have the same dimensions; if not, matrix addition and subtraction is undefined. Just as in vector addition, each corresponding element is added or subtracted and placed in the corresponding location in the new matrix. Consider the following two matrices: \\[ \\mathbf{A} = \\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix} 101 &amp; 89 &amp; 1 \\\\110 &amp; 90 &amp; 0 \\end{bmatrix} \\] Then \\[ \\begin{split} \\mathbf{A} + \\mathbf{B} &amp;= \\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} + \\begin{bmatrix} 101 &amp; 89 &amp; 1 \\\\110 &amp; 90 &amp; 0 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix} 112 + 101 &amp; 86+89 &amp; 0+1 \\\\ 134+110 &amp; 94+90 &amp; 0+0 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix} 213 &amp; 175 &amp; 1 \\\\ 244 &amp; 184 &amp; 0 \\end{bmatrix} \\end{split} \\] In general, the elements in the summed matrix, C, are defined as \\(\\mathbf{C}_{ij}=\\mathbf{A}_{ij} + \\mathbf{B}_{ij}\\) for all i and j. Since subtraction is equivalent to adding the inverse, subtracting the elements of B from A is equivalent to adding the inverted elements of B to the elements of A (where “inverting the elements” means switching the sign on each element). \\[ \\begin{split} \\mathbf{A} - \\mathbf{B} &amp;= \\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} + \\begin{bmatrix} -101 &amp; -89 &amp; -1 \\\\-110 &amp; -90 &amp; 0 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix} 11 &amp; -3 &amp; -1 \\\\ 24 &amp; 4 &amp; 0 \\end{bmatrix} \\end{split} \\] Computationally, we can use the + and - operators in R to add and subtract matrices. # Create A A = matrix(data = c(112, 86, 0, 134, 94, 0), byrow = TRUE, nrow = 2) # Create B B = matrix(data = c(101, 89, 1, 110, 90, 0), byrow = TRUE, nrow = 2) # Matrix addition A + B [,1] [,2] [,3] [1,] 213 175 1 [2,] 244 184 0 # Matrix Subtraction A - B [,1] [,2] [,3] [1,] 11 -3 -1 [2,] 24 4 0 8.1.1 Properties of Matrix Addition Matrix addition satisfies both the commutative and associative properties. That is, \\[ \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A} \\] and \\[ \\begin{split} \\mathbf{A} + (\\mathbf{B} + \\mathbf{C}) &amp;= (\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} \\\\[2ex] &amp;= \\mathbf{A} + \\mathbf{B} + \\mathbf{C} \\end{split} \\] 8.1.2 Deviation Matrix We have seen that deviation scores are particularly useful in statistics. We can create a deviation matrix by taking a score matrix and subtracting from it a matrix of means, where each column contains the mean for each corresponding column in the original matrix, that is: \\[ \\mathbf{D} = \\mathbf{X} - \\mathbf{M} \\] For example, consider the following score matrix: \\[ \\mathbf{X} = \\begin{bmatrix} 3.0 &amp; 11 &amp; 112 \\\\ 3.9 &amp; 10 &amp; 143 \\\\ 2.9 &amp; 19 &amp; 124 \\\\ 2.7 &amp; 7 &amp; 129 \\end{bmatrix} \\] To compute the mean deviation matrix, D we use: \\[ \\begin{split} \\mathbf{D} &amp;= \\mathbf{X} - \\mathbf{M} \\\\[2em] &amp;= \\begin{bmatrix} 3.0 &amp; 11 &amp; 112 \\\\ 3.9 &amp; 10 &amp; 143 \\\\ 2.9 &amp; 19 &amp; 124 \\\\ 2.7 &amp; 7 &amp; 129 \\end{bmatrix} - \\begin{bmatrix} 3.125 &amp; 11.75 &amp; 127 \\\\ 3.125 &amp; 11.75 &amp; 127 \\\\ 3.125 &amp; 11.75 &amp; 127 \\\\ 3.125 &amp; 11.75 &amp; 127 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix} -0.125 &amp; -0.75 &amp; -15 \\\\ 0.775 &amp; -1.75 &amp; 16 \\\\ -0.225 &amp; 7.25 &amp; -3 \\\\ -0.425 &amp; -4.75 &amp; 2 \\end{bmatrix} \\end{split} \\] Computationally, we can use the colMeans() function to compute the mean in each column of X. Then we can create matrix M by using the rep() function to repeat this set of means four times. (There are four rows in M that have the exact same elements.) # Create X X = matrix(data = c(3.0, 11, 112, 3.9, 10, 143, 2.9, 19, 124, 2.7, 7, 129), byrow = TRUE, nrow = 4) X [,1] [,2] [,3] [1,] 3.0 11 112 [2,] 3.9 10 143 [3,] 2.9 19 124 [4,] 2.7 7 129 # Compute column means colMeans(X) [1] 3.125 11.750 127.000 # Create M M = matrix(rep(colMeans(X), 4), byrow = TRUE, nrow =4) M [,1] [,2] [,3] [1,] 3.125 11.75 127 [2,] 3.125 11.75 127 [3,] 3.125 11.75 127 [4,] 3.125 11.75 127 # Compute deviation matrix X - M [,1] [,2] [,3] [1,] -0.125 -0.75 -15 [2,] 0.775 -1.75 16 [3,] -0.225 7.25 -3 [4,] -0.425 -4.75 2 8.2 Scalar–Matrix Multiplication Any matrix can be multiplied by a scalar. The result is a matrix in which each element in the original matrix is multiplied by the value of the scalar. For example to multiply matrix A (from the previous section) by a scalar \\(\\lambda = -2\\), \\[ \\begin{split} -2\\mathbf{A} &amp;= -2\\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix} -2(112) &amp; -2(86) &amp; -2(0) \\\\ -2(134) &amp; -2(94) &amp; -2(0) \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix} -224 &amp; -172 &amp; 0 \\\\ -268 &amp; -188 &amp; 0 \\end{bmatrix} \\end{split} \\] In general the elements of the product matrix, \\(\\lambda\\mathbf{A}\\), are \\(\\lambda (a_{ij})\\) for every i and j. Scalar–matrix multiplication is commutative, that is: \\[ \\lambda\\mathbf{A} = \\mathbf{A}\\lambda \\] In R we use the * operator to perform scalar–matrix multiplication. -2 * A [,1] [,2] [,3] [1,] -224 -172 0 [2,] -268 -188 0 8.3 Matrix Multiplication The process of multiplying matrices follows the same basic principle as vector multiplication, where we consider matrices to be a collection of column vectors. When we multiply vectors, they must have the same number of elements because we multiply corresponding elements and add the resulting products, obtaining a scalar (dot) product (i.e., \\(\\mathbf{a}\\bullet\\mathbf{b}\\)). To multiply matrix X by matrix Y, we are going to compute the dot product between each row in X and each column in Y, that is, \\(\\mathbf{x}^\\intercal_{j}\\bullet\\mathbf{b}_j\\). As an example, consider the following two matrices: \\[ \\mathbf{X} = \\begin{bmatrix} 2 &amp; 3 \\\\ 1 &amp; 2 \\end{bmatrix} \\qquad \\mathbf{Y} = \\begin{bmatrix} 5 &amp; -2 \\\\ 4 &amp; -1 \\end{bmatrix} \\] To complete the multiplication of XY, we compute the dot product between the first row of X and the first column of Y (i.e., \\(\\mathbf{X}_1^\\intercal\\bullet\\mathbf{Y}_1\\)). \\[ \\begin{split} \\begin{bmatrix}2 &amp; 3\\end{bmatrix}\\begin{bmatrix}5 \\\\ 4\\end{bmatrix} &amp;= 2(5) + 3(4)\\\\[2em] &amp;= 22 \\end{split} \\] This gives us the element in the first row and first column of the product matrix, Z. The element in the first row and second column of Z is based on the dot product between the the first row of X and the second column of Y. This continues, finding the dot product between rows of X and columns of Y for each corresponding element in Z. \\[ \\begin{split} \\mathbf{Z} &amp;= \\begin{bmatrix}\\begin{bmatrix}2 &amp; 3\\end{bmatrix}\\begin{bmatrix}5 \\\\ 4\\end{bmatrix} &amp; \\begin{bmatrix}2 &amp; 3\\end{bmatrix}\\begin{bmatrix}-2 \\\\ -1\\end{bmatrix} \\\\ \\begin{bmatrix}1 &amp; 2\\end{bmatrix}\\begin{bmatrix}5 \\\\ 4\\end{bmatrix} &amp; \\begin{bmatrix}1 &amp; 2\\end{bmatrix}\\begin{bmatrix}-2 \\\\ -1\\end{bmatrix}\\end{bmatrix}\\\\[2em] &amp;= \\begin{bmatrix} 22 &amp; -7 \\\\ 13 &amp; -4 \\end{bmatrix} \\end{split} \\] This process must always be carefully followed: Finding the dot product between a row of the first matrix and a column of the second matrix. The general rule on matrix multiplication for \\(\\mathbf{AB} = \\mathbf{C}\\), for each element in C, \\(C_{ij}\\) is the dot product of the ith row of A and the jth column of B. To compute the product between two matrices we use the matrix multiplication operator, %*%. # Create A A = matrix(data = c(2, 3, 1, 2), byrow = TRUE, nrow = 2) # Create B B = matrix(data = c(5, -2, 4, -1), byrow = TRUE, nrow = 2) # Matrix multiplication A %*% B [,1] [,2] [1,] 22 -7 [2,] 13 -4 In contrast to scalar multiplication, matrix multiplication is rarely commutative, that is, it is rare that \\(\\mathbf{AB}=\\mathbf{BA}\\). In our previous example, \\(\\mathbf{AB}\\neq\\mathbf{BA}\\). # Matrix multiplication B %*% A [,1] [,2] [1,] 8 11 [2,] 7 10 The resulting product is not the same as computing AB. The order of the multiplication is quite important, so rather than saying that “A is multiplied by B,” we instead specify the order by saying: A is postmultiplied by B; or B is premultiplied by A. 8.3.0.1 Conformability We can also multiply matrices that are not of equal dimensions, as long as they are conformable. To be conformable, the number of columns in the premultiplied (first) matrix must equal the number of rows in the postmultiplied (second) matrix. For example, matrices A and B (below) are conformable since the number of columns in A is equal to the number of rows in B, namely 4. \\[ \\underset{2\\times4}{\\mathbf{A}} = \\begin{bmatrix} 5 &amp; 2 &amp; 3 &amp; 4 \\\\ 5 &amp; 4 &amp; 6 &amp; 1 \\end{bmatrix} \\qquad \\underset{4\\times3}{\\mathbf{B}} = \\begin{bmatrix} 8 &amp; 9 &amp; 4 \\\\ 6 &amp; 5 &amp; 1 \\\\ 2 &amp; 3 &amp; 4 \\\\ 6 &amp; 1 &amp; 2 \\end{bmatrix} \\] One way to consider conformability is to ensure that the inner dimensions of the two matrices being multiplied are equal. Here the inner dimensions (shown in red) are both 4. \\[ \\underset{2\\times\\color{red}{4}}{\\mathbf{A}}~\\underset{{\\color{red}{4}} \\times3}{\\mathbf{B}} \\] # Create A A = matrix(data = c(5, 2, 3, 4, 5, 4, 6, 1), byrow = TRUE, nrow = 2) # Create B B = matrix(data = c(8, 9, 4, 6, 5, 1, 2, 3, 4, 6, 1, 2), byrow = TRUE, nrow = 4) # Postmultiply A by B A %*% B [,1] [,2] [,3] [1,] 82 68 42 [2,] 82 84 50 Notice the product AB has dimensions of \\(2\\times4\\), which are the outer dimensions shown in the product (below in red). \\[ \\underset{{\\color{red}{2}} \\times4}{\\mathbf{A}}~\\underset{4\\times\\color{red}{3}}{\\mathbf{B}} \\] If we tried to postmultiply B by A, that product is not conformable since the inner dimensions would no longer match (i.e., \\(3\\neq2\\)). \\[ \\underset{4\\times\\color{red}{3}}{\\mathbf{B}} ~ \\underset{{\\color{red}{2}}\\times4}{\\mathbf{A}} \\] # Postmultiply B by A B %*% A Error in B %*% A: non-conformable arguments 8.3.1 Properties of Matrix Multiplication Although matrix multiplication is rarely commutative, it does have the following properties: \\(\\mathbf{A}(\\mathbf{BC})=(\\mathbf{AB})\\mathbf{C})\\) (Associative Property) \\(\\mathbf{A}(\\mathbf{B}+\\mathbf{C})=\\mathbf{AB} +\\mathbf{AC})\\) (Left Distributive Property) \\((\\mathbf{B}+\\mathbf{C})\\mathbf{A}=\\mathbf{BA} +\\mathbf{CA})\\) (Right Distributive Property) so long as the matrices being considered are conformable. Aside from commutativity, there are also some other properties that matrix multiplication lack. For example, in scalar arithmetic, if \\(ab = 0\\), then either a or b must be zero. In matrix multiplication, however, this is not always the case. To illustrate, consider the following three matrices: \\[ \\mathbf{A} = \\begin{bmatrix}-2 &amp; 4 \\\\-2 &amp; 4 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}0 &amp; 0 \\\\0 &amp; 0 \\end{bmatrix} \\qquad \\mathbf{C} = \\begin{bmatrix}4 &amp; 4 \\\\2 &amp; 2 \\end{bmatrix} \\] Matrix B, in which every element is zero is called the null matrix. (A null matrix is a matrix in which every element is zero.) If we postmultiply matrix A by the null matrix B, we get: \\[ \\begin{split} \\mathbf{A}\\mathbf{B} &amp;= \\begin{bmatrix}-2 &amp; 4 \\\\-2 &amp; 4 \\end{bmatrix} \\begin{bmatrix}0 &amp; 0 \\\\0 &amp; 0 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix}0 &amp; 0 \\\\0 &amp; 0 \\end{bmatrix} \\end{split} \\] The result of postmultiplying a matrix (so long as it is conformable) by the null matrix is a null matrix. This is also true when we premultiply by a null matrix. Now consider postmultiplying matrix A by matrix C: \\[ \\begin{split} \\mathbf{A}\\mathbf{C} &amp;= \\begin{bmatrix}-2 &amp; 4 \\\\-2 &amp; 4 \\end{bmatrix} \\begin{bmatrix}4 &amp; 4 \\\\2 &amp; 2 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix}0 &amp; 0 \\\\0 &amp; 0 \\end{bmatrix} \\end{split} \\] We again obtain a null matrix. However, neither A nor C was a null matrix. (In fact, none of the elements of A or C were zero). Relatedly, in scalar arithmetic, if \\(ab = 0\\), then \\(ba = 0\\). This property does not hold in matrix multiplication. For example, we just saw that \\(\\mathbf{A}\\mathbf{C}=\\mathbf{0}\\). But, if we postmultiply C by A: \\[ \\begin{split} \\mathbf{C}\\mathbf{A} &amp;= \\begin{bmatrix}4 &amp; 4 \\\\2 &amp; 2 \\end{bmatrix}\\begin{bmatrix}-2 &amp; 4 \\\\-2 &amp; 4 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix}-16 &amp; 16 \\\\-8 &amp; 8 \\end{bmatrix} \\end{split} \\] That is, although \\(\\mathbf{A}\\mathbf{C}=\\mathbf{0}\\), \\(\\mathbf{C}\\mathbf{A}\\neq\\mathbf{0}\\). Changing the order of the matrix multiplication changes the resulting product. Drawing from this example we see that since \\(\\mathbf{AB}=\\mathbf{0}\\) and \\(\\mathbf{AC}=\\mathbf{0}\\), this means that \\(\\mathbf{AB}=\\mathbf{AC}\\). However, \\(\\mathbf{B}\\neq\\mathbf{C}\\). This implies that “cancellation” is not a valid property of matrix multiplication. 8.3.2 Revisiting Dot Products It turns out that we can express the dot product between two vectors as matrix multiplication. Consider the column vectors a and b: \\[ \\mathbf{a} = \\begin{bmatrix}5 \\\\ 4 \\\\ 7 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}1 \\\\ 0 \\\\ -1 \\\\ 2\\end{bmatrix} \\] Remember column vectors are matrices with one column. The dot product between a and b is the same as premultiplying b by the transpose of a, that is: \\[ \\mathbf{a}\\bullet\\mathbf{b} = \\underset{1\\times4}{\\mathbf{a}^\\intercal}~\\underset{4\\times1}{\\mathbf{b}} \\] By transposing a, the matrices are conformable, and the resulting product will be a \\(1\\times1\\) matrix (i.e., a scalar). # Create a a = matrix(data = c(5, 4, 7, 2), ncol = 1) # Create b b = matrix(data = c(1, 0, -1, 2), ncol = 1) # Postmultiply the transpose of a by b t(a) %*% b [,1] [1,] 2 # Compute dot product as sum of products sum(a * b) [1] 2 8.4 Exercises Consider the following matrices: \\[ \\mathbf{X} = \\begin{bmatrix}2 &amp; 3 \\\\1 &amp; 2 \\end{bmatrix} \\qquad \\mathbf{Y} = \\begin{bmatrix}3 &amp; 4 \\\\2 &amp; 1 \\end{bmatrix} \\qquad \\mathbf{Z} = \\begin{bmatrix}2 &amp; 3 &amp; 1 \\\\5 &amp; 6 &amp; 8\\\\9 &amp; 4 &amp; 7 \\end{bmatrix} \\] Find \\(\\mathbf{X} + \\mathbf{Y}\\). Show/Hide Solution \\[ \\begin{split} \\mathbf{X}+\\mathbf{Y} &amp;= \\begin{bmatrix}2+3 &amp; 3+4 \\\\1+2 &amp; 2+1 \\end{bmatrix} \\\\[1em] &amp;= \\begin{bmatrix}5 &amp; 7 \\\\3 &amp; 3 \\end{bmatrix} \\end{split} \\] Find \\(\\mathbf{X} - \\mathbf{Y}\\). Show/Hide Solution \\[ \\begin{split} \\mathbf{X}-\\mathbf{Y} &amp;= \\begin{bmatrix}2-3 &amp; 3-4 \\\\1-2 &amp; 2-1 \\end{bmatrix} \\\\[1em] &amp;= \\begin{bmatrix}-1 &amp; -1 \\\\-1 &amp; 1 \\end{bmatrix} \\end{split} \\] Find \\(3 \\mathbf{Z}\\). Show/Hide Solution \\[ \\begin{split} 3\\mathbf{Z} &amp;= \\begin{bmatrix}3(2) &amp; 3(3) &amp; 3(1) \\\\3(5) &amp; 3(6) &amp; 3(8)\\\\3(9) &amp; 3(4) &amp; 3(7) \\end{bmatrix} \\\\[1em] &amp;= \\begin{bmatrix}6 &amp; 9 &amp; 3 \\\\15 &amp; 18 &amp; 24\\\\27 &amp; 12 &amp; 21 \\end{bmatrix} \\end{split} \\] Find \\(-2 \\mathbf{X} + 4\\mathbf{Y}\\). Show/Hide Solution \\[ \\begin{split} -2\\mathbf{X}+4\\mathbf{Y} &amp;= \\begin{bmatrix}-2(2)+4(3) &amp; -2(3)+4(4) \\\\-2(1)+4(2) &amp; -2(2)+4(1) \\end{bmatrix} \\\\[1em] &amp;= \\begin{bmatrix}8 &amp; 10 \\\\6 &amp; 0 \\end{bmatrix} \\end{split} \\] Consider the following matrices: \\[ \\mathbf{A} = \\begin{bmatrix}0 &amp; 6 \\\\5 &amp; 1 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}0 &amp; 5 \\\\2 &amp; \\frac{1}{2} \\end{bmatrix} \\qquad \\mathbf{C} = \\begin{bmatrix}6 &amp; 2 &amp; 1 \\\\5 &amp; 3 &amp; 1\\\\8 &amp; 4 &amp; 1 \\end{bmatrix} \\qquad \\mathbf{D} = \\begin{bmatrix}0&amp; 1 &amp; 4 &amp; 6 \\\\1 &amp; 2 &amp; 5 &amp; -2\\\\1 &amp; 3 &amp; 2 &amp; 8 \\end{bmatrix} \\] Is AB conformable? Show/Hide Solution Yes. Since A has two columns and B has the same number of rows, AB is conformable. \\[ \\underset{2\\times\\color{red}{2}}{\\mathbf{A}}~\\underset{{\\color{red}{2}} \\times2}{\\mathbf{B}} \\] Is BC conformable? Show/Hide Solution No. Since B has two columns and C has three rows, BC is not conformable. \\[ \\underset{2\\times\\color{red}{2}}{\\mathbf{B}}~\\underset{{\\color{red}{3}} \\times3}{\\mathbf{C}} \\] Is the product where we premultiply D by C conformable? Show/Hide Solution Yes. Since C has three columns and D has the same number of rows, CD is conformable. \\[ \\underset{3\\times\\color{red}{3}}{\\mathbf{C}}~\\underset{{\\color{red}{3}} \\times4}{\\mathbf{D}} \\] Statistics Example: Weights Consider the scores for five students on four course exams (each out of 100 points) shown in matrix X. The final percentage in the course is based on the following weighting: the first and second exams are worth 10% of the course, the third exam is worth 30% of the course, and the fourth exam is worth 50% of the course. These weights are presented in the column vector w. Use R to find the final percentage for each student by postmultiplying the score matrix by the weight vector. \\[ \\mathbf{X} = \\begin{bmatrix}32 &amp; 54 &amp; 56 &amp; 21 \\\\42 &amp; 23 &amp; 52 &amp; 35 \\\\ 16 &amp; 41 &amp; 54 &amp; 56 \\\\ 58 &amp; 52 &amp; 31 &amp; 24 \\\\ 41 &amp; 50 &amp; 42 &amp; 40 \\end{bmatrix} \\qquad \\mathbf{w} = \\begin{bmatrix} 0.10 \\\\ 0.10 \\\\ 0.30 \\\\ 0.50 \\end{bmatrix} \\] Show/Hide Solution # Create X X = matrix( data = c(32, 54, 56, 21, 42, 23, 52, 35, 16, 41, 54, 56, 58, 52, 31, 24, 41, 50, 42, 40), byrow = TRUE, ncol = 4 ) # Create w w = matrix(data = c(0.10, 0.10, 0.30, 0.50), ncol = 1) # Compute Xw X %*% w [,1] [1,] 35.9 [2,] 39.6 [3,] 49.9 [4,] 32.3 [5,] 41.7 "],["special-matrices.html", "Chapter 9 Special Matrices 9.1 Matrix Transpose 9.2 Square Matrices 9.3 Diagonal Matrices 9.4 Scalar Matrices 9.5 Symmetric Matrices 9.6 Triangular Matrices 9.7 Exercises", " Chapter 9 Special Matrices In this chapter, you will learn about several special matrices encountered in statistical and psychometric applications. Many of these matrices occur quite frequently in matrix applications. Others have properties that make them incredibly useful for computation. 9.1 Matrix Transpose Transposition is an operation that can also be carried out on matrices. Just as when we transpose a vector, we transpose a matrix by taking each column in turn and making it a row. In other words, we interchange each column and row, so the first column becomes the first row, the second column becomes the second row, etc. For example, \\[ \\mathbf{A} = \\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} \\qquad \\mathbf{A}^\\intercal = \\begin{bmatrix} 112 &amp; 134 \\\\86 &amp; 94 \\\\ 0 &amp; 0 \\end{bmatrix} \\] Formally, if A is an \\(n \\times k\\) matrix with elements \\(a_{ij}\\), then the transpose of A, denoted \\(\\mathbf{A}^\\intercal\\) is a \\(k \\times n\\) matrix where element \\(a^\\intercal_{ij}=a_{ji}\\). Some properties of the matrix transpose are: \\((\\mathbf{A}^{\\intercal}) ^{^{\\intercal}} = \\mathbf{A}\\) \\((\\lambda\\mathbf{A})^{^{\\intercal}} = \\lambda \\mathbf{A}^{\\intercal}\\) where \\(\\lambda\\) is a scalar \\((\\mathbf{A} + \\mathbf{B})^{^{\\intercal}} = \\mathbf{A}^{\\intercal} + \\mathbf{B}^{\\intercal}\\) (This property extends to more than two matrices; \\((\\mathbf{A} + \\mathbf{B} + \\mathbf{C})^{^{\\intercal}} = \\mathbf{A}^{\\intercal} + \\mathbf{B}^{\\intercal} + \\mathbf{C}^{\\intercal}\\).) \\((\\mathbf{A}\\mathbf{B})^{^{\\intercal}} = \\mathbf{B}^{\\intercal} \\mathbf{A}^{\\intercal}\\) (This property also extends to more than two matrices; \\((\\mathbf{A}\\mathbf{B}\\mathbf{C})^{^{\\intercal}} = \\mathbf{C}^{\\intercal}\\mathbf{B}^{\\intercal} \\mathbf{A}^{\\intercal}\\).) Computationally, the t() function will produce the transpose of a matrix in R. # Create A A = matrix( data = c(112, 134, 86, 94, 0, 0), nrow = 2 ) # Display A A [,1] [,2] [,3] [1,] 112 86 0 [2,] 134 94 0 # Compute transpose t(A) [,1] [,2] [1,] 112 134 [2,] 86 94 [3,] 0 0 9.2 Square Matrices When the number of rows and columns in a matrix are equal, the matrix is referred to as a square matrix. For example, X and Y are both square matrices. \\[ \\underset{2\\times 2}{\\mathbf{X}} = \\begin{bmatrix} 4 &amp; 1 \\\\ 0 &amp; 3 \\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{Y}} = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 3 &amp; -8\\\\ 10 &amp; 4 &amp; -2 \\end{bmatrix} \\] 9.2.1 Main Diagonal In a square matrix, the main diagonal (a.k.a., major diagonal or principal diagonal) includes the elements that lie along the diagonal from the upper-left element to the lower-right element. For example, the main diagonal in Y (highlighted in red) includes the elements \\(0\\), \\(3\\), and \\(-2\\). Elements not in the main diagonal are referred to as off-diagonal elements. \\[ \\underset{3\\times 3}{\\mathbf{Y}} = \\begin{bmatrix} \\color{red}{0} &amp; 1 &amp; 0 \\\\ 1 &amp; \\color{red}{3} &amp; -8\\\\ 10 &amp; 4 &amp; \\color{red}{-2} \\end{bmatrix} \\] One interesting and useful property of a square matrix is that if we compute its transpose, the main diagonal is the same as in the original matrix. \\[ \\underset{3\\times 3}{\\mathbf{Y}} = \\begin{bmatrix} \\color{red}0 &amp; 1 &amp; 0 \\\\ 1 &amp; \\color{red}3 &amp; -8\\\\ 10 &amp; 4 &amp; \\color{red}{-2} \\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{Y}^\\intercal} = \\begin{bmatrix} \\color{red}0 &amp; 1 &amp; 10 \\\\ 1 &amp; \\color{red}3 &amp; 4\\\\ 0 &amp; -8 &amp; \\color{red}{-2} \\end{bmatrix} \\] We can use the diag() function to return the elements on the main diagonal in a square matrix. # Create Y Y = matrix( data = c(0, 1, 10, 1, 3, 4, 0, -8, -2), nrow = 3 ) # Display Y Y [,1] [,2] [,3] [1,] 0 1 0 [2,] 1 3 -8 [3,] 10 4 -2 # Find diagonal elements diag(Y) [1] 0 3 -2 🚧 CAUTION: The diag() function also works on non-square matrices. However, it returns the elements on the diagonal starting with the element in the first row and column. # Create X X = matrix( data = c(0, 1, 1, 3, 4, 0), nrow = 3 ) # Display X X [,1] [,2] [1,] 0 3 [2,] 1 4 [3,] 1 0 # Find &#39;diagonal&#39; elements diag(X) [1] 0 4 This is because non-square matrices also have a main diagonal. The main diagonal in a non-square matrix includes the elements \\(a_{11}, a_{22}, \\ldots, a_{mm}\\) where m is the minimum of the row and column values. 9.3 Diagonal Matrices A diagonal matrix is a square matrix in which all the off-diagonal elements are zero. Two examples of diagonal matrices are as follows: \\[ \\underset{2\\times 2}{\\mathbf{D}_1} = \\begin{bmatrix} 5 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{D}_2} = \\begin{bmatrix} 3 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{bmatrix} \\] In general, D is a diagonal matrix if element \\(d_{ij}=0\\) for all \\(i\\neq j\\). 9.4 Scalar Matrices A diagonal matrix in which all the diagonal elements have the same value is called a scalar matrix. The following two matrices are scalar matrices: \\[ \\underset{2\\times 2}{\\mathbf{S}_1} = \\begin{bmatrix} -3 &amp; 0 \\\\ 0 &amp; -3 \\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{S}_2} = \\begin{bmatrix} 10 &amp; 0 &amp; 0 \\\\ 0 &amp; 10 &amp; 0 \\\\ 0 &amp; 0 &amp; 10 \\end{bmatrix} \\] The null matrix is also a scalar matrix. In general S is a scalar matrix if element \\[ s_{ij}=\\begin{cases}k \\quad \\mathrm{when} \\quad i=j \\\\0 \\quad \\mathrm{when} \\quad i\\neq j\\end{cases} \\] 9.4.1 Identity Matrices A special scalar matrix, the identity matrix, is one in which all the diagonal elements are the value of one (1). \\[ \\underset{2\\times 2}{\\mathbf{I}} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{I}} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] The identity matrix has the following property: For any matrix A, \\(\\mathbf{AI} = \\mathbf{IA} = \\mathbf{A}\\) The diag() function can be used to create an identity matrix. We give this function an argument which provides the number of rows and columns. # Create a 3x3 identity matrix I = diag(3) # Display I I [,1] [,2] [,3] [1,] 1 0 0 [2,] 0 1 0 [3,] 0 0 1 9.5 Symmetric Matrices If \\(\\mathbf{A} = \\mathbf{A}^{\\intercal}\\), the matrix A is considered symmetric. For example, the following matrix R is symmetric: \\[ \\underset{3\\times 3}{\\mathbf{R}} = \\begin{bmatrix} 1.00 &amp; 0.32 &amp; 0.69\\\\ 0.32 &amp; 1.00 &amp; 0.85 \\\\ 0.69 &amp; 0.85 &amp; 1.00 \\end{bmatrix} = \\underset{3\\times 3}{\\mathbf{R}^\\intercal} = \\begin{bmatrix} 1.00 &amp; 0.32 &amp; 0.69\\\\ 0.32 &amp; 1.00 &amp; 0.85 \\\\ 0.69 &amp; 0.85 &amp; 1.00 \\end{bmatrix} \\] In general, a matrix A is symmetric if, \\[ \\mathbf{A}_{ij} = \\mathbf{A}_{ji} \\] If matrix A is symmetric, it necessitates that: \\(\\mathbf{A}\\) and \\(\\mathbf{A}^{\\intercal}\\) have the same dimensions. This also implies that the matrix must be a square matrix. All of the corresponding elements in \\(\\mathbf{A}\\) and \\(\\mathbf{A}^{\\intercal}\\) are equal. All scalar matrices, including the identity matrix, are symmetric. In statistical practice, variance–covariance matrices and correlation matrices are symmetric. Computationally, we can examine whether a matrix is symmetric by checking whether the logical statement equating a matrix and its transpose evaluates as TRUE for all elements. For example, consider the correlation matrix R: \\[ \\mathbf{R}=\\begin{bmatrix}1.00 &amp; 0.32 &amp; 0.69\\\\ 0.32 &amp; 1.00 &amp; 0.85\\\\ 0.69 &amp; 0.85 &amp; 1.00\\end{bmatrix} \\] To check its symmetry, we can use the following syntax. # Create matrix R R = matrix( data = c(1.00, 0.32, 0.69, 0.32, 1.00, 0.85, 0.69, 0.85, 1.00), nrow = 3 ) # Display R R [,1] [,2] [,3] [1,] 1.00 0.32 0.69 [2,] 0.32 1.00 0.85 [3,] 0.69 0.85 1.00 # Test for symmetry R == t(R) [,1] [,2] [,3] [1,] TRUE TRUE TRUE [2,] TRUE TRUE TRUE [3,] TRUE TRUE TRUE If the logical statement of equality evaluates as TRUE for all the elements, the matrix is symmetric. If the logical expression returns an non-conformable array error, or evaluates as FALSE for any of the elements, then it is not symmetric. 9.5.1 Skew Symmetric Matrices A matrix A is said to be skew symmetric if \\(\\mathbf{A} = -\\mathbf{A}^{\\intercal}\\).For example, the following matrix S is skew symmetric: \\[ \\underset{3\\times 3}{\\mathbf{S}} = \\begin{bmatrix} 0 &amp; 2 &amp; -3\\\\ -2 &amp; 0 &amp; 1 \\\\ 3 &amp; -1 &amp; 0 \\end{bmatrix} \\] In general, a matrix A is skew symmetric if, \\(\\mathbf{a}_{ij} = -\\mathbf{a}_{ji}\\). Note that if a matrix is skew symmetric, all of its diagonal elements must be zero. 9.6 Triangular Matrices A matrix is a triangular matrix if all of the elements above or below the main diagonal are zero. These are referred to as upper triangular matrices and lower triangular matrices, respectively. A matrix A is a lower triangular matrix if elements \\(a_{ij}=0\\) for \\(j&gt;i\\) (all the elements above the main diagonal are zero). For example the matrix L is lower triangular: \\[ \\underset{4\\times 4}{\\mathbf{L}} = \\begin{bmatrix} 6 &amp; \\color{red}{0} &amp; \\color{red}{0} &amp; \\color{red}{0}\\\\ -1 &amp; 5 &amp; \\color{red}{0} &amp; \\color{red}{0} \\\\ 0 &amp; 1 &amp; 2 &amp; \\color{red}{0} \\\\ 3 &amp; -2 &amp; 5 &amp; 7 \\end{bmatrix} \\] A matrix A is an upper triangular matrix if elements \\(a_{ij}=0\\) for \\(i&gt;j\\) (all the elements below the main diagonal are zero). For example the matrix U is lower triangular: \\[ \\underset{4\\times 4}{\\mathbf{U}} = \\begin{bmatrix} 3 &amp; -2 &amp; 5 &amp; 7\\\\ \\color{red}{0} &amp; 1 &amp; 2 &amp; 0 \\\\ \\color{red}{0} &amp; \\color{red}{0} &amp; -1 &amp; 5 &amp; \\\\ \\color{red}{0} &amp; \\color{red}{0} &amp; \\color{red}{0} &amp; 6\\\\ \\end{bmatrix} \\] 9.7 Exercises Consider the following matrices: \\[ \\mathbf{A} = \\begin{bmatrix}1 &amp; 5 &amp; -1 \\\\ 3 &amp; 2 &amp; -1 \\\\ 0 &amp; 1 &amp; 5 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}3 &amp; 1 &amp; 6 \\\\ 2 &amp; 0 &amp; 1 \\\\ -7 &amp; -1 &amp; 2 \\end{bmatrix} \\] Verify that \\((\\mathbf{A}+\\mathbf{B})^\\intercal = \\mathbf{A}^\\intercal + \\mathbf{B}^\\intercal\\)? Show/Hide Solution \\[ \\begin{split} (\\mathbf{A}+\\mathbf{B})^\\intercal &amp;= \\begin{bmatrix}4 &amp; 6 &amp; 5\\\\5 &amp; 2 &amp; 0\\\\ -7 &amp; 0 &amp; 7 \\end{bmatrix}^\\intercal = \\begin{bmatrix}4 &amp; 5 &amp; -7\\\\6 &amp; 2 &amp; 0\\\\5 &amp; 0&amp; 7 \\end{bmatrix} \\\\[3em] \\mathbf{A}^\\intercal + \\mathbf{B}^\\intercal &amp;= \\begin{bmatrix}1 &amp; 3 &amp; 0\\\\5 &amp; 2 &amp; 1\\\\-1 &amp; -1 &amp; 5 \\end{bmatrix} + \\begin{bmatrix}3 &amp; 2 &amp; -7\\\\1 &amp; 0 &amp; -1\\\\6 &amp; 1 &amp; 2 \\end{bmatrix} = \\begin{bmatrix}4 &amp; 5 &amp; -7\\\\6 &amp; 2 &amp; 0\\\\5 &amp; 0&amp; 7 \\end{bmatrix} \\end{split} \\] Verify that \\((\\mathbf{A}\\mathbf{B})^\\intercal = \\mathbf{B}^\\intercal \\mathbf{A}^\\intercal\\)?. Show/Hide Solution \\[ \\begin{split} (\\mathbf{A}\\mathbf{B})^\\intercal &amp;= \\begin{bmatrix}20 &amp; 2 &amp; 9\\\\20 &amp; 4 &amp; 18\\\\-33 &amp; -5 &amp; 11 \\end{bmatrix}^\\intercal = \\begin{bmatrix}20 &amp; 20 &amp; -33\\\\ 2 &amp; 4&amp; -5\\\\9 &amp; 18 &amp; 11 \\end{bmatrix} \\\\[3em] \\mathbf{B}^\\intercal \\mathbf{A}^\\intercal &amp;= \\begin{bmatrix}3 &amp; 2 &amp; -7\\\\1 &amp; 0 &amp; -1\\\\6 &amp; 1 &amp; 2 \\end{bmatrix}\\begin{bmatrix}1 &amp; 3 &amp; 0\\\\5 &amp; 2 &amp; 1\\\\-1 &amp; -1 &amp; 5 \\end{bmatrix} = \\begin{bmatrix}20 &amp; 20 &amp; -33\\\\ 2 &amp; 4&amp; -5\\\\9 &amp; 18 &amp; 11 \\end{bmatrix} \\end{split} \\] List the diagonal elements of \\(\\mathbf{A}^\\intercal\\). Show/Hide Solution \\[ \\begin{split} B^\\intercal_{1,1}=1 \\\\[1em] B^\\intercal_{2,2}=2 \\\\[1em] B^\\intercal_{3,3}=5 \\\\[1em] \\end{split} \\] Create an identity matrix of order 4. Show/Hide Solution \\[ \\underset{4\\times4}{\\mathbf{I}} = \\begin{bmatrix}1 &amp; 0 &amp; 0&amp; 0\\\\0 &amp; 1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix} \\] Explain why the transpose of an upper triangular matrix will be a lower triangular matrix. Show/Hide Solution An upper triangular matrix, U, has elements \\(u_{ij}=0\\) for \\(i&gt;j\\) (all the elements below the main diagonal are zero). Taking the transpose of U, element \\(u_{ij}\\) becomes \\(u^\\intercal_{ji}\\). This means that in \\(\\mathbf{U}^\\intercal\\) the zero elements all be where \\(j&gt;i\\). Consider the following: \\[ \\mathbf{X} = \\begin{bmatrix} 10 &amp; 4 &amp; 6\\\\4 &amp; 10 &amp; 8\\\\6 &amp; 8 &amp; 9 \\end{bmatrix} \\qquad \\mathbf{Y} = \\begin{bmatrix}0 &amp; 1 &amp; 2\\\\-1 &amp; 0 &amp; 4\\\\-2 &amp; -4 &amp; 0 \\end{bmatrix} \\] Use R to show that X is symmetric. Show/Hide Solution # Create X X = matrix( data = c(10, 4, 6, 4, 10, 8, 6, 8, 9), byrow = TRUE, ncol = 3 ) # Check symmetry X == t(X) [,1] [,2] [,3] [1,] TRUE TRUE TRUE [2,] TRUE TRUE TRUE [3,] TRUE TRUE TRUE Use R to show that Y is skew symmetric. Show/Hide Solution # Create X Y = matrix( data = c(0, 1, 2, -1, 0, 4, -2, -4, 0), byrow = TRUE, ncol = 3 ) # Check symmetry Y == -t(Y) [,1] [,2] [,3] [1,] TRUE TRUE TRUE [2,] TRUE TRUE TRUE [3,] TRUE TRUE TRUE "],["references.html", "References", " References Bi, X., Tang, X., Yuan, Y., Zhang, Y., &amp; Qu, A. (2021). Tensors in Statistics. Annual Review of Statistics and Its Application, 8(1), 345–368. https://doi.org/10.1146/annurev-statistics-042720-020816 McCullagh, P. (2018). Tensor methods in statistics. http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=5228165 "]]

[["index.html", "Matrix Algebra for Educational Scientists Foreword Acknowledgments Colophon", " Matrix Algebra for Educational Scientists Michael Rodriguez &amp; Andrew Zieffler 2021-08-11 Foreword Spring 2021. The contents of this book constitute information from several set of notes from a variety of QME courses, including from an old course called EPsy 8269. We are making this book available as a resource for anyone who wants to use it. We will be adding and revising the content for awhile. Feel free to offer criticism, suggestion, and feedback. You can either open an issue on the book’s github page or send us an email directly. Andrew Zieffler &amp; Michael Rodriguez zief0002@umn.edu Acknowledgments Many thanks to James Terwilliger, whose initial notes gave rise to some of this material. Also, thank you to all the students in our courses who have been through previous iterations of this material. Your feedback has been invaluable, and you are the world’s greatest copy editors. Colophon The book is typeset using Karla for the body font, Lora for the headings and Sue Ellen Francisco for the title. The color palette was generated using coolors.co. Icon and note ideas and prototypes by Desirée De Leon. Some of the book style and CSS code were inspired by: De Leon, D., &amp; Hill, A. (2019). A handbook for teaching and learning with R and RStudio. "],["intro.html", "Chapter 1 Introduction 1.1 Prerequisites", " Chapter 1 Introduction Having a basic understanding of the vocabulary, notation, and ideas of matrix algebra, is important for all educational scientists who use quantitative methods in their work. The statistical and psychometric models underlying many quantitative methodologies employed in educational research rely on matrix algebra. Subsequently, educational scientists use the language and notation of matrix algebra to communicate in the scientific literature. Moreover, matrix algebra forms the bedrock of statistical computation. Having fundamental knowledge of matrix algebra can often help an educational scientist troubleshoot problems that arise in their own work, and devise solutions for those issues. For quantitative methodologists, it is important to have a much deeper understanding of matrix algebra, as it is foundational to the computational estimation and optimization used in methodological work. Statistical programming, formulating the mathematics of quantitative methods, and even back-of-the-napkin calculations are all made easier (and more efficient) through matrix algebra. 1.1 Prerequisites ADD PREREQUISITES "],["datastructures.html", "Chapter 2 Data Structures 2.1 Scalars 2.2 Vectors 2.3 Matrices 2.4 Tensors 2.5 A Word about Notation 2.6 Exercises", " Chapter 2 Data Structures In this chapter you will be introduced to four common data structures that form the building blocks of matrix algebra: scalars, vectors, matrices, and tensors. You will also be introduced to some of the vocabulary that we use to describe these structures. In future chapters, we will examine these structures in more detail and learn how to mathematically manipulate and operate on these structures. 2.1 Scalars A scalar is a single real number. You have likely had a lot of previous experience with scalars, as they are emphasized in much of the mathematics taught in high schools in the United States. Here are three examples of scalars: \\[ 1 \\qquad \\sqrt{2} \\qquad -7 \\] Scalar arithmetic is the arithmetic operations (addition, subtraction, multiplication, and division) we perform using real numbers. For example, \\[ \\begin{split} 2 + 3 = 5 \\\\[1ex] 21 \\div 3 = 7 \\end{split} \\] Notice that scalar arithmetic also produces a scalar. For example the scalar addition in the example, \\(2+3\\) produces the scalar \\(5\\). 2.2 Vectors A vector is a specifically ordered one-dimensional array of values. Here are three examples of vectors: \\[ \\begin{bmatrix} -1 \\\\ 3 \\\\ \\sqrt{7} \\\\2 \\end{bmatrix} \\qquad \\begin{bmatrix} 5 &amp; 4 \\end{bmatrix} \\qquad \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{bmatrix} \\] A vector may be written as a row or a column, and are respectively referred to as row vectors or column vectors. Each value in a vector is called an element or component. Vectors are also typically described in terms of the number of elements they have. For example, the following is a two-element row vector: \\[ \\begin{bmatrix} 5 &amp; 4 \\end{bmatrix} \\] Here, a is a four-element column vector: \\[ \\mathbf{a} = \\begin{bmatrix} 5 \\\\ 4 \\\\ 7 \\\\2 \\end{bmatrix} \\] 2.3 Matrices A matrix is specifically ordered two-dimensional array of values. Here are three examples of matrices: \\[ \\begin{bmatrix} -1 &amp; 5\\\\ 3 &amp; 2 \\\\ \\sqrt{7} &amp; -4 \\\\2 &amp; 2 \\end{bmatrix} \\qquad \\begin{bmatrix} 5 &amp; 4 \\\\ 1 &amp; 6 \\end{bmatrix} \\qquad \\begin{bmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\\\ a_{3,1} &amp; a_{32} \\end{bmatrix} \\] Matrices have both rows and columns, and are typically described by the number of rows and columns they have. For example, the matrix B (below) has 3 rows and 2 columns: \\[ \\underset{3\\times 2}{\\mathbf{B}} = \\begin{bmatrix} 5 &amp; 1 \\\\ 7 &amp; 3 \\\\ -2 &amp; -1 \\end{bmatrix} \\] We say that B is a “3 by 2” matrix. The number of rows and columns are referred to as the dimensions or order of the matrix, and, for matrix B, is denoted as \\(3\\times 2\\). The dimension is often appended to the bottom of the matrix (e.g., \\(\\underset{3\\times 2}{\\mathbf{B}}\\)). The elements within the matrix are indexed by their row number and column number, respectively. For example, \\(\\mathbf{B}_{1,2} = 1\\) since the element in the first row and second column is 1. The subscripts on each element indicate the row and column positions of the element.1 More generally, we define matrix A, which has n rows and k columns as: \\[ \\underset{n\\times k}{\\mathbf{A}} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\ldots &amp; a_{1k} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\ldots &amp; a_{2k} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; \\ldots &amp; a_{3k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\ldots &amp; a_{nk} \\end{bmatrix} \\] where element \\(a_{ij}\\) is in the \\(i^{\\mathrm{th}}\\) row and \\(j^{\\mathrm{th}}\\) column of A. 2.4 Tensors Tensors, generally speaking, are the generalization of the matrix to three or more dimensions. Here is an example of a tensor: \\[ \\begin{bmatrix} \\begin{bmatrix} 5 &amp; 4 \\end{bmatrix} &amp; \\begin{bmatrix} 1 &amp; 6 \\end{bmatrix} \\\\ \\begin{bmatrix} 2 &amp; 3 \\end{bmatrix} &amp; \\begin{bmatrix} 0 &amp; 7 \\end{bmatrix} \\end{bmatrix} \\] Technically, this definition is not quite true, but for our purposes it will be adequate to think of a tensor as a structure for data in N dimensions. This book will primarily deal with scalars, vectors, and matrices, but tensors do come up in statistical work. For example, image data are often represented as tensors; with each pixel in a two-dimensional image having multiple values associated with it to represent the color information for the pixel. In longitudinal data analysis, the variance–covariance matrix of the responses for multiple subjects is also represented as a tensor. Bi et al. (2021) and McCullagh (2018) are good resources to learn more about working with and operating on tensors. 2.5 A Word about Notation Authors and textbooks use a wide variety of notation to represent scalars, vectors, and matrices. For example, vectors might be denoted using a lower-case underlined letter (\\(\\underline{a}\\)), a lower-case bolded letter (a), or a lower-case letter with an overset arrow (\\(\\vec{a}\\)). In this book, we will try to use a consistent notation to denote each of these structures. Scalars will be denoted with an italicized lower-case letter (e.g., \\(a\\)) or a non-bolded lower-case Greek letter (e.g., \\(\\lambda\\)). Vectors will be denoted using a bold-faced lower-case letter (e.g., a. Matrices will be denoted using a bold-faced upper-case letter (e.g., A) or a bold-faced upper-case Greek letter (e.g., \\(\\boldsymbol\\Phi\\)). 2.6 Exercises Identify each of the following as a scalar, row vector, column vector, matrix, or tensor. \\((a_1 + a_2 + a_3 + a_4)\\) Show/Hide Solution Scalar \\(\\begin{bmatrix} 520 \\\\ 640 \\\\ 780\\end{bmatrix}\\) Show/Hide Solution Column vector 13 Show/Hide Solution Scalar \\(\\begin{bmatrix} 115 &amp; 129 &amp; 92 &amp; 89\\end{bmatrix}\\) Show/Hide Solution Row vector \\(\\begin{bmatrix}5 &amp; 11 \\\\ -3 &amp; 0\\end{bmatrix}\\) Show/Hide Solution Matrix \\(\\begin{bmatrix} 0 &amp; 0 &amp; 0\\end{bmatrix}\\) Show/Hide Solution Row vector How many elements are in each of the following vectors? \\(\\begin{bmatrix} 115 &amp; 129 &amp; 92 &amp; 89\\end{bmatrix}\\) Show/Hide Solution 4 elements \\(\\begin{bmatrix}5 \\\\ 11 \\\\ -3 \\end{bmatrix}\\) Show/Hide Solution 3 elements \\(\\begin{bmatrix} a_1 &amp; a_2 &amp; a_3 &amp; \\ldots &amp; a_k\\end{bmatrix}\\) Show/Hide Solution k elements What is the order (dimensions) of each of the following matrices? \\(\\begin{bmatrix}5 &amp; 11 \\\\ -3 &amp; 0 \\\\ 2 &amp; -1\\end{bmatrix}\\) Show/Hide Solution \\(3\\times2\\) \\(\\begin{bmatrix}a_{11} &amp; a_{12} &amp; a_{13} &amp; \\ldots &amp; a_{1k} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\ldots &amp; a_{2k} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; \\ldots &amp; a_{3k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\ldots &amp; a_{nk}\\end{bmatrix}\\) Show/Hide Solution \\(n\\times k\\) References "],["vec.html", "Chapter 3 Vectors 3.1 What is a Vector? 3.2 General Form of a Vector 3.3 The Geometry of Vectors 3.4 Vector Properties 3.5 Vector Equality 3.6 Special Vectors 3.7 Exercises", " Chapter 3 Vectors In this chapter you will learn more about the vector data structure. You will be introduced to some of the vocabulary that we use to describe vectors. You will also be introduced to the geometry of vectors. It is important to recognize that although the illustration of vector geometry is restricted to 2-dimensional space, all of these ideas can be extended to n-dimensions. 3.1 What is a Vector? A vector is a specifically ordered one-dimensional array of values written as a row or column. Vectors are typically described by the number of elements they have. Here, for example, a is a four-element column vector: \\[ \\mathbf{a} = \\begin{bmatrix} 5 \\\\ 4 \\\\ 7 \\\\ 2 \\end{bmatrix} \\] We might also say that a is a column vector in 4-dimensions. In statistical or psychometric applications, vectors are how we typically represent and structure data collected on a particular attribute. For example, a might represent test scores for \\(n=4\\) students. Each element in this vector would correspond to a student’s test score. In this case, \\(a_1=5\\) would be the test score for the first student recorded in the vector, \\(a_2=4\\) would be the test score for the second student recorded in the vector, etc. Computationally, we can also work with vectors using R. We use the matrix() function to create a column or row vector in R. We include the elements of the vector in a c() function and provide this to the data= argument. We also include either the argument ncol=1 (column vector) or nrow=1 (row vector). Below we create vector a. We can also use the length() function to count the number of elements in a vector. # Create column vector a a = matrix(data = c(5, 4, 7, 2), ncol = 1) a [,1] [1,] 5 [2,] 4 [3,] 7 [4,] 2 # Create row vector b b = matrix(data = c(1, 0, 0), nrow = 1) b [,1] [,2] [,3] [1,] 1 0 0 # Count number of elements in a length(a) [1] 4 # Count number of elements in b length(b) [1] 3 3.1.1 Transposition One important vector operation is transposition. Transposition is an operation in which we replace the ith element of a column vector as the ith element of a row vector, and vice-versa. In other words, we are converting a column vector into a row vector, or, conversely, converting a row vector into a column vector. Notationally, we use a superscripted prime or a superscripted intercalate symbol (looks like a “T”) to denote a vector’s transpose. For example, transposing vector a, which we defined earlier: \\[ \\mathbf{a} = \\begin{bmatrix} 5 \\\\ 4 \\\\ 7 \\\\ 2 \\end{bmatrix} \\qquad \\mathbf{a}^{\\prime} = \\mathbf{a}^{\\intercal} = \\begin{bmatrix} 5 &amp; 4 &amp; 7 &amp; 2 \\end{bmatrix} \\] In R, the t() function will compute the transpose of a vector. Here we use t() to compute the transpose of the earlier defined vectors a and b. # Transpose of a t(a) [,1] [,2] [,3] [,4] [1,] 5 4 7 2 # Transpose of b t(b) [,1] [1,] 1 [2,] 0 [3,] 0 Throughout the remainder of the book, row vectors will be denoted using a transpose. So, for example, a will indicate a column vector, and \\(\\mathbf{a}^{\\intercal}\\) will indicate a row vector. 3.2 General Form of a Vector Now that we have introduced transposition, we can formalize the general notation for both column and row vectors. The general form of a column vector with n elements is: \\[ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\] While, the general form of a row vector with n elements is: \\[ \\mathbf{x}^{\\intercal} = \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 &amp; \\ldots &amp; x_n \\end{bmatrix} \\] Vectors can also be thought of as a special case of a matrix in one of the dimensions is equal to 1. For example x could be considered a \\(n \\times 1\\) matrix, and \\(\\mathbf{x}^{\\intercal}\\) could be considered a \\(1 \\times n\\) matrix. 3.3 The Geometry of Vectors A vector can be represented geometrically by using a coordinate system where each element in the vector corresponds to a distance along one of the reference axes defining the coordinate system. Consider the column vector a: \\[ \\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\] This vector has a distance of 2 on the first reference axis (R1) and a distance of 3 on the second reference axis (R2). Figure 3.1: Plot showing vector a (in red) in the R1–R2 dimensional space. In Figure 3.1 the reference coordinate system (R1–R2) define a two-dimensional space. Vector a had two elements and thus resides in a two-dimensional space. In general, an n-dimensional vector resides in an n-dimensional space. The reference axes always intersect at the origin of the space (i.e., the reference point \\((0, 0)\\)). Here the reference axes are at right angles to one another, although this is not a requirement. When the reference axes are at right angles to each other, the reference system is referred to as an orthogonal coordinate system. 3.4 Vector Properties Geometrically, a vector is displayed as a line segment with an arrowhead at one end of the segment. The end of the vector with the arrowhead is called the head or terminus, and the other end of the vector is called the tail or origin. The head of the vector indicates the direction of the vector. All vectors have a direction. They also have a length. These two properties completely define a vector within a given reference system. Note that location of the vector in the reference system is not a property of the vector. For example, the red vector and the blue vector below are identical—they have the same direction and length! Figure 3.2: Plot showing two identical vectors (in red and blue) in the R1–R2 dimensional space. They are identical because they have the same length and direction. Location in the reference system is a convenience, not a property of the vector. Location is a convenience in that we can re-locate any vector within the reference system to make it easier to work with. 3.4.1 Vector Length The length of a vector (also referred to as the vector’s magnitude or norm) is the distance from the vector’s tip to tail. In an orthogonal reference system, if we locate the tail of the vector at the origin, multiple right triangles with the vector as hypotenuse can be formed with the reference axes. The Pythagorean Theorem can then be applied to determine the length of a vector. Figure 3.3: Plot showing vector a (in red) in the R1–R2 dimensional space. This vector is the hypotenuse of a right triangle with legs of length 2 and 3. In Figure 3.3, vector a is the hypotenuse of a right triangle with legs of length 2 and 3. We can use the Pythagorean Theorem to find the length of the vector: \\[ \\begin{split} C^2 &amp;= A^2 + B^2 \\\\[2ex] &amp;= 2^2 + 3 ^2 \\\\[2ex] &amp;= 13 \\\\[2ex] C &amp;= \\sqrt{13} \\approx 3.61 \\end{split} \\] Note that the sum of the squared elements is equal to the square of the length of the vector, and to determine the length of a (denoted as \\(\\lvert\\lvert \\mathbf{a} \\rvert\\rvert\\)), we take the square root of that sum. For our two-dimensional vector, this is \\[ \\lvert\\lvert \\mathbf{a} \\rvert\\rvert = \\sqrt{a_1^2 + a_2^2} \\] We can generalize this to finding the length of a vector a with n-dimensions: \\[ \\lvert\\lvert \\mathbf{a} \\rvert\\rvert = \\sqrt{a_1^2 + a_2^2 + a_3^2 + \\ldots + a_n^2} \\] Computationally, to compute the length of a vector, we chain together several computations that (1) square the elements of in the vector, (2) sum those squares together, and (3) compute the square root of this sum. Below we create a 2-element column vector a and compute its length. # Create vector a a = matrix(data = c(2, 3), ncol = 1) a [,1] [1,] 2 [2,] 3 # Compute length of vector a sqrt(sum(a ^ 2)) [1] 3.605551 3.4.2 Vector Direction A vector’s direction is often expressed as the measure of the angle between the horizontal reference axis and the vector.2 Figure 3.4 shows this angle (denoted \\(\\theta\\)) for vector a. Figure 3.4: Plot showing vector a (in red) in the R1–R2 dimensional space. The direction of this vector is the measure of the angle (\\(\\theta\\)) between the horizontal reference axis and the vector. To determine this angle, we need to use a trigonometric function. For example, here we can use the fact that the cosine of an angle \\(\\theta\\) is defined as the length of the side of the right triangle which is adjacent to \\(\\theta\\) divided by the length of the hypotenuse of the triangle.3 \\[ \\cos (\\theta) = \\frac{\\lvert\\lvert\\mathrm{Adjacent~Side}\\rvert\\rvert}{\\lvert\\lvert\\mathrm{Hypotenuse}\\rvert\\rvert} \\] In our example, \\[ \\cos (\\theta) = \\frac{2}{\\sqrt{13}} \\] To find \\(\\theta\\), we need to calculate the arc-cosine of \\(\\frac{2}{\\sqrt{13}}\\). We can compute the arc-cosine in R using the acos() function. This function will compute the value of \\(\\theta\\) in radians. We can also convert this to degrees by multiplying by \\(\\frac{180}{\\pi}\\) (where \\(\\pi\\) is the mathematical constant approximated as 3.14). # Find theta using arccosine (radians) acos(2 / sqrt(13)) [1] 0.9827937 # Find theta using arccosine (degrees) acos(2 / sqrt(13)) * 180 / pi [1] 56.30993 We always measure the angle counter-clockwise to the reference axis. So if we measure the direction associated with the vector \\(\\begin{bmatrix}-2 \\\\ -2\\end{bmatrix}\\), the direction is more than \\(\\pi\\) radians (or \\(180^\\circ\\)). Figure 3.5: Plot showing vector (-2, -2) (in red) in the R1–R2 dimensional space. The direction of this vector is the measure of the angle (\\(\\theta\\)) between the vector and the horizontal reference axis measured in the counter-clockwise direction from the axis. There are several ways to compute this angle. Remember that when we use the trigonometric functions, we are computing the angle in the right triangle formed by the vector and the reference axes (see Figure 3.6). Figure 3.6: Plot showing vector (-2, -2) (in red) in the R1–R2 dimensional space. The direction of this vector is the measure of the angle (\\(\\theta\\)) between the vector and the horizontal reference axis measured in the counter-clockwise direction from the axis. Here we have split this angle up into two parts; \\(\\alpha\\) can be computed using a trigonometric function, and \\(\\beta = \\pi\\) radians. To find \\(\\theta\\) we need to find \\(\\alpha\\) using trigonometric functions and then add it to \\(\\beta\\). For example, \\[ \\begin{split} \\tan (\\alpha) &amp;= \\frac{\\lvert\\lvert\\mathrm{Opposite~Side}\\rvert\\rvert}{\\lvert\\lvert\\mathrm{Adjacent~Side}\\rvert\\rvert} \\\\[2ex] &amp;= \\frac{2}{2} \\\\[2ex] &amp;= 1 \\end{split} \\] Computing the arc-tangent with the atan() function we find that \\(\\alpha=0.785\\) radians. Adding this to the \\(\\beta\\) value which is \\(\\pi\\) radians, we get the vector’s direction of 3.927 radians (or \\(225^\\circ\\)). # Compute alpha (in radians) atan(1) [1] 0.7853982 # Compute theta = alpha + beta (in radians) atan(1) + pi [1] 3.926991 # Compute theta = alpha + beta (in degrees) (atan(1) + pi) * 180 / pi [1] 225 Beyond two dimensions, direction in more complicated, involving more than one angle. For example, reporting the direction of a vector in three dimensions requires two angles. One common method is to give (1) the angle between the first reference axis (e.g., x-axis) and the projection of the vector onto the plane defined by the first and second reference axes (e.g., the xy-plane), and (2) the angle between the vector and the third reference axis (e.g., the z-axis). Suffice it to say that we need additional angles to fully define the direction of a vector in higher dimensions. 3.5 Vector Equality Two vectors are said to be equal if they satisfy two conditions: Both vectors have the same dimensions (i.e., they have the same number of elements and both are row or column vectors); Corresponding elements from each vector must be equal. Consider the following vectors: \\[ \\mathbf{a} = \\begin{bmatrix} 5 \\\\ 4 \\\\ 7 \\\\ 2 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 5 \\\\ 4 \\\\ 2 \\\\ 7 \\end{bmatrix} \\qquad \\mathbf{c}^{\\intercal} = \\begin{bmatrix} 5 &amp; 4 &amp; 7 &amp; 2 \\end{bmatrix} \\] Within this set of vectors, \\(\\mathbf{a} \\neq \\mathbf{b}\\), since not all of the corresponding elements are equal. \\(\\mathbf{a} \\neq \\mathbf{c}^{\\intercal}\\), since a’s dimensions are \\(4 \\times 1\\) (column vector) and \\(\\mathbf{c}^{\\intercal}\\)’s dimensions are \\(1 \\times 4\\) (row vector). \\(\\mathbf{a} = \\mathbf{c}\\) since both vectors a and c have the same dimensions (\\(4 \\times 1\\)) and all corresponding elements are equal. Geometrically, two vectors are equal if they have the same length and direction. (Remember, location in the reference coordinate system is not a property of the vectors, so two equal vectors might be in different locations.) 3.6 Special Vectors There are several vectors that are special in that they have unique properties. 3.6.1 Zero Vector One special vector is the zero vector. A zero vector (denoted 0) is a vector in which every element is 0. For example, the following vector is a zero vector: \\[ \\mathbf{0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] The length of a zero vector is 0. (Convince yourself of this!) Furthermore, since geometrically the head and tail of a zero vector coincide (are at the same location in the reference coordinate system) they have an undefinable direction. 3.6.2 Ones Vector Another special vector is referred to as a ones vector. A ones vector is a vector in which every elements is 1. For example, the following vector 1 is a ones vector: \\[ \\mathbf{1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\] The length of a ones vector is \\(\\sqrt{n}\\). (Convince yourself of this!) The direction of a ones vector is \\(45^\\circ\\) or 0.785 radians since the each element in the vector is equal. For example, locating the tail of a 2-dimensional ones vector at the origin, it would lie on the \\(R1=R2\\) line. Figure 3.7 shows a 2-dimensional ones vector with its tail located at the origin. Figure 3.7: Plot showing the ones vector (in red) in the R1–R2 dimensional space. The \\(R1=R2\\) line is also displayed. 3.6.3 Unit Vector Consider the n-element column vector u: \\[ \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ \\vdots \\\\ u_n \\end{bmatrix} \\] u is a unit vector if the length of the vector is 1. For example, each of the following vectors are unit vectors: \\[ \\mathbf{u}_1 = \\begin{bmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{bmatrix} \\qquad \\mathbf{u}_2 = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{u}_3 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Although the length of a unit vector must be 1, the direction is not specified. Thus there are an infinite number of unit vectors. Figure 3.8 shows several unit vectors with their tail located at the origin. Figure 3.8: Plot showing several unit vectors in the R1–R2 dimensional space. All vector tails have been located at the origin. 3.6.4 Elementary Vectors An elementary vector is a vector that has one element that is equal to one and the remainder of its elements equal to 0. For example, each of the 3-element column vectors below are elementary vectors. \\[ \\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] For an n-dimensional vector there are always n elementary vectors. Moreover, every elementary vector is also a unit vector. Geometrically, each elementary vector lies on one of the reference axes. Figure 3.9 shows the two elementary vectors (with their tail located at the origin) in our 2-dimensional reference coordinate system. Figure 3.9: Plot showing the two elementary vectors in the R1–R2 dimensional space. All vector tails have been located at the origin. 3.7 Exercises Create a 2-dimensional orthogonal reference system. Add the following vectors to that system. \\(\\mathbf{x} = \\begin{bmatrix}2 \\\\ -1 \\end{bmatrix}\\) Show/Hide Solution \\(\\mathbf{y} = \\begin{bmatrix}-3 \\\\ 2 \\end{bmatrix}\\) Show/Hide Solution A unit-vector z in which all the elements are equal. Show/Hide Solution \\[ \\mathbf{z} = \\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{bmatrix} \\] All elementary vectors. Show/Hide Solution \\[ \\mathbf{e}_1 = \\begin{bmatrix} 0 \\\\ 1\\end{bmatrix} \\quad \\mathbf{e}_2 = \\begin{bmatrix} 1 \\\\ 0\\end{bmatrix} \\] Compute the following: \\(\\lvert\\lvert\\mathbf{x}\\rvert\\rvert\\) Show/Hide Solution C \\[ \\begin{split} \\lvert\\lvert\\mathbf{x}\\rvert\\rvert &amp;= \\sqrt{(2)^2 + (-1)^2} \\\\[2ex] &amp;= \\sqrt{5} \\end{split} \\] \\(\\lvert\\lvert\\mathbf{y}\\rvert\\rvert\\) Show/Hide Solution C \\[ \\begin{split} \\lvert\\lvert\\mathbf{y}\\rvert\\rvert &amp;= \\sqrt{(-3)^2 + (2)^2} \\\\[2ex] &amp;= \\sqrt{13} \\end{split} \\] \\(\\lvert\\lvert\\mathbf{z}\\rvert\\rvert\\) Show/Hide Solution C \\[ \\begin{split} \\lvert\\lvert\\mathbf{z}\\rvert\\rvert &amp;= \\sqrt{\\bigg(\\frac{1}{\\sqrt{2}}\\bigg)^2 + \\bigg(\\frac{1}{\\sqrt{2}}\\bigg)^2} \\\\[2ex] &amp;= \\sqrt{\\frac{1}{2} + \\frac{1}{2}} \\\\[2ex] &amp;= \\sqrt{1} \\\\[2ex] &amp;= 1 \\end{split} \\] Remember, a unit-vector has a length of 1. The direction based on the angle (in radians) from the horizontal reference axis for x. Show/Hide Solution C \\[ \\begin{split} \\theta &amp;= 2\\pi - \\arccos(\\frac{2}{\\sqrt{5}}) \\\\[2ex] &amp;= 5.8195 \\mathrm{~radians} \\end{split} \\] The direction based on the angle (in degrees) from the horizontal reference axis for y. Show/Hide Solution C \\[ \\begin{split} \\theta &amp;= \\bigg(\\frac{\\pi}{2} + \\arctan(\\frac{3}{2})\\bigg) \\times \\frac{180}{\\pi} \\\\[2ex] &amp;= 146.31 \\mathrm{~degrees} \\end{split} \\] The direction based on the angle (in radians) from the horizontal reference axis for z. Show/Hide Solution C \\[ \\begin{split} \\theta &amp;= \\arctan\\bigg(\\frac{\\frac{1}{\\sqrt{2}}}{\\frac{1}{\\sqrt{2}}}\\bigg) \\\\[2ex] &amp;= \\arctan(1) \\\\[2ex] &amp;= 0.786 \\mathrm{~radians} \\end{split} \\] Indicate whether the vectors are equal or not equal. \\(\\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix} \\overset{?}{=} \\begin{bmatrix} 6 \\\\ 5 \\end{bmatrix}\\) Show/Hide Solution Not equal \\(\\begin{bmatrix} 55 \\\\ 66 \\\\ 48 \\end{bmatrix} \\overset{?}{=} \\begin{bmatrix} 55 &amp; 66 &amp; 48 \\end{bmatrix}\\) Show/Hide Solution Not equal Assume a and b are equal. Find the values for \\(a_1\\), \\(a_3\\), and \\(b_2\\). \\(\\mathbf{a} = \\begin{bmatrix} a_1 \\\\ 0 \\\\ a_3 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 9 \\\\ b_2 \\\\ 1 \\end{bmatrix}\\) Show/Hide Solution \\(a_1=9\\), \\(a_3=1\\), and \\(b_2=0\\) While it is common to express the direction as the measure of the angle between the horizontal reference axis, it could also be expressed as the measure of the angle between the vector and any of the reference axes. In more than two-dimensions, the direction would need to include multiple angles to specify the direction; you would need \\(n-1\\) angles to specify an n-dimensional vector.↩︎ You could also use the definition for sine or tangent to compute \\(\\theta\\).↩︎ "],["vecop.html", "Chapter 4 Vector Operations 4.1 Vector Addition and Subtraction 4.2 Vector–Scalar Multiplication 4.3 Vector–Vector Multiplication: Dot Product 4.4 Vector Operations Using R 4.5 Exercises", " Chapter 4 Vector Operations In this chapter you will learn about some of the common arithmetic operations (addition, subtraction, and multiplication) that can be performed with vectors. As in the previous chapter, you will also be introduced to the geometry of these vector operations in 2-dimensional space. 4.1 Vector Addition and Subtraction Vectors can be added together if they have the same dimensions (i.e., they have the same number of elements and are both row or column vectors). To add two vectors, each of length n, together, we sum the corresponding elements in the vectors. For example, consider the column vectors a and b, where, \\[ \\mathbf{a} = \\begin{bmatrix}5 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} \\] Since both vectors have dimensions \\(2 \\times 1\\), we can compute the sum of these vectors as: \\[ \\begin{split} \\mathbf{a} + \\mathbf{b} &amp;= \\begin{bmatrix}5 \\\\ 2\\end{bmatrix} + \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix}5 + 1 \\\\ 2 + 3\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix}6 \\\\ 5\\end{bmatrix} \\end{split} \\] Note that the resulting vector is a column vector with two elements, the same as the vectors we summed together. In general, if we sum two n-dimensional column vectors, x and y, the resulting vector will also be an n-dimensional column vector: \\[ \\begin{split} \\mathbf{x} + \\mathbf{y} &amp;= \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\\end{bmatrix} + \\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix}x_1 + y_1 \\\\ x_2 + y_2 \\\\ x_3 + y_3 \\\\ \\vdots \\\\ x_n + y_n\\end{bmatrix} \\end{split} \\] This process is similar for summing two n-dimensional row vectors, except the resulting vector will be an n-dimensional row vector. 4.1.1 Geometry of Adding Vectors Geometrically, adding two vectors, say a and b, is equivalent to drawing vector b so that its tail is placed at the head of vector a. Then the sum is the new vector that originates at vector a’s tail and terminates at vector b’s head. Figure 4.1 geometrically shows the sum of the following vectors: \\[ \\mathbf{a} = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} \\qquad \\mathbf{a}+\\mathbf{b} = \\begin{bmatrix} 6 \\\\ 5 \\end{bmatrix} \\] Figure 4.1: Plot showing the sum of vector a (in red) and vector b (in blue) in the R1–R2 dimensional space. For convenience we have located the tail of vector a at the origin. The vector that corresponds to the sum of a and b is shown in purple. 4.1.2 Vector Subtraction To subtract vector b from vector a, we subtract the corresponding elements in vector b from those in vector a. For example, working with our previously defined vectors, \\[ \\begin{split} \\mathbf{a} - \\mathbf{b} &amp; = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} 5-1 \\\\ 2 - 3 \\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} 4 \\\\ -1 \\end{bmatrix} \\end{split} \\] Since subtraction is equivalent to adding the inverse, subtracting the elements of b from a is equivalent to adding the inverted elements of b to the elements of a (where “inverting the elements” means switching the sign on each element). For example, \\[ \\begin{split} \\mathbf{a} - \\mathbf{b} &amp; = \\mathbf{a} + -\\mathbf{b} \\\\[2ex] &amp;= \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} -1 \\\\ -3 \\end{bmatrix} \\end{split} \\] Figure 4.2 geometrically shows the operation of vector subtraction by adding the inverse of b to a. Figure 4.2: Plot showing the sum of vector a (in red) and the inverted vector b (in blue) in the R1–R2 dimensional space. For convenience we have located the tail of vector a at the origin. The vector that corresponds to the difference of a and b is shown in purple. 4.1.3 Properties of Vector Addition and Subtraction In vector addition, since each corresponding element is added, vector addition satisfies both the commutative and associative properties. That is, \\[ \\mathbf{a} + \\mathbf{b} = \\mathbf{b} + \\mathbf{a} \\] and \\[ \\begin{split} \\mathbf{a} + (\\mathbf{b} + \\mathbf{c}) &amp;= (\\mathbf{a} + \\mathbf{b}) + \\mathbf{c} \\\\[2ex] &amp;= \\mathbf{a} + \\mathbf{b} + \\mathbf{c} \\end{split} \\] Convince yourself these two properties are satisfied using the following vectors. \\[ \\mathbf{a} = \\begin{bmatrix}5 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} \\qquad \\mathbf{c} = \\begin{bmatrix}2 \\\\ 0\\end{bmatrix} \\] Since we can re-write vector subtraction as vector addition, these same conditions and properties also apply for vector subtraction. 4.2 Vector–Scalar Multiplication When a vector is multiplied by a scalar, each element of the vector is multiplied by the value of the scalar. Consider the following vector a and scalar \\(\\lambda\\), \\[ \\mathbf{a} = \\begin{bmatrix}2 \\\\ 3 \\\\ 0 \\\\ -1\\end{bmatrix} \\qquad \\lambda=3 \\] Multiplying a by \\(\\lambda\\) gives: \\[ \\begin{split} \\lambda\\mathbf{a} &amp;= 3\\begin{bmatrix}2 \\\\ 3 \\\\ 0 \\\\ -1\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix}3 \\times 2 \\\\ 3 \\times 3 \\\\ 3 \\times 0 \\\\ 3 \\times -1\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix}6 \\\\ 9 \\\\ 0 \\\\ -3\\end{bmatrix} \\end{split} \\] This same method applies to row vectors. Scalars may be fractions, negative numbers, or unknowns. For example, if the scalar \\(\\gamma\\) is an unknown scalar, then \\[ \\gamma\\mathbf{a} = \\begin{bmatrix}2\\gamma \\\\ 3\\gamma \\\\ 0 \\\\ -1\\gamma\\end{bmatrix} \\] 4.2.1 Division by a Scalar Technically, division is undefined for vectors. However, if we are dividing by a scalar, we can multiply the vector by the scalar’s reciprocal. For example, using our previously defined vector a and scalar \\(\\lambda\\), \\[ \\mathbf{a} \\div \\lambda \\quad \\mathrm{does~not~exist} \\] But, we can multiply by the reciprocal of \\(\\lambda\\), namely \\(\\dfrac{1}{\\lambda}\\). For example, using the values from our previous example: \\[ \\begin{split} \\mathbf{a} = \\begin{bmatrix}2 \\\\ 3 \\\\ 0 \\\\ -1\\end{bmatrix} \\quad &amp;\\mathrm{and} \\quad \\lambda = 3 \\\\[2ex] \\frac{1}{\\lambda}\\mathbf{a} &amp;= \\frac{1}{3}\\begin{bmatrix}2 \\\\ 3 \\\\ 0 \\\\ -1\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} \\frac{2}{3} \\\\ 1 \\\\ 0 \\\\ -\\frac{1}{3}\\end{bmatrix} \\end{split} \\] 4.2.2 Geometry of Vector–Scalar Multiplication Consider the product \\(\\lambda \\mathbf{a}\\) where, \\[ \\lambda = 2 \\quad \\mathrm{and} \\quad \\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\] The product is \\[ \\begin{bmatrix} 4 \\\\ 6 \\end{bmatrix} \\] To show what happens geometrically when we multiply a vector by a scalar, let’s examine a plot of the original vector and the product. Figure 4.3: LEFT: Plot showing vector a in the R1–R2 dimensional space. RIGHT: Plot showing vector 2(a) in the R1–R2 dimensional space. For convenience we have located the tail of both vectors at the origin. Multiplying vector a by 2 doubled its length. However the direction of the new vector is the same as the original. In general, multiplying a vector by a positive scalar changes the length of a vector but not the direction. If it is multiplied by a negative scalar, not only does the length of the resulting vector change, but its direction is \\(180^\\circ\\) from the original. For example, Figure 4.4 shows vector a and the vector \\(\\lambda \\mathbf{a}\\) where \\(\\lambda = -0.5\\). Figure 4.4: LEFT: Plot showing vector a in the R1–R2 dimensional space. RIGHT: Plot showing vector -0.5(a) in the R1–R2 dimensional space. For convenience we have located the tail of both vectors at the origin. The product vector is half the length of the original and is pointed in the complete opposite direction. 4.3 Vector–Vector Multiplication: Dot Product For two column vectors, a and b each having n elements, the dot product (i.e., scalar product) is defined as: \\[ \\mathbf{a} \\bullet \\mathbf{b} = \\sum_{i=1}^n a_ib_i \\] In other words, the dot product is calculated by multiplying together the corresponding elements of each vector, and summing those products. Consider the vectors, \\[ \\mathbf{a} = \\begin{bmatrix}5 \\\\ 4 \\\\ 7 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}1 \\\\ 0 \\\\ -1 \\\\ 2\\end{bmatrix} \\] The dot product, or \\(\\mathbf{a} \\bullet \\mathbf{b}\\), is calculated as: \\[ \\begin{split} \\mathbf{a} \\bullet \\mathbf{b} &amp;= 5(1) + 4(0) + 7(-1) + 2(2) \\\\[2ex] &amp;= 2 \\end{split} \\] Remember that the result of a dot product is a scalar. 4.3.1 Re-visiting Vector Length Recall that to find the length of a vector a with n-dimensions: \\[ \\lvert\\lvert \\mathbf{a} \\rvert\\rvert = \\sqrt{a_1^2 + a_2^2 + a_3^2 + \\ldots + a_n^2} \\] The sum under the square root is equivalent to computing the dot product of a with itself (i.e., \\(\\mathbf{a} \\bullet \\mathbf{a}\\)). Therefore, the length of an n-dimensional vector a can be found by computing the square root of the dot product between a and itself: \\[ \\lvert\\lvert \\mathbf{a} \\rvert\\rvert = \\sqrt{\\mathbf{a} \\bullet \\mathbf{a}} \\] 4.3.2 Dot Products Using the Special Vectors In the previous chapter we introduced several special vectors, including zero vectors, ones vectors, and elementary vectors. It is useful to explore what happens when one of these special vectors is used to calculate an dot product. Consider finding the dot product between an n-dimensional vector a and an n-dimensional zero vector 0: \\[ \\begin{split} \\mathbf{a} \\bullet \\mathbf{0} &amp;= \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\bullet \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\\\[2ex] &amp;= a_1(0) + a_2(0) + a_3(0) + \\ldots + a_n(0) \\\\[2ex] &amp;= 0 \\end{split} \\] The dot product is 0. Now, consider finding the dot product between an n-dimensional vector a and an n-dimensional ones vector 1: \\[ \\begin{split} \\mathbf{a} \\bullet \\mathbf{1} &amp;= \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\bullet \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\\\[2ex] &amp;= a_1(1) + a_2(1) + a_3(1) + \\ldots + a_n(1) \\\\[2ex] &amp;= \\sum_{i=1}^n a_i \\end{split} \\] Notice that the dot product here is simply the sum of the elements in a. Because of this property, ones vectors are also sometimes referred to as sum vectors. Next, consider finding the dot product between an n-dimensional vector a and an n-dimensional elementary vector, say \\(\\mathbf{e}_1\\) in which the first element is 1 and the rest are 0: \\[ \\begin{split} \\mathbf{a} \\bullet \\mathbf{e}_1 &amp;= \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\bullet \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\\\[2ex] &amp;= a_1(1) + a_2(0) + a_3(0) + \\ldots + a_n(0) \\\\[2ex] &amp;= a_1 \\end{split} \\] Notice that the dot product here is simply the same as the first element in a. In general, the dot product between an n-element column vector, a, and an n-element elementary vector, \\(\\mathbf{e}_i\\) with element \\(i=1\\), is: \\[ \\mathbf{a} \\bullet \\mathbf{e}_i = a_i \\] 4.4 Vector Operations Using R We can also carry out these vector operations using R. Below we show how to carry out the operations of vector addition and vector subtraction. # Create vectors a = matrix(data = c(5, 2), ncol = 1) a [,1] [1,] 5 [2,] 2 b = matrix(data = c(1, 3), ncol = 1) b [,1] [1,] 1 [2,] 3 # Vector addition a + b [,1] [1,] 6 [2,] 5 # Vector subtraction a - b [,1] [1,] 4 [2,] -1 Multiplication of a vector by a scalar is carried out using the * operator. This operator carries out element-wise multiplication; in this case it multiplies each element in the vector by the given scalar. # Create vectors a = matrix(data = c(2, 3, 0, -1), ncol = 1) a [,1] [1,] 2 [2,] 3 [3,] 0 [4,] -1 # Multiplication by 3 3 * a [,1] [1,] 6 [2,] 9 [3,] 0 [4,] -3 # Division by 3 (this multiplies each element by the reciprical) a / 3 [,1] [1,] 0.6666667 [2,] 1.0000000 [3,] 0.0000000 [4,] -0.3333333 Lastly, although there is no dot product operator, we can mimic this function by using the elemement-wise multiplication operator to find the product of the corresponding elements of two vectors, and then use the sum() function to add those products together. # Create vectors a = matrix(data = c(5, 4, 7, 2), ncol = 1) a [,1] [1,] 5 [2,] 4 [3,] 7 [4,] 2 b = matrix(data = c(1, 0, -1, 2), ncol = 1) b [,1] [1,] 1 [2,] 0 [3,] -1 [4,] 2 # Element-wise multiplication a * b [,1] [1,] 5 [2,] 0 [3,] -7 [4,] 4 # Compute dot product sum(a * b) [1] 2 # Compute length of a using dot product sqrt(sum(a * a)) [1] 9.69536 4.5 Exercises Consider the following vectors: \\[ \\mathbf{a} = \\begin{bmatrix}1 \\\\ 2 \\\\ 4 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}2 \\\\ 1 \\\\ -1\\end{bmatrix} \\qquad \\mathbf{c} = \\begin{bmatrix}5 \\\\ 2 \\\\ 3 \\end{bmatrix} \\qquad \\mathbf{d} = \\begin{bmatrix}2 \\\\ 1 \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{e} = \\begin{bmatrix}1 \\\\ 3 \\\\ 2 \\end{bmatrix} \\] Find \\(\\mathbf{a}\\bullet\\mathbf{b}\\). Show/Hide Solution \\[ 1(2) + 2(1) + 4(-1) = 0 \\]$ Find \\(\\mathbf{c}\\bullet\\mathbf{c}\\). Show/Hide Solution \\[ 5(5) + 2(2) + 3(3) = 38 \\] Create a 2-dimensional orthogonal reference system. Show the following vector operations on that system. \\(\\mathbf{x}+\\mathbf{y}\\) where \\(\\mathbf{x} = \\begin{bmatrix}2 \\\\ -1 \\end{bmatrix}\\) and \\(\\mathbf{y} = \\begin{bmatrix}-3 \\\\ 2 \\end{bmatrix}\\) Show/Hide Solution \\[ \\begin{split} \\mathbf{x} + \\mathbf{y} &amp;= \\begin{bmatrix} 2 \\\\ -1\\end{bmatrix} + \\begin{bmatrix} -3 \\\\ 2\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} -1 \\\\ 1\\end{bmatrix} \\end{split} \\] \\(3\\mathbf{z}\\) where z is a unit vector in which all the elements are equal. Show/Hide Solution Students’ scores on two exams ranged from 32 to 98 out of a possible 100 points on both exams. The scores for six students are shown below for Exam 1 (x) and Exam 2 (y). \\[ \\mathbf{x} = \\begin{bmatrix}56 \\\\ 64 \\\\ 32 \\\\ 88 \\\\ 90 \\\\ 79\\end{bmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix}50 \\\\ 69 \\\\ 51 \\\\ 98 \\\\ 87 \\\\ 70\\end{bmatrix} \\] Determine the total score for each student by finding \\(\\mathbf{z} = \\mathbf{x} + \\mathbf{y}\\). Show/Hide Solution \\[ \\mathbf{z} = \\begin{bmatrix}56 + 50 \\\\ 64 + 69 \\\\ 32 + 51 \\\\ 88 + 98 \\\\ 90 + 87 \\\\ 79 + 70\\end{bmatrix} = \\begin{bmatrix}106 \\\\ 133 \\\\ 83 \\\\ 186 \\\\ 177 \\\\ 149\\end{bmatrix} \\] Determine the mean score for each student by finding \\(\\frac{1}{2} \\mathbf{z}\\). Show/Hide Solution \\[ \\frac{1}{2}\\mathbf{z} = \\frac{1}{2}\\begin{bmatrix}106 \\\\ 133 \\\\ 83 \\\\ 186 \\\\ 177 \\\\ 149\\end{bmatrix}= \\begin{bmatrix}53.0 \\\\ 66.5 \\\\ 41.5 \\\\ 93.0 \\\\ 88.5 \\\\ 74.5\\end{bmatrix} \\] If the students took a third exam, and the scores for the same six students were presented in vector w, how would you write the algebraic expression (using vector notation) to obtain the students’ mean scores for all three exams? Show/Hide Solution \\[ \\frac{1}{3}(\\mathbf{x} + \\mathbf{y} + \\mathbf{w}) \\] If the mean score on Exam 3 was 60, write the algebraic expression (using vector notation) to find the six students’ mean deviation scores on Exam 3. Show/Hide Solution \\[ \\mathbf{w} - \\begin{bmatrix}60 \\\\ 60 \\\\ 60 \\\\ 60 \\\\ 60 \\\\ 60\\end{bmatrix} \\] "],["vector-geometry-angles-projection-and-decomposition.html", "Chapter 5 Vector Geometry: Angles, Projection, and Decomposition 5.1 Angle Between Vectors 5.2 Orthogonal Projection 5.3 Orthogonal Decomposition", " Chapter 5 Vector Geometry: Angles, Projection, and Decomposition In this chapter you will learn some about some additional ideas in the geometry of vectors. Again, while the illustration of these concepts is restricted to 2-dimensional space, all of these ideas can be extended to n-dimensions. 5.1 Angle Between Vectors It can be quite useful to determine the angle between two vectors. For example, what is the angle between vector a and b where, \\[ \\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\quad \\mathrm{and} \\quad \\mathbf{b}= \\begin{bmatrix} -4 \\\\ 1 \\end{bmatrix} \\] Figure 5.1 shows both vectors displayed in the same two-dimensional reference coordinate system. Figure 5.1: Plot showing two vector a (in red) and b (in blue) in the R1–R2 dimensional space. The angle between them is denoted as \\(\\theta\\). For convenience we have located the tail of both vectors at the origin. The angle between these vectors is denoted as \\(\\theta\\), and can be found using the following: \\[ \\cos (\\theta) = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\] That is, the cosine of the angle between the two vectors is equal to the dot product of the vectors divided by the product of their lengths. To find the angle (\\(\\theta\\)), we can compute the arc-cosine of this ratio. In our example, \\[ \\begin{split} \\cos (\\theta) &amp;= \\frac{-5}{\\sqrt{13}\\times\\sqrt{17}} \\\\[2ex] &amp;= -0.336 \\\\[4ex] \\arccos(1.914) &amp;= 1.914 \\end{split} \\] and \\(\\theta=109.65^{\\circ}\\). Below is the R syntax to compute this angle. # Create vectors a = matrix(data = c(2, 3), ncol = 1) b = matrix(data = c(-4, 1), ncol = 1) # Compute dot product between a and b a_dot_b = sum(a * b) # Compute vector lengths l_a = sqrt(sum(a * a)) l_b = sqrt(sum(b * b)) # Compute theta (in radians) acos(a_dot_b / (l_a * l_b)) [1] 1.91382 # Compute theta (in degrees) acos(a_dot_b / (l_a * l_b)) * 180 / pi [1] 109.6538 Manipulating the formula to compute the angle between two vectors provides a common formula to determine the dot product between two vectors. \\[ \\begin{split} \\cos (\\theta) &amp;= \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\\\[2ex] \\mathbf{a}\\bullet\\mathbf{b} &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert \\cos(\\theta) \\end{split} \\] That is the dot product between vectors a and b is equal to the product of their magnitudes and the cosine of the angle between them. 5.1.1 Orthogonal Vectors Two vectors a and b are orthogonal when the angle between them is \\(90^\\circ\\). Since the cosine of a \\(90^\\circ\\) angle is 0, if a and b are orthogonal, then \\[ 0 = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\] For example, consider the following two elementary vectors \\[ \\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\] The dot product between these two vectors is 0, which implies that the cosine of the angle between them must also be 0, indicating that \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) are orthogonal. 5.1.2 Collinear Vectors Two vectors a and b are collinear when the angle between them is \\(0^\\circ\\). Since the cosine of a \\(0^\\circ\\) angle is 1, if a and b are collinear, then \\[ 1 = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\] For example, consider the following two vectors \\[ \\mathbf{a} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix} \\] \\[ \\begin{split} \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} &amp;= \\frac{14}{\\sqrt{5}\\times\\sqrt{45}} \\\\[2ex] &amp;= \\frac{14}{\\sqrt{225}} \\\\[2ex] &amp;= \\frac{14}{14} \\\\[2ex] &amp;= 1 \\end{split} \\] This implies that a and b are collinear. Two vectors are collinear when one can be written as a linear combination of the other. In our example, \\[ \\mathbf{b} = 3\\mathbf{a} \\] Geometrically, collinear vectors are parallel to one another (remember location in the reference space is a convenience). 5.2 Orthogonal Projection Orthogonal projection of vector a on vector b occurs by dropping a perpendicular line from the terminus of a to intersect with x2. Figure 5.2: Orthogonal projection of vector a (vermillion) onto vector b (black). Note that the projection creates a 90-degree angle with vector b. The result of the projection is the vector p (blue). The result is a vector p which is collinear with b but has a different length. To compute the length of p, we make use of the fact that the projection creates a right triangle having a hypotenuse of a and an adjacent leg of p to the angle \\(\\theta\\). Then, \\[ \\begin{split} \\cos(\\theta) &amp;= \\frac{\\lvert\\lvert\\mathbf{p}\\rvert\\rvert}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert} \\\\[2em] \\lvert\\lvert\\mathbf{p}\\rvert\\rvert &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\cos(\\theta) \\end{split} \\] Since \\(\\theta\\) is the angle between a and b, \\[ \\begin{split} \\lvert\\lvert\\mathbf{p}\\rvert\\rvert &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\cos(\\theta) \\\\[2em] &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert}\\\\[2em] &amp;= \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\end{split} \\] That is, the magnitude of the projection p, is the ratio of the dot product between vectors a and b to the magnitude of b. Consider the following two vectors: \\[ \\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} \\] Projecting a onto b, geometrically, Figure 5.3: Orthogonal projection of vector a (in red) onto vector b (in blue). The result of the projection is the vector p (in black). We can find the magnitude of p using our formula: \\[ \\begin{split} \\lvert\\lvert\\mathbf{p}\\rvert\\rvert &amp;= \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\\\[2ex] &amp;= \\frac{1}{\\sqrt{5}} \\\\[2ex] &amp;= 0.447 \\end{split} \\] # Create vectors a = c(2, 3) b = c(2, -1) # Compute dot product of a and b a_dot_b = sum(a * b) # Compute length of b l_b = sqrt(sum(b * b)) # Compute length of p l_p = a_dot_b / l_b l_p [1] 0.4472136 Henceforth, we will denote the projection of vector a onto vector b as: \\[ \\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}} \\] 5.3 Orthogonal Decomposition Orthogonal projection of a vector results in a geometric decomposition of the vector into two additive components. Figure 5.4 illustrates the decomposition of vector a into two additive components, \\(\\mathbf{p}_1\\) and \\(\\mathbf{p}_2\\). That is, \\[ \\mathbf{a} = \\mathbf{p}_1 + \\mathbf{p}_2 \\] Figure 5.4: Two orthogonal projections of vector a (vermillion). The first orthogonal projection is from vector a onto vector b (horizontal black) and the secondorthogonal projection is from vector a is onto vector o (vertical black). The result of the projections are the vectors \\(\\mathbf{p}_1\\) (blue horizontal) and \\(\\mathbf{p}_2\\) (blue vertical). The vector \\(\\mathbf{p}_1\\) is the same orthogonal projection from the earlier example, namely \\(\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}}\\). The vector \\(\\mathbf{p}_1\\) is a second projection, of a onto a vector o (\\(\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\)). The vector o is, by definition, orthogonal to b. The lengths of the two projections correspond to the lengths of the sides of the right triangle where the hypotenuse is a. Namely4, \\[ \\begin{split} \\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}}\\rvert\\rvert &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\cos(\\theta) \\\\[2em] \\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\rvert\\rvert &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\sin(\\theta) \\end{split} \\] As an example, we can decompose a Using our two previous example vectors: \\[ \\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} \\] We previously determined that \\(\\lvert\\lvert\\mathbf{a}\\rvert\\rvert=\\sqrt{13}\\), \\(\\lvert\\lvert\\mathbf{b}\\rvert\\rvert=\\sqrt{5}\\), and \\(\\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}}\\rvert\\rvert = 0.447\\). Recall, \\[ \\cos(\\theta) = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\] This implies that \\(\\cos(\\theta)=\\frac{1}{\\sqrt{65}}\\) and, taking the arc-cosine, that \\(\\theta = 82.87^\\circ\\) (or 1.45 radians). Using this value, we can compute the magnitude of the second projection as: \\[ \\begin{split} \\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\rvert\\rvert &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\sin(\\theta) \\\\[2em] &amp;= \\sqrt{13} \\times \\sin(82.87^\\circ) \\\\[2em] &amp;= 3.58 \\end{split} \\] # Compute length of a l_a = sqrt(sum(a * a)) # Compute theta (in radians) theta = acos(a_dot_b / (l_a * l_b)) theta [1] 1.446441 # Compute length of projection of a onto o l_p2 = l_a * sin(theta) l_p2 [1] 3.577709 We can use the Pythagorean theorem to verify the computation of the two projections’ lengths. Since the square of the hypotenuse of a right triangle is the sum of the squares of the sides, \\[ \\begin{split} \\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}}\\rvert\\rvert^2 +\\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\rvert\\rvert^2 &amp;= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert^2 \\\\[2em] 3.58^2 + 0.447^2 &amp;= (\\sqrt{13})^2 \\\\[2em] 13 &amp;= 13 \\end{split} \\] # Compute sum of the squared projection lengths l_p^2 + l_p2^2 [1] 13 # Compute length of a squared l_a^2 [1] 13 # Check values using Pythagorean Theorem l_a^2 == l_p^2 + l_p2^2 [1] TRUE The final system is shown in Figure 5.5. Figure 5.5: Orthogonal projection of vector a (in red) onto vector b (in blue). The result of the projection is the vector \\(\\mathbf{p}_1\\) (in black). A second projection is the vector \\(\\mathbf{p}_2\\) (in black) on to the vector o, which is orthogonal to b. There are other ways to compute the length of \\(\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\). For example, since o is orthogonal to b, the angle between o and a is \\(\\phi=90-\\theta\\). Then \\(\\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\rvert\\rvert=\\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\cos(\\phi)\\).↩︎ "],["statistical-application-vectors.html", "Chapter 6 Statistical Application: Vectors 6.1 Deviation Scores and the Standard Deviation 6.2 Vector Correlation and Separation 6.3 Orthogonal Decomposition and Bivariate Regression", " Chapter 6 Statistical Application: Vectors In this chapter, we will provide examples of how vectors define the underlying geometry of various statistical summaries (standard deviation and correlation coefficient), including linear models. We will provide an example using a single predictor, but again, the ideas can be extended to models that include multiple predictors. 6.1 Deviation Scores and the Standard Deviation Consider the following vector scores (x) and the vector of the mean deviation scores (\\(\\mathbf{d_x}\\)): \\[ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\\end{bmatrix} \\qquad \\mathbf{d_x} = \\begin{bmatrix} x_1-\\bar{x} \\\\ x_2-\\bar{x} \\\\ x_3-\\bar{x} \\\\ \\vdots \\\\ x_n-\\bar{x}\\end{bmatrix} \\] Remember that the length of a vector is the square root of the dot product of the vector with itself. If we were to compute the length of the deviation score vector, the length would be the square root of the sum of squared deviations: \\[ \\sqrt{\\mathbf{d_x}\\bullet \\mathbf{d_x}} = \\sqrt{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + (x_3 - \\bar{x})^2 + \\ldots + (x_n - \\bar{x})^2} \\] If we divided this result by \\(\\sqrt{n}\\), this would be equivalent to the standard deviation of the scores in x (\\(s_\\mathbf{x}\\)). \\[ s_\\mathbf{x} = \\frac{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert}{\\sqrt{n}} \\] So there is a direct relation between the length of a deviation vector and the standard deviation, namely, \\[ \\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert = \\sqrt{n} (s_\\mathbf{x}) \\] Consider the following vectors of data representing SAT scores (x) and GPA values (y) for \\(n=10\\) student: \\[ \\mathbf{x} = \\begin{bmatrix}760 \\\\ 710 \\\\ 680 \\\\ 730 \\\\ 420 \\\\ 410 \\\\ 620 \\\\ 630 \\\\ 720 \\\\ 670\\end{bmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix} 3.8 \\\\ 2.4 \\\\ 2.6 \\\\ 3.1 \\\\ 1.9 \\\\ 1.7 \\\\ 2.5 \\\\ 2.4 \\\\ 3.5 \\\\ 3.1\\end{bmatrix} \\] We compute the mean scores for each vector as \\(\\bar{x}=635\\) and \\(\\bar{y}=2.7\\), respectively. By subtracting the mean (which is a scalar) from each vector of data, we can create deviation vectors: \\[ \\mathbf{d_x} = \\mathbf{x} - 635 = \\begin{bmatrix}125 \\\\ 75 \\\\ 45 \\\\ 95 \\\\ -215 \\\\ -225 \\\\ -15 \\\\ -5 \\\\ 85 \\\\ 35\\end{bmatrix} \\qquad \\mathbf{d_y} =\\mathbf{y}-2.7 = \\begin{bmatrix}1.1 \\\\ -0.3 \\\\ -0.1 \\\\ 0.4 \\\\ -0.8 \\\\ -1.0 \\\\ -0.2 \\\\ -0.3 \\\\ 0.8 \\\\ 0.4\\end{bmatrix} \\] The length of the deviation vectors are computed as \\(\\sqrt{\\mathbf{d}\\bullet\\mathbf{d}}\\), namely, \\[ \\begin{split} \\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert &amp;= \\sqrt{\\mathbf{d_x}\\bullet\\mathbf{d_x}} \\\\[2em] \\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert &amp;= \\sqrt{\\mathbf{d_y}\\bullet\\mathbf{d_y}} \\end{split} \\] And using the values in the deviation vectors, \\[ \\begin{split} \\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert &amp;= \\sqrt{137850} = 371.28 \\\\[2em] \\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert &amp;= \\sqrt{4.04} = 2.01 \\end{split} \\] Finally, we can compute the standard deviations for x and y by dividing these lengths by \\(\\sqrt{n}\\). \\[ \\begin{split} s_\\mathbf{x} &amp;= \\frac{371.28}{\\sqrt{10}} = 117.41 \\\\[2em] s_\\mathbf{y} &amp;= \\frac{4.04}{\\sqrt{10}} = 0.64 \\end{split} \\] # Original vectors x = matrix(data = c(760, 710, 680, 730, 420, 410, 620, 630, 720, 670), ncol = 1) y = matrix(data = c(3.8, 2.4, 2.6, 3.1, 1.9, 1.7, 2.5, 2.4, 3.5, 3.1), ncol = 1) # Compute deviation vectors d_x = x - mean(x) d_x [,1] [1,] 125 [2,] 75 [3,] 45 [4,] 95 [5,] -215 [6,] -225 [7,] -15 [8,] -5 [9,] 85 [10,] 35 d_y = y - mean(y) d_y [,1] [1,] 1.1 [2,] -0.3 [3,] -0.1 [4,] 0.4 [5,] -0.8 [6,] -1.0 [7,] -0.2 [8,] -0.3 [9,] 0.8 [10,] 0.4 # Compute lengths of x deviation vector l_x = sqrt(sum(d_x * d_x)) l_x [1] 371.2816 # Compute lengths of y deviation vector l_y = sqrt(sum(d_y * d_y)) l_y [1] 2.009975 # Compute sd of x s_x = l_x / sqrt(10) s_x [1] 117.4095 # Compute sd of y s_y = l_y / sqrt(10) s_y [1] 0.6356099 Note that if we are using \\(s_\\mathbf{x}\\) as an estimate for the population parameter \\(\\sigma_\\mathbf{x}\\), that is x is a sample of student scores, then we need to divide the length of vector x by \\(n-1\\) rather than \\(n\\). In that case, \\[ \\begin{split} \\hat\\sigma_\\mathbf{x} &amp;= \\frac{371.28}{\\sqrt{9}} = 123.76 \\\\[2em] \\hat\\sigma_\\mathbf{y} &amp;= \\frac{4.04}{\\sqrt{9}} = 0.67 \\end{split} \\] This is the denominator that is used in the sd() function in R. # Compute sd of x sigma_x = l_x / sqrt(9) sigma_x [1] 123.7605 # Compute sd of y sigma_y = l_y / sqrt(9) sigma_y [1] 0.6699917 # Check results sd(x) [1] 123.7605 sd(y) [1] 0.6699917 6.2 Vector Correlation and Separation The correlation between two vectors can also be expressed in terms of deviation scores: \\[ \\begin{split} r_{xy} &amp;= \\frac{\\mathrm{Cov}_{\\mathbf{xy}}}{s_\\mathbf{x} s_\\mathbf{y}} \\\\[2em] &amp;= \\frac{\\frac{\\sum (\\mathbf{x}-\\bar{x})(\\mathbf{y}-\\bar{y})}{n}}{s_\\mathbf{x} s_\\mathbf{y}} \\end{split} \\] The expression \\(\\sum (x-\\bar{x})(y-\\bar{y})\\) is equivalent to \\(\\mathbf{x}\\bullet\\mathbf{y}\\). Furthermore, we can re-write the standard deviations using our previous relationship with length of the deviation vectors. This implies, \\[ \\begin{split} r_{xy} &amp;= \\frac{\\frac{\\mathbf{d_x}\\bullet\\mathbf{d_y}}{n}}{\\frac{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert}{\\sqrt{n}}\\frac{\\lvert\\lvert\\mathbf{d_y}\\rvert\\rvert}{\\sqrt{n}}} \\\\[2em] &amp;= \\frac{\\frac{\\mathbf{d_x}\\bullet\\mathbf{d_y}}{n}}{\\frac{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert~\\lvert\\lvert\\mathbf{d_y}\\rvert\\rvert}{n}} \\\\[2em] &amp;= \\frac{\\mathbf{d_x}\\bullet\\mathbf{d_y}}{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert~\\lvert\\lvert\\mathbf{d_y}\\rvert\\rvert} \\end{split} \\] That is, the correlation between x and y is equal to the ratio between the dot product of the deviation vectors and the product of their lengths.5 In our example, # Compute correlation sum(d_x * d_y) / (l_x * l_y) [1] 0.8468822 # Check with correlation function cor(x, y) [,1] [1,] 0.8468822 Recall that the angle between two vectors a and b, denoted \\(\\theta_{\\mathbf{ab}}\\), is given by, \\[ \\cos (\\theta_{\\mathbf{ab}}) = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\] Thus to compute the angle between the two deviation vectors (\\(\\theta\\)), this is: \\[ \\cos (\\theta) = \\frac{\\mathbf{d_x}\\bullet\\mathbf{d_y}}{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{d_y}\\rvert\\rvert} \\] This is the same as the formula for the correlation! In other words, \\[ r_{\\mathbf{xy}} = \\cos (\\theta) \\] The correlation between two vectors x and y is equal to the cosine of the angle between their deviation vectors. Using this equality, we can find the angle between the deviation vectors for our example SAT scores and GPA values. Since \\(r_{\\mathbf{xy}}=0.847\\), that implies \\(\\theta\\approx32^\\circ\\). # Compute theta (in radians) acos(0.847) [1] 0.5604801 # Compute theta (in degrees) acos(0.847) * 180 / pi [1] 32.11314 This correlation corresponds to an angle of approximately \\(32^\\circ\\) of separation between the deviation score vectors for SAT scores and GPA values in the 10-dimensional space defined by our 10 students. We can use the fact that \\(r_{\\mathbf{xy}} = \\cos (\\theta)\\) to make some connections between the value of the correlation coefficient and the angle between the deviation vectors. Perfect positive correlation (\\(r_{\\mathbf{xy}}=1\\)) indicates that deviation vectors are collinear. In this case \\(\\cos(\\theta)=1\\) which implies that \\(\\theta=0^\\circ\\). Perfect negative correlation (\\(r_{\\mathbf{xy}}=-1\\)) indicates that deviation vectors are in opposite directions. In this case \\(\\cos(\\theta)=-1\\) which implies that \\(\\theta=180^\\circ\\). Perfect lack of correlation (\\(r_{\\mathbf{xy}}=0\\)) indicates that deviation vectors are orthogonal. In this case \\(\\cos(\\theta)=0\\) which implies that \\(\\theta=90^\\circ\\). 6.3 Orthogonal Decomposition and Bivariate Regression Recall that the simple regression model seeks to explain variation in an outcome variable Y using a predictor variable X. Mathematically, the model fitted is expressed as: \\[ \\mathbf{y} = b_0 + b_1(\\mathbf{x}) + \\mathbf{e} \\] where y is a vector of fitted values, \\(b_0\\) and \\(b_1\\) are scalars produced from the OLS estimation, x is a vector of the predictor values, and e is a vector of residuals. Recall that when we have mean centered the outcome and predictor, the intercept (\\(b_0\\)) drops out of this equation. we can express this as: \\[ \\mathbf{y} - \\bar{y} = b_1(\\mathbf{x} - \\bar{x}) + \\mathbf{e} \\] The outcome and predictors are now expressed as deviation vectors, say \\[ \\mathbf{d}_\\mathbf{y} = b_1(\\mathbf{d}_\\mathbf{x} ) + \\mathbf{e} \\] where \\(\\hat{\\mathbf{d}}_\\mathbf{y}=b_1(\\mathbf{d}_\\mathbf{x} )\\). That is, we can partition the deviation vector of y into two components: \\[ \\mathbf{d}_\\mathbf{y} = \\hat{\\mathbf{d}}_\\mathbf{y} + \\mathbf{e} \\] Ordinary least squares (OLS) estimation determines the value of \\(b_1\\) by minimizing the sum of squared residuals, that is, it minimizes the quantity \\(\\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2\\). Geometrically, minimizing \\(\\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2\\) is determining the orthogonal projection of \\(\\mathbf{d}_\\mathbf{y}\\) onto \\(\\mathbf{d}_\\mathbf{x}\\).6 This projection is the first component of the partitioning of the deviation vector described previously, \\(\\hat{\\mathbf{d}}_\\mathbf{y}=b_1(\\mathbf{d}_\\mathbf{x})\\), which is collinear with \\(\\mathbf{d}_\\mathbf{x}\\) since \\(b_1(\\mathbf{d}_\\mathbf{x})\\) is a scalar multiple of \\(\\mathbf{d}_\\mathbf{x}\\) This is shown in Figure 6.1. Figure 6.1: The two orthogonal projections from the deviation vector of y form the basis for the model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the e vector. The ‘triangle’ formed by the vectors \\(\\hat{\\mathbf{d}}_\\mathbf{y}\\), e, and \\(\\mathbf{d}_\\mathbf{y}\\) is referred to as the model triangle. Figure 6.2 shows the model triangle. (Remember e can be moved to the right-side of the triangle since location is not a vector property.) Figure 6.2: The model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the e vector. The geometry of this triangle is the same as the geometry visualizing the sum of these vectors, namely \\[ \\mathbf{d}_\\mathbf{y} = \\hat{\\mathbf{d}}_\\mathbf{y} + \\mathbf{e} \\] Namely that the vectors that create the legs of the model triangle can be added together to create the \\(\\mathbf{d}_\\mathbf{y}\\) hypotenuse vector. The e vector is also the second orthogonal projection vector, \\(\\mathbf{e} = \\mathbf{p}_{\\mathbf{d}_\\mathbf{y}\\perp\\mathbf{o}}\\). By definition, this means that the vector of residuals (e) is orthogonal to the vector of fitted values (\\(\\hat{\\mathbf{d}}_\\mathbf{y}\\)), which means, the correlation between those two vectors is zero. \\[ r_{\\mathbf{e},\\hat{\\mathbf{d}}_\\mathbf{y}} = 0 \\] It also means that the model triangle is a right triangle, whose side lengths are governed by the Pythagorean Theorem. \\[ \\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert^2 = \\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert^2 + \\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2 \\] Expressing these lengths using the deviations we get \\[ \\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 + \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\] This is the partitioning that describes the ANOVA decomposition: \\[ \\mathrm{Total~SS} = \\mathrm{Model~SS} + \\mathrm{Residual~SS} \\] Each of these sum of squares is the squared length of one of the vectors in the model triangle. \\[ \\begin{split} \\mathrm{Total~SS} &amp;= \\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert^2 \\\\[2em] \\mathrm{Model~SS} &amp;= \\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert^2 \\\\[2em] \\mathrm{Residual~SS} &amp;= \\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2 \\end{split} \\] Relatedly, the lengths of the vectors making up the model triangle are the square roots of the these sum of square terms: \\[ \\begin{split} \\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert &amp;= \\sqrt{\\mathrm{Total~SS}} \\\\[2em] \\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert &amp;= \\sqrt{\\mathrm{Model~SS}} \\\\[2em] \\lvert\\lvert\\mathbf{e}\\rvert\\rvert &amp;= \\sqrt{\\mathrm{Residual~SS}} \\end{split} \\] Figure 6.3: The side lengths of the model triangle (yellow) correspond to the square roots of the sum of square terms used in the ANOVA decomposition. Lastly, since the model-level \\(R^2\\) value is defined as \\(R^2 = \\frac{\\mathrm{Model~SS}}{\\mathrm{Total~SS}}\\) and there is only a single predictor in the model then, \\[ \\begin{split} r_{\\mathbf{xy}} &amp;= \\sqrt{R^2} \\\\[2em] &amp;= \\sqrt{\\frac{\\mathrm{Model~SS}}{\\mathrm{Total~SS}}} \\\\[2em] &amp;= \\frac{\\sqrt{\\mathrm{Model~SS}}}{\\sqrt{\\mathrm{Total~SS}}} \\\\[2em] &amp;= \\frac{\\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert}{\\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert} \\end{split} \\] That is, the correlation coefficient between x and y is equivalent to the ratio of the lengths between the orthogonal projection vector collinear with the deviation vector of x and the deviation vector of y. Note this is also exactly how we compute cosine of \\(\\theta\\). 6.3.1 Back to the SAT and GPA Example Here we return to our GPA and SAT data to provide an example of these computations. Suppose we want to predict SAT (y) from GPA (x), employing deviation scores. We begin by computing the length of both deviation vectors: \\[ \\begin{split} \\lvert\\lvert\\mathbf{d}_\\mathbf{x}\\rvert\\rvert &amp;= \\sqrt{137850} = 371.28\\\\[2em] \\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert &amp;= \\sqrt{4.04} = 2.01 \\end{split} \\] # Compute length of deviation vector d_x sqrt(sum(l_x * l_x)) [1] 371.2816 # Compute length of deviation vector d_y sqrt(sum(l_y * l_y)) [1] 2.009975 We can also compute the correlation coefficient between SAT scores and GPAs by finding the cosine of the angle between the two vectors, \\(r = 0.847\\). Taking the arc-cosine, we find that \\(\\theta = 32.12^\\circ\\). # Compute correlation r = sum(d_x * d_y) / (l_x * l_y) r [1] 0.8468822 Figure 6.4: The model triangle (yellow) for our regression of GPA (y) onto SAT scores (x). We can now use the definition of cosine and sine to compute the lengths of the two orthogonal projections from \\(\\mathbf{d}_\\mathbf{y}\\). The length of the projection onto \\(\\mathbf{d}_\\mathbf{x}\\) is calculated as: \\[ \\begin{split} \\cos(\\theta) &amp;= \\frac{\\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert}{\\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert} \\\\[2em] 0.847 &amp;= \\frac{\\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert}{2.01} \\\\[2em] \\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert &amp;= 1.70 \\end{split} \\] The length of the second projection onto o is calculated as: \\[ \\begin{split} \\sin(\\theta) &amp;= \\frac{\\lvert\\lvert\\mathbf{e}\\rvert\\rvert}{\\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert} \\\\[2em] 0.531 &amp;= \\frac{\\lvert\\lvert\\mathbf{e}\\rvert\\rvert}{2.01} \\\\[2em] \\lvert\\lvert\\mathbf{e}\\rvert\\rvert &amp;= 1.07 \\end{split} \\] # Compute length of projection on d_x r * l_y [1] 1.702212 # Compute length of projection on o sin(acos(r)) * l_y [1] 1.068866 Figure 6.5 shows the model triangle for the regression of GPA (y) onto SAT scores (x) with all of the computed side lengths. Figure 6.5: The model triangle (yellow) for our regression of GPA (y) onto SAT scores (x). Finally, we can use these lengths, along with the length of \\(\\mathbf{d}_\\mathbf{y}\\) to write out the ANOVA decomposition; the partitioning of the sum of squares. \\[ \\begin{split} \\mathrm{Total~SS} &amp;= \\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert^2 = 2.01^2 = 4.04\\\\[2em] \\mathrm{Model~SS} &amp;= \\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert^2 = 1.70^2 = 2.90\\\\[2em] \\mathrm{Residual~SS} &amp;= \\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2 = 1.07^2 = 1.14 \\end{split} \\] These sums of squares are additive within rounding: \\[ \\begin{split} \\mathrm{Total~SS} &amp;= \\mathrm{Model~SS} + \\mathrm{Residual~SS} \\\\[2em] 4.04 &amp;= 2.90 + 1.14 \\end{split} \\] Using these values we can also compute the model-level \\(R^2\\). \\[ R^2 = \\frac{\\mathrm{Model~SS}}{\\mathrm{Total~SS}} = \\frac{2.90}{4.04} = 0.718 \\] That is, differences in SAT scores explain 71.8% of the variation in GPAs. Most formulas for the correlation will use \\(n-1\\) rather than \\(n\\), but here it doesn’t matter as the \\(n\\)s drop out when we reduce this.↩︎ Remember from geometry that the shortest distance from a point (at the end of the \\(\\mathbf{d}_\\mathbf{y}\\)) to a line (spanned by \\(\\mathbf{d}_\\mathbf{x}\\)) is the perpendicular line segment between them.↩︎ "],["matrices-1.html", "Chapter 7 Matrices 7.1 Matrix Equality 7.2 Exercises", " Chapter 7 Matrices A matrix is a rectangular array of elements arranged in rows and columns. We typically denote matrices using a bold-faced, upper-case letter. For example, consider the matrix B which has 3 rows and 2 columns: \\[ \\mathbf{B} = \\begin{bmatrix} 5 &amp; 1 \\\\ 7 &amp; 3 \\\\ -2 &amp; -1 \\end{bmatrix} \\] In psychometric and statistical applications, the data we work with typically have this type of rectangular arrangement. For example, the data in 7.1, which includes measures of 5 variables for 100 students, is arranged into the familiar case-by-variable rectangular ‘data matrix.’7 Table 7.1: Example set of education data. The data are rectangular, having rows (cases) and columns (variables). ID SAT GPA IQ School 1 560 3.0 112 Public 2 780 3.9 143 Public 3 620 2.9 124 Private \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) 100 600 2.7 129 Public 7.0.1 Dimensions of a Matrix We define matrices in terms of their dimensions or order; the number of rows and columns within the matrix. The dimensions of the matrix B is \\(3\\times 2\\); it has three rows and two columns. Whereas the dimensions of the data matrix is \\(100\\times 5\\). We often denote the matrix’s dimension by appending it to the bottom of the matrix’s name, \\[ \\underset{3\\times 2}{\\mathbf{B}} = \\begin{bmatrix} 5 &amp; 1 \\\\ 7 &amp; 3 \\\\ -2 &amp; -1 \\end{bmatrix} \\] 7.0.2 Indexing Individual Elements The elements within the matrix are indexed by their row number and column number. For example, \\(\\mathbf{B}_{1,2} = 1\\) since the element in the first row and second column is 1. The subscripts on each element indicate the row and column positions of the element. Vectors are a special case of a matrix, where a row vector is a \\(1\\times n\\) matrix, and a column vector is a \\(n \\times 1\\) matrix. Note that although a vector can always be described as a matrix, a matrix cannot always be described as a vector. In general, we define matrix A, which has n rows and k columns as: \\[ \\underset{n\\times k}{\\mathbf{A}} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\ldots &amp; a_{1k} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\ldots &amp; a_{2k} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; \\ldots &amp; a_{3k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\ldots &amp; a_{nk} \\end{bmatrix} \\] where element \\(a_{ij}\\) is in the \\(i^{\\mathrm{th}}\\) row and \\(j^{\\mathrm{th}}\\) column of A. 7.0.3 Geometry of Matrices A matrix is an k-dimensional representation where k is the number of columns in the matrix. For example, consider the matrix M: \\[ \\mathbf{M} = \\begin{bmatrix} 1 &amp; 3 \\\\ 2 &amp; -2 \\end{bmatrix} \\] Since M has two columns, this matrix represents a 2-dimensional structure (i.e., a plane). Each column in the matrix corresponds to a vector in the coordinate system that helps define that structure. M is defined by the space encompassed by the vectors \\((1, 2)\\) and \\((3, -2)\\). Figure 7.1 shows matrix M plotted in the 2-dimensional coordinate space defined by R1 and R2. Figure 7.1: Matrix M (yellow) in the 2-dimensional coordinate system defined by R1 and R2. Because most matrices have much higher dimensions, we typically do not visualize them geometrically. For additional detail on the geometry of matrices, see YouTube content creator 3Blue1Brown’s excellent playlist, Essence of Linear Algebra. 7.0.4 Matrices in R To enter a matrix in R, we use the matrix() function, similar to creating a vector. The only difference is that the nrow= or ncol= argument will include a value other than 1. The elements of the matrix given in the data= argument, by default, will be filled-in by columns. For example to create matrix B from the earlier example, we can use the following syntax. # Create B by filling in columns B = matrix( data = c(5, 7, -2, 1, 3, -1), nrow = 3, ncol = 2 ) # Display B B [,1] [,2] [1,] 5 1 [2,] 7 3 [3,] -2 -1 The byrow=TRUE argument will fill the elements by rows rather than columns. # Create B by filling in rows B = matrix( data = c(5, 1, 7, 3, -2, -1), byrow = TRUE, nrow = 3 ) # Display B B [,1] [,2] [1,] 5 1 [2,] 7 3 [3,] -2 -1 The dim() function can be used to return the dimensions of a matrix. # Get matrix dimensions dim(B) [1] 3 2 Lastly, we can index elements of a matrix by specifying the row and column number of the matrix object for the element we want in square brackets. These values are separated by a comma. For example, to index the element in B that is in the 2nd row and 1st column, we use the following syntax: # Index the element in the 2nd row, 1st column B[2, 1] [1] 7 7.1 Matrix Equality Two matrices are said to be equal if they satisfy two conditions: They have the same dimensions, and All corresponding elements are equal. Consider the following matrices: \\[ \\mathbf{A} = \\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix} 112 &amp; 134 \\\\86 &amp; 94 \\\\ 0 &amp; 0 \\end{bmatrix} \\qquad \\mathbf{C} = \\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} \\] Within this set of matrices, \\(\\mathbf{A} \\neq \\mathbf{B}\\), since they do not have the same dimensions. A is a \\(2\\times3\\) matrix and B is a \\(3\\times2\\) matrix. Similarly, \\(\\mathbf{A} \\neq \\mathbf{B}\\), they also do not have the same dimensions. \\(\\mathbf{A} = \\mathbf{C}\\) since both matrices have the same dimensions and all corresponding elements are equal. 7.2 Exercises Consider the following matrices: \\[ \\mathbf{A} = \\begin{bmatrix}5 &amp; 6 &amp; 4&amp; 5 &amp; 9 \\\\21 &amp; 23 &amp; 24 &amp; 22 &amp; 20 \\end{bmatrix} \\qquad \\mathbf{b}^\\intercal = \\begin{bmatrix}0 &amp; 1 &amp; 0 &amp; 0 \\end{bmatrix} \\qquad \\mathbf{Y} = \\begin{bmatrix}2 &amp; 3 &amp; 1 \\\\5 &amp; 6 &amp; 8\\\\9 &amp; 4 &amp; 7 \\end{bmatrix} \\] Use notation to describe the dimensions of A. Show/Hide Solution \\[ \\underset{2\\times5}{\\mathbf{A}} \\] Use notation to describe the dimensions of b. Show/Hide Solution \\[ \\underset{4\\times1}{\\mathbf{b}} \\] Use notation to describe the dimensions of Y. Show/Hide Solution \\[ \\underset{3\\times3}{\\mathbf{Y}} \\] The matrix X contains data from four students’ of four different course exams. \\[ \\mathbf{X} = \\begin{bmatrix}32 &amp; 54 &amp; 56 &amp; 21 \\\\42 &amp; 23 &amp; 52 &amp; 35 \\\\ 16 &amp; 41 &amp; 54 &amp; 56 \\\\ 58 &amp; 52 &amp; 31 &amp; 24 \\end{bmatrix} \\] Which student obtained a 35 and on which test? Report the row and column using subscript notation. Show/Hide Solution \\[ X_{2,4}=35 \\] The 2nd student received a 35 on the 4th exam. Using subscript notation, indicate the 4th student’s score on the 1st exam. Show/Hide Solution \\[ X_{4,1}=58 \\] In computation, a matrix is a very particular type of data structure in which every column has the same type of data (e.g., every column is numeric, or every column is a character variable). A data frame is a more generalized computational structure that accommodates multiple types of columns. In some computational languages this structure is referred to as a data matrix.↩︎ "],["matrix-addition-and-subtraction.html", "Chapter 8 Matrix Addition and Subtraction 8.1 Properties of Matrix Addition 8.2 Statistical Application: Deviation Matrix", " Chapter 8 Matrix Addition and Subtraction Two or more matrices can be added or subtracted if they have the same dimensions; if not, matrix addition and subtraction is undefined. Just as in vector addition, each corresponding element is added or subtracted and placed in the corresponding location in the new matrix. Consider the following two matrices: \\[ \\mathbf{A} = \\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix} 101 &amp; 89 &amp; 1 \\\\110 &amp; 90 &amp; 0 \\end{bmatrix} \\] Then \\[ \\begin{split} \\mathbf{A} + \\mathbf{B} &amp;= \\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} + \\begin{bmatrix} 101 &amp; 89 &amp; 1 \\\\110 &amp; 90 &amp; 0 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix} 112 + 101 &amp; 86+89 &amp; 0+1 \\\\ 134+110 &amp; 94+90 &amp; 0+0 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix} 213 &amp; 175 &amp; 1 \\\\ 244 &amp; 184 &amp; 0 \\end{bmatrix} \\end{split} \\] In general, the elements in the summed matrix, C, are defined as \\(\\mathbf{C}_{ij}=\\mathbf{A}_{ij} + \\mathbf{B}_{ij}\\) for all i and j. Since subtraction is equivalent to adding the inverse, subtracting the elements of B from A is equivalent to adding the inverted elements of B to the elements of A (where “inverting the elements” means switching the sign on each element). \\[ \\begin{split} \\mathbf{A} - \\mathbf{B} &amp;= \\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} + \\begin{bmatrix} -101 &amp; -89 &amp; -1 \\\\-110 &amp; -90 &amp; 0 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix} 11 &amp; -3 &amp; -1 \\\\ 24 &amp; 4 &amp; 0 \\end{bmatrix} \\end{split} \\] Computationally, we can use the + and - operators in R to add and subtract matrices. # Create A A = matrix(data = c(112, 86, 0, 134, 94, 0), byrow = TRUE, nrow = 2) # Create B B = matrix(data = c(101, 89, 1, 110, 90, 0), byrow = TRUE, nrow = 2) # Matrix addition A + B [,1] [,2] [,3] [1,] 213 175 1 [2,] 244 184 0 # Matrix Subtraction A - B [,1] [,2] [,3] [1,] 11 -3 -1 [2,] 24 4 0 8.1 Properties of Matrix Addition Matrix addition satisfies both the commutative and associative properties. That is, \\[ \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A} \\] and \\[ \\begin{split} \\mathbf{A} + (\\mathbf{B} + \\mathbf{C}) &amp;= (\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} \\\\[2ex] &amp;= \\mathbf{A} + \\mathbf{B} + \\mathbf{C} \\end{split} \\] 8.2 Statistical Application: Deviation Matrix We have seen that deviation scores are particularly useful in statistics. We can create a deviation matrix by taking a score matrix and subtracting from it a matrix of means, where each column contains the mean for each corresponding column in the original matrix, that is: \\[ \\mathbf{D} = \\mathbf{X} - \\mathbf{M} \\] For example, consider the following score matrix: \\[ \\mathbf{X} = \\begin{bmatrix} 3.0 &amp; 11 &amp; 112 \\\\ 3.9 &amp; 10 &amp; 143 \\\\ 2.9 &amp; 19 &amp; 124 \\\\ 2.7 &amp; 7 &amp; 129 \\end{bmatrix} \\] To compute the mean deviation matrix, D we use: \\[ \\begin{split} \\mathbf{D} &amp;= \\mathbf{X} - \\mathbf{M} \\\\[2em] &amp;= \\begin{bmatrix} 3.0 &amp; 11 &amp; 112 \\\\ 3.9 &amp; 10 &amp; 143 \\\\ 2.9 &amp; 19 &amp; 124 \\\\ 2.7 &amp; 7 &amp; 129 \\end{bmatrix} - \\begin{bmatrix} 3.125 &amp; 11.75 &amp; 127 \\\\ 3.125 &amp; 11.75 &amp; 127 \\\\ 3.125 &amp; 11.75 &amp; 127 \\\\ 3.125 &amp; 11.75 &amp; 127 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix} -0.125 &amp; -0.75 &amp; -15 \\\\ 0.775 &amp; -1.75 &amp; 16 \\\\ -0.225 &amp; 7.25 &amp; -3 \\\\ -0.425 &amp; -4.75 &amp; 2 \\end{bmatrix} \\end{split} \\] Computationally, we can use the colMeans() function to compute the mean in each column of X. Then we can create matrix M by using the rep() function to repeat this set of means four times. (There are four rows in M that have the exact same elements.) # Create X X = matrix(data = c(3.0, 11, 112, 3.9, 10, 143, 2.9, 19, 124, 2.7, 7, 129), byrow = TRUE, nrow = 4) X [,1] [,2] [,3] [1,] 3.0 11 112 [2,] 3.9 10 143 [3,] 2.9 19 124 [4,] 2.7 7 129 # Compute column means colMeans(X) [1] 3.125 11.750 127.000 # Create M M = matrix(rep(colMeans(X), 4), byrow = TRUE, nrow =4) M [,1] [,2] [,3] [1,] 3.125 11.75 127 [2,] 3.125 11.75 127 [3,] 3.125 11.75 127 [4,] 3.125 11.75 127 # Compute deviation matrix X - M [,1] [,2] [,3] [1,] -0.125 -0.75 -15 [2,] 0.775 -1.75 16 [3,] -0.225 7.25 -3 [4,] -0.425 -4.75 2 "],["matrix-multiplication.html", "Chapter 9 Matrix Multiplication 9.1 Scalar–Matrix Multiplication 9.2 Matrix Multiplication 9.3 Properties of Matrix Multiplication 9.4 Revisiting Dot Products", " Chapter 9 Matrix Multiplication In this chapter you will learn about scalar-matrix multiplication and matrix-matrix multiplication. 9.1 Scalar–Matrix Multiplication Any matrix can be multiplied by a scalar. The result is a matrix in which each element in the original matrix is multiplied by the value of the scalar. For example to multiply matrix A (from the previous section) by a scalar \\(\\lambda = -2\\), \\[ \\begin{split} -2\\mathbf{A} &amp;= -2\\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix} -2(112) &amp; -2(86) &amp; -2(0) \\\\ -2(134) &amp; -2(94) &amp; -2(0) \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix} -224 &amp; -172 &amp; 0 \\\\ -268 &amp; -188 &amp; 0 \\end{bmatrix} \\end{split} \\] In general the elements of the product matrix, \\(\\lambda\\mathbf{A}\\), are \\(\\lambda (a_{ij})\\) for every i and j. Scalar–matrix multiplication is commutative, that is: \\[ \\lambda\\mathbf{A} = \\mathbf{A}\\lambda \\] In R we use the * operator to perform scalar–matrix multiplication. -2 * A [,1] [,2] [,3] [1,] -224 -172 0 [2,] -268 -188 0 9.2 Matrix Multiplication The process of multiplying matrices follows the same basic principle as vector multiplication, where we consider matrices to be a collection of column vectors. When we multiply vectors, they must have the same number of elements because we multiply corresponding elements and add the resulting products, obtaining a scalar (dot) product (i.e., \\(\\mathbf{a}\\bullet\\mathbf{b}\\)). To multiply matrix X by matrix Y, we are going to compute the dot product between each row in X and each column in Y, that is, \\(\\mathbf{x}^\\intercal_{j}\\bullet\\mathbf{b}_j\\). As an example, consider the following two matrices: \\[ \\mathbf{X} = \\begin{bmatrix} 2 &amp; 3 \\\\ 1 &amp; 2 \\end{bmatrix} \\qquad \\mathbf{Y} = \\begin{bmatrix} 5 &amp; -2 \\\\ 4 &amp; -1 \\end{bmatrix} \\] To complete the multiplication of XY, we compute the dot product between the first row of X and the first column of Y (i.e., \\(\\mathbf{X}_1^\\intercal\\bullet\\mathbf{Y}_1\\)). \\[ \\begin{split} \\begin{bmatrix}2 &amp; 3\\end{bmatrix}\\begin{bmatrix}5 \\\\ 4\\end{bmatrix} &amp;= 2(5) + 3(4)\\\\[2em] &amp;= 22 \\end{split} \\] This gives us the element in the first row and first column of the product matrix, Z. The element in the first row and second column of Z is based on the dot product between the the first row of X and the second column of Y. This continues, finding the dot product between rows of X and columns of Y for each corresponding element in Z. \\[ \\begin{split} \\mathbf{Z} &amp;= \\begin{bmatrix}\\begin{bmatrix}2 &amp; 3\\end{bmatrix}\\begin{bmatrix}5 \\\\ 4\\end{bmatrix} &amp; \\begin{bmatrix}2 &amp; 3\\end{bmatrix}\\begin{bmatrix}-2 \\\\ -1\\end{bmatrix} \\\\ \\begin{bmatrix}1 &amp; 2\\end{bmatrix}\\begin{bmatrix}5 \\\\ 4\\end{bmatrix} &amp; \\begin{bmatrix}1 &amp; 2\\end{bmatrix}\\begin{bmatrix}-2 \\\\ -1\\end{bmatrix}\\end{bmatrix}\\\\[2em] &amp;= \\begin{bmatrix} 22 &amp; -7 \\\\ 13 &amp; -4 \\end{bmatrix} \\end{split} \\] This process must always be carefully followed: Finding the dot product between a row of the first matrix and a column of the second matrix. The general rule on matrix multiplication for \\(\\mathbf{AB} = \\mathbf{C}\\), for each element in C, \\(C_{ij}\\) is the dot product of the ith row of A and the jth column of B. To compute the product between two matrices we use the matrix multiplication operator, %*%. # Create A A = matrix(data = c(2, 3, 1, 2), byrow = TRUE, nrow = 2) # Create B B = matrix(data = c(5, -2, 4, -1), byrow = TRUE, nrow = 2) # Matrix multiplication A %*% B [,1] [,2] [1,] 22 -7 [2,] 13 -4 In contrast to scalar multiplication, matrix multiplication is rarely commutative, that is, it is rare that \\(\\mathbf{AB}=\\mathbf{BA}\\). In our previous example, \\(\\mathbf{AB}\\neq\\mathbf{BA}\\). # Matrix multiplication B %*% A [,1] [,2] [1,] 8 11 [2,] 7 10 The resulting product is not the same as computing AB. The order of the multiplication is quite important, so rather than saying that “A is multiplied by B,” we instead specify the order by saying: A is postmultiplied by B; or B is premultiplied by A. 9.2.1 Conformability We can also multiply matrices that are not of equal dimensions, as long as they are conformable. To be conformable, the number of columns in the premultiplied (first) matrix must equal the number of rows in the postmultiplied (second) matrix. For example, matrices A and B (below) are conformable since the number of columns in A is equal to the number of rows in B, namely 4. \\[ \\underset{2\\times4}{\\mathbf{A}} = \\begin{bmatrix} 5 &amp; 2 &amp; 3 &amp; 4 \\\\ 5 &amp; 4 &amp; 6 &amp; 1 \\end{bmatrix} \\qquad \\underset{4\\times3}{\\mathbf{B}} = \\begin{bmatrix} 8 &amp; 9 &amp; 4 \\\\ 6 &amp; 5 &amp; 1 \\\\ 2 &amp; 3 &amp; 4 \\\\ 6 &amp; 1 &amp; 2 \\end{bmatrix} \\] One way to consider conformability is to ensure that the inner dimensions of the two matrices being multiplied are equal. Here the inner dimensions (shown in red) are both 4. \\[ \\underset{2\\times\\color{red}{4}}{\\mathbf{A}}~\\underset{{\\color{red}{4}} \\times3}{\\mathbf{B}} \\] # Create A A = matrix(data = c(5, 2, 3, 4, 5, 4, 6, 1), byrow = TRUE, nrow = 2) # Create B B = matrix(data = c(8, 9, 4, 6, 5, 1, 2, 3, 4, 6, 1, 2), byrow = TRUE, nrow = 4) # Postmultiply A by B A %*% B [,1] [,2] [,3] [1,] 82 68 42 [2,] 82 84 50 Notice the product AB has dimensions of \\(2\\times4\\), which are the outer dimensions shown in the product (below in red). \\[ \\underset{{\\color{red}{2}} \\times4}{\\mathbf{A}}~\\underset{4\\times\\color{red}{3}}{\\mathbf{B}} \\] If we tried to postmultiply B by A, that product is not conformable since the inner dimensions would no longer match (i.e., \\(3\\neq2\\)). \\[ \\underset{4\\times\\color{red}{3}}{\\mathbf{B}} ~ \\underset{{\\color{red}{2}}\\times4}{\\mathbf{A}} \\] # Postmultiply B by A B %*% A Error in B %*% A: non-conformable arguments 9.3 Properties of Matrix Multiplication Although matrix multiplication is rarely commutative, it does have the following properties: \\(\\mathbf{A}(\\mathbf{BC})=(\\mathbf{AB})\\mathbf{C})\\) (Associative Property) \\(\\mathbf{A}(\\mathbf{B}+\\mathbf{C})=\\mathbf{AB} +\\mathbf{AC})\\) (Left Distributive Property) \\((\\mathbf{B}+\\mathbf{C})\\mathbf{A}=\\mathbf{BA} +\\mathbf{CA})\\) (Right Distributive Property) so long as the matrices being considered are conformable. Aside from commutativity, there are also some other properties that matrix multiplication lack. For example, in scalar arithmetic, if \\(ab = 0\\), then either a or b must be zero. In matrix multiplication, however, this is not always the case. To illustrate, consider the following three matrices: \\[ \\mathbf{A} = \\begin{bmatrix}-2 &amp; 4 \\\\-2 &amp; 4 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}0 &amp; 0 \\\\0 &amp; 0 \\end{bmatrix} \\qquad \\mathbf{C} = \\begin{bmatrix}4 &amp; 4 \\\\2 &amp; 2 \\end{bmatrix} \\] Matrix B, in which every element is zero is called the null matrix. (A null matrix is a matrix in which every element is zero.) If we postmultiply matrix A by the null matrix B, we get: \\[ \\begin{split} \\mathbf{A}\\mathbf{B} &amp;= \\begin{bmatrix}-2 &amp; 4 \\\\-2 &amp; 4 \\end{bmatrix} \\begin{bmatrix}0 &amp; 0 \\\\0 &amp; 0 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix}0 &amp; 0 \\\\0 &amp; 0 \\end{bmatrix} \\end{split} \\] The result of postmultiplying a matrix (so long as it is conformable) by the null matrix is a null matrix. This is also true when we premultiply by a null matrix. Now consider postmultiplying matrix A by matrix C: \\[ \\begin{split} \\mathbf{A}\\mathbf{C} &amp;= \\begin{bmatrix}-2 &amp; 4 \\\\-2 &amp; 4 \\end{bmatrix} \\begin{bmatrix}4 &amp; 4 \\\\2 &amp; 2 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix}0 &amp; 0 \\\\0 &amp; 0 \\end{bmatrix} \\end{split} \\] We again obtain a null matrix. However, neither A nor C was a null matrix. (In fact, none of the elements of A or C were zero). Relatedly, in scalar arithmetic, if \\(ab = 0\\), then \\(ba = 0\\). This property does not hold in matrix multiplication. For example, we just saw that \\(\\mathbf{A}\\mathbf{C}=\\mathbf{0}\\). But, if we postmultiply C by A: \\[ \\begin{split} \\mathbf{C}\\mathbf{A} &amp;= \\begin{bmatrix}4 &amp; 4 \\\\2 &amp; 2 \\end{bmatrix}\\begin{bmatrix}-2 &amp; 4 \\\\-2 &amp; 4 \\end{bmatrix} \\\\[2em] &amp;= \\begin{bmatrix}-16 &amp; 16 \\\\-8 &amp; 8 \\end{bmatrix} \\end{split} \\] That is, although \\(\\mathbf{A}\\mathbf{C}=\\mathbf{0}\\), \\(\\mathbf{C}\\mathbf{A}\\neq\\mathbf{0}\\). Changing the order of the matrix multiplication changes the resulting product. Drawing from this example we see that since \\(\\mathbf{AB}=\\mathbf{0}\\) and \\(\\mathbf{AC}=\\mathbf{0}\\), this means that \\(\\mathbf{AB}=\\mathbf{AC}\\). However, \\(\\mathbf{B}\\neq\\mathbf{C}\\). This implies that “cancellation” is not a valid property of matrix multiplication. 9.4 Revisiting Dot Products It turns out that we can express the dot product between two vectors as matrix multiplication. Consider the column vectors a and b: \\[ \\mathbf{a} = \\begin{bmatrix}5 \\\\ 4 \\\\ 7 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}1 \\\\ 0 \\\\ -1 \\\\ 2\\end{bmatrix} \\] Remember column vectors are matrices with one column. The dot product between a and b is the same as premultiplying b by the transpose of a, that is: \\[ \\mathbf{a}\\bullet\\mathbf{b} = \\underset{1\\times4}{\\mathbf{a}^\\intercal}~\\underset{4\\times1}{\\mathbf{b}} \\] By transposing a, the matrices are conformable, and the resulting product will be a \\(1\\times1\\) matrix (i.e., a scalar). # Create a a = matrix(data = c(5, 4, 7, 2), ncol = 1) # Create b b = matrix(data = c(1, 0, -1, 2), ncol = 1) # Postmultiply the transpose of a by b t(a) %*% b [,1] [1,] 2 # Compute dot product as sum of products sum(a * b) [1] 2 "],["matrix-transposition.html", "Chapter 10 Matrix Transposition 10.1 Exercises", " Chapter 10 Matrix Transposition In this chapter, you will learn about matrix transposition. Transposition is another operation that can also be carried out on matrices. Just as when we transpose a vector, we transpose a matrix by taking each column in turn and making it a row. In other words, we interchange each column and row, so the first column becomes the first row, the second column becomes the second row, etc. For example, \\[ \\mathbf{A} = \\begin{bmatrix} 112 &amp; 86 &amp; 0 \\\\ 134 &amp; 94 &amp; 0 \\end{bmatrix} \\qquad \\mathbf{A}^\\intercal = \\begin{bmatrix} 112 &amp; 134 \\\\86 &amp; 94 \\\\ 0 &amp; 0 \\end{bmatrix} \\] Formally, if A is an \\(n \\times k\\) matrix with elements \\(a_{ij}\\), then the transpose of A, denoted \\(\\mathbf{A}^\\intercal\\) is a \\(k \\times n\\) matrix where element \\(a^\\intercal_{ij}=a_{ji}\\). Some properties of the matrix transpose are: \\((\\mathbf{A}^{\\intercal}) ^{^{\\intercal}} = \\mathbf{A}\\) \\((\\lambda\\mathbf{A})^{^{\\intercal}} = \\lambda \\mathbf{A}^{\\intercal}\\) where \\(\\lambda\\) is a scalar \\((\\mathbf{A} + \\mathbf{B})^{^{\\intercal}} = \\mathbf{A}^{\\intercal} + \\mathbf{B}^{\\intercal}\\) (This property extends to more than two matrices; \\((\\mathbf{A} + \\mathbf{B} + \\mathbf{C})^{^{\\intercal}} = \\mathbf{A}^{\\intercal} + \\mathbf{B}^{\\intercal} + \\mathbf{C}^{\\intercal}\\).) \\((\\mathbf{A}\\mathbf{B})^{^{\\intercal}} = \\mathbf{B}^{\\intercal} \\mathbf{A}^{\\intercal}\\) (This property also extends to more than two matrices; \\((\\mathbf{A}\\mathbf{B}\\mathbf{C})^{^{\\intercal}} = \\mathbf{C}^{\\intercal}\\mathbf{B}^{\\intercal} \\mathbf{A}^{\\intercal}\\).) Computationally, the t() function will produce the transpose of a matrix in R. # Create A A = matrix( data = c(112, 134, 86, 94, 0, 0), nrow = 2 ) # Display A A [,1] [,2] [,3] [1,] 112 86 0 [2,] 134 94 0 # Compute transpose t(A) [,1] [,2] [1,] 112 134 [2,] 86 94 [3,] 0 0 10.1 Exercises Consider the following matrices: \\[ \\mathbf{A} = \\begin{bmatrix}1 &amp; 5 &amp; -1 \\\\ 3 &amp; 2 &amp; -1 \\\\ 0 &amp; 1 &amp; 5 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}3 &amp; 1 &amp; 6 \\\\ 2 &amp; 0 &amp; 1 \\\\ -7 &amp; -1 &amp; 2 \\end{bmatrix} \\] Verify that \\((\\mathbf{A}+\\mathbf{B})^\\intercal = \\mathbf{A}^\\intercal + \\mathbf{B}^\\intercal\\)? Show/Hide Solution \\[ \\begin{split} (\\mathbf{A}+\\mathbf{B})^\\intercal &amp;= \\begin{bmatrix}4 &amp; 6 &amp; 5\\\\5 &amp; 2 &amp; 0\\\\ -7 &amp; 0 &amp; 7 \\end{bmatrix}^\\intercal = \\begin{bmatrix}4 &amp; 5 &amp; -7\\\\6 &amp; 2 &amp; 0\\\\5 &amp; 0&amp; 7 \\end{bmatrix} \\\\[3em] \\mathbf{A}^\\intercal + \\mathbf{B}^\\intercal &amp;= \\begin{bmatrix}1 &amp; 3 &amp; 0\\\\5 &amp; 2 &amp; 1\\\\-1 &amp; -1 &amp; 5 \\end{bmatrix} + \\begin{bmatrix}3 &amp; 2 &amp; -7\\\\1 &amp; 0 &amp; -1\\\\6 &amp; 1 &amp; 2 \\end{bmatrix} = \\begin{bmatrix}4 &amp; 5 &amp; -7\\\\6 &amp; 2 &amp; 0\\\\5 &amp; 0&amp; 7 \\end{bmatrix} \\end{split} \\] Verify that \\((\\mathbf{A}\\mathbf{B})^\\intercal = \\mathbf{B}^\\intercal \\mathbf{A}^\\intercal\\)?. Show/Hide Solution \\[ \\begin{split} (\\mathbf{A}\\mathbf{B})^\\intercal &amp;= \\begin{bmatrix}20 &amp; 2 &amp; 9\\\\20 &amp; 4 &amp; 18\\\\-33 &amp; -5 &amp; 11 \\end{bmatrix}^\\intercal = \\begin{bmatrix}20 &amp; 20 &amp; -33\\\\ 2 &amp; 4&amp; -5\\\\9 &amp; 18 &amp; 11 \\end{bmatrix} \\\\[3em] \\mathbf{B}^\\intercal \\mathbf{A}^\\intercal &amp;= \\begin{bmatrix}3 &amp; 2 &amp; -7\\\\1 &amp; 0 &amp; -1\\\\6 &amp; 1 &amp; 2 \\end{bmatrix}\\begin{bmatrix}1 &amp; 3 &amp; 0\\\\5 &amp; 2 &amp; 1\\\\-1 &amp; -1 &amp; 5 \\end{bmatrix} = \\begin{bmatrix}20 &amp; 20 &amp; -33\\\\ 2 &amp; 4&amp; -5\\\\9 &amp; 18 &amp; 11 \\end{bmatrix} \\end{split} \\] "],["exercises-matrix-operations.html", "Chapter 11 Exercises: Matrix Operations", " Chapter 11 Exercises: Matrix Operations Consider the following matrices: \\[ \\mathbf{X} = \\begin{bmatrix}2 &amp; 3 \\\\1 &amp; 2 \\end{bmatrix} \\qquad \\mathbf{Y} = \\begin{bmatrix}3 &amp; 4 \\\\2 &amp; 1 \\end{bmatrix} \\qquad \\mathbf{Z} = \\begin{bmatrix}2 &amp; 3 &amp; 1 \\\\5 &amp; 6 &amp; 8\\\\9 &amp; 4 &amp; 7 \\end{bmatrix} \\] Find \\(\\mathbf{X} + \\mathbf{Y}\\). Show/Hide Solution \\[ \\begin{split} \\mathbf{X}+\\mathbf{Y} &amp;= \\begin{bmatrix}2+3 &amp; 3+4 \\\\1+2 &amp; 2+1 \\end{bmatrix} \\\\[1em] &amp;= \\begin{bmatrix}5 &amp; 7 \\\\3 &amp; 3 \\end{bmatrix} \\end{split} \\] Find \\(\\mathbf{X} - \\mathbf{Y}\\). Show/Hide Solution \\[ \\begin{split} \\mathbf{X}-\\mathbf{Y} &amp;= \\begin{bmatrix}2-3 &amp; 3-4 \\\\1-2 &amp; 2-1 \\end{bmatrix} \\\\[1em] &amp;= \\begin{bmatrix}-1 &amp; -1 \\\\-1 &amp; 1 \\end{bmatrix} \\end{split} \\] Find \\(3 \\mathbf{Z}\\). Show/Hide Solution \\[ \\begin{split} 3\\mathbf{Z} &amp;= \\begin{bmatrix}3(2) &amp; 3(3) &amp; 3(1) \\\\3(5) &amp; 3(6) &amp; 3(8)\\\\3(9) &amp; 3(4) &amp; 3(7) \\end{bmatrix} \\\\[1em] &amp;= \\begin{bmatrix}6 &amp; 9 &amp; 3 \\\\15 &amp; 18 &amp; 24\\\\27 &amp; 12 &amp; 21 \\end{bmatrix} \\end{split} \\] Find \\(-2 \\mathbf{X} + 4\\mathbf{Y}\\). Show/Hide Solution \\[ \\begin{split} -2\\mathbf{X}+4\\mathbf{Y} &amp;= \\begin{bmatrix}-2(2)+4(3) &amp; -2(3)+4(4) \\\\-2(1)+4(2) &amp; -2(2)+4(1) \\end{bmatrix} \\\\[1em] &amp;= \\begin{bmatrix}8 &amp; 10 \\\\6 &amp; 0 \\end{bmatrix} \\end{split} \\] Consider the following matrices: \\[ \\mathbf{A} = \\begin{bmatrix}0 &amp; 6 \\\\5 &amp; 1 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}0 &amp; 5 \\\\2 &amp; \\frac{1}{2} \\end{bmatrix} \\qquad \\mathbf{C} = \\begin{bmatrix}6 &amp; 2 &amp; 1 \\\\5 &amp; 3 &amp; 1\\\\8 &amp; 4 &amp; 1 \\end{bmatrix} \\qquad \\mathbf{D} = \\begin{bmatrix}0&amp; 1 &amp; 4 &amp; 6 \\\\1 &amp; 2 &amp; 5 &amp; -2\\\\1 &amp; 3 &amp; 2 &amp; 8 \\end{bmatrix} \\] Is AB conformable? Show/Hide Solution Yes. Since A has two columns and B has the same number of rows, AB is conformable. \\[ \\underset{2\\times\\color{red}{2}}{\\mathbf{A}}~\\underset{{\\color{red}{2}} \\times2}{\\mathbf{B}} \\] Is BC conformable? Show/Hide Solution No. Since B has two columns and C has three rows, BC is not conformable. \\[ \\underset{2\\times\\color{red}{2}}{\\mathbf{B}}~\\underset{{\\color{red}{3}} \\times3}{\\mathbf{C}} \\] Is the product where we premultiply D by C conformable? Show/Hide Solution Yes. Since C has three columns and D has the same number of rows, CD is conformable. \\[ \\underset{3\\times\\color{red}{3}}{\\mathbf{C}}~\\underset{{\\color{red}{3}} \\times4}{\\mathbf{D}} \\] Statistics Example: Weights Consider the scores for five students on four course exams (each out of 100 points) shown in matrix X. The final percentage in the course is based on the following weighting: the first and second exams are worth 10% of the course, the third exam is worth 30% of the course, and the fourth exam is worth 50% of the course. These weights are presented in the column vector w. Use R to find the final percentage for each student by postmultiplying the score matrix by the weight vector. \\[ \\mathbf{X} = \\begin{bmatrix}32 &amp; 54 &amp; 56 &amp; 21 \\\\42 &amp; 23 &amp; 52 &amp; 35 \\\\ 16 &amp; 41 &amp; 54 &amp; 56 \\\\ 58 &amp; 52 &amp; 31 &amp; 24 \\\\ 41 &amp; 50 &amp; 42 &amp; 40 \\end{bmatrix} \\qquad \\mathbf{w} = \\begin{bmatrix} 0.10 \\\\ 0.10 \\\\ 0.30 \\\\ 0.50 \\end{bmatrix} \\] Show/Hide Solution # Create X X = matrix( data = c(32, 54, 56, 21, 42, 23, 52, 35, 16, 41, 54, 56, 58, 52, 31, 24, 41, 50, 42, 40), byrow = TRUE, ncol = 4 ) # Create w w = matrix(data = c(0.10, 0.10, 0.30, 0.50), ncol = 1) # Compute Xw X %*% w [,1] [1,] 35.9 [2,] 39.6 [3,] 49.9 [4,] 32.3 [5,] 41.7 "],["square-matrices-and-friends.html", "Chapter 12 Square Matrices and Friends 12.1 Square Matrices 12.2 Diagonal Matrices 12.3 Scalar Matrices 12.4 Symmetric Matrices 12.5 Triangular Matrices 12.6 Exercises", " Chapter 12 Square Matrices and Friends In this chapter, you will learn about square matrices and several specific types of square matrices encountered in statistical and psychometric applications. Many of these matrices have properties that make them incredibly useful for computation. 12.1 Square Matrices When the number of rows and columns in a matrix are equal, the matrix is referred to as a square matrix. For example, X and Y are both square matrices. \\[ \\underset{2\\times 2}{\\mathbf{X}} = \\begin{bmatrix} 4 &amp; 1 \\\\ 0 &amp; 3 \\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{Y}} = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 3 &amp; -8\\\\ 10 &amp; 4 &amp; -2 \\end{bmatrix} \\] 12.1.1 Main Diagonal In a square matrix, the main diagonal (a.k.a., major diagonal or principal diagonal) includes the elements that lie along the diagonal from the upper-left element to the lower-right element. For example, the main diagonal in Y (highlighted in red) includes the elements \\(0\\), \\(3\\), and \\(-2\\). Elements not in the main diagonal are referred to as off-diagonal elements. \\[ \\underset{3\\times 3}{\\mathbf{Y}} = \\begin{bmatrix} \\color{red}{0} &amp; 1 &amp; 0 \\\\ 1 &amp; \\color{red}{3} &amp; -8\\\\ 10 &amp; 4 &amp; \\color{red}{-2} \\end{bmatrix} \\] One interesting and useful property of a square matrix is that if we compute its transpose, the main diagonal is the same as in the original matrix. \\[ \\underset{3\\times 3}{\\mathbf{Y}} = \\begin{bmatrix} \\color{red}0 &amp; 1 &amp; 0 \\\\ 1 &amp; \\color{red}3 &amp; -8\\\\ 10 &amp; 4 &amp; \\color{red}{-2} \\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{Y}^\\intercal} = \\begin{bmatrix} \\color{red}0 &amp; 1 &amp; 10 \\\\ 1 &amp; \\color{red}3 &amp; 4\\\\ 0 &amp; -8 &amp; \\color{red}{-2} \\end{bmatrix} \\] We can use the diag() function to return the elements on the main diagonal in a square matrix. # Create Y Y = matrix( data = c(0, 1, 10, 1, 3, 4, 0, -8, -2), nrow = 3 ) # Display Y Y [,1] [,2] [,3] [1,] 0 1 0 [2,] 1 3 -8 [3,] 10 4 -2 # Find diagonal elements diag(Y) [1] 0 3 -2 🚧 CAUTION: The diag() function also works on non-square matrices. However, it returns the elements on the diagonal starting with the element in the first row and column. # Create X X = matrix( data = c(0, 1, 1, 3, 4, 0), nrow = 3 ) # Display X X [,1] [,2] [1,] 0 3 [2,] 1 4 [3,] 1 0 # Find &#39;diagonal&#39; elements diag(X) [1] 0 4 This is because non-square matrices also have a main diagonal. The main diagonal in a non-square matrix includes the elements \\(a_{11}, a_{22}, \\ldots, a_{mm}\\) where m is the minimum of the row and column values. 12.2 Diagonal Matrices A diagonal matrix is a square matrix in which all the off-diagonal elements are zero. Two examples of diagonal matrices are as follows: \\[ \\underset{2\\times 2}{\\mathbf{D}_1} = \\begin{bmatrix} 5 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{D}_2} = \\begin{bmatrix} 3 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{bmatrix} \\] In general, D is a diagonal matrix if element \\(d_{ij}=0\\) for all \\(i\\neq j\\). 12.3 Scalar Matrices A diagonal matrix in which all the diagonal elements have the same value is called a scalar matrix. The following two matrices are scalar matrices: \\[ \\underset{2\\times 2}{\\mathbf{S}_1} = \\begin{bmatrix} -3 &amp; 0 \\\\ 0 &amp; -3 \\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{S}_2} = \\begin{bmatrix} 10 &amp; 0 &amp; 0 \\\\ 0 &amp; 10 &amp; 0 \\\\ 0 &amp; 0 &amp; 10 \\end{bmatrix} \\] The null matrix is also a scalar matrix. In general S is a scalar matrix if element \\[ s_{ij}=\\begin{cases}k \\quad \\mathrm{when} \\quad i=j \\\\0 \\quad \\mathrm{when} \\quad i\\neq j\\end{cases} \\] 12.3.1 Identity Matrices A special scalar matrix, the identity matrix, is one in which all the diagonal elements are the value of one (1). \\[ \\underset{2\\times 2}{\\mathbf{I}} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{I}} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] The identity matrix has the following property: For any matrix A, \\(\\mathbf{AI} = \\mathbf{IA} = \\mathbf{A}\\) The diag() function can be used to create an identity matrix. We give this function an argument which provides the number of rows and columns. # Create a 3x3 identity matrix I = diag(3) # Display I I [,1] [,2] [,3] [1,] 1 0 0 [2,] 0 1 0 [3,] 0 0 1 12.4 Symmetric Matrices If \\(\\mathbf{A} = \\mathbf{A}^{\\intercal}\\), the matrix A is considered symmetric. For example, the following matrix R is symmetric: \\[ \\underset{3\\times 3}{\\mathbf{R}} = \\begin{bmatrix} 1.00 &amp; 0.32 &amp; 0.69\\\\ 0.32 &amp; 1.00 &amp; 0.85 \\\\ 0.69 &amp; 0.85 &amp; 1.00 \\end{bmatrix} = \\underset{3\\times 3}{\\mathbf{R}^\\intercal} = \\begin{bmatrix} 1.00 &amp; 0.32 &amp; 0.69\\\\ 0.32 &amp; 1.00 &amp; 0.85 \\\\ 0.69 &amp; 0.85 &amp; 1.00 \\end{bmatrix} \\] In general, a matrix A is symmetric if, \\[ \\mathbf{A}_{ij} = \\mathbf{A}_{ji} \\] If matrix A is symmetric, it necessitates that: \\(\\mathbf{A}\\) and \\(\\mathbf{A}^{\\intercal}\\) have the same dimensions. This also implies that the matrix must be a square matrix. All of the corresponding elements in \\(\\mathbf{A}\\) and \\(\\mathbf{A}^{\\intercal}\\) are equal. All scalar matrices, including the identity matrix, are symmetric. In statistical practice, variance–covariance matrices and correlation matrices are symmetric. Computationally, we can examine whether a matrix is symmetric by checking whether the logical statement equating a matrix and its transpose evaluates as TRUE for all elements. For example, consider the correlation matrix R: \\[ \\mathbf{R}=\\begin{bmatrix}1.00 &amp; 0.32 &amp; 0.69\\\\ 0.32 &amp; 1.00 &amp; 0.85\\\\ 0.69 &amp; 0.85 &amp; 1.00\\end{bmatrix} \\] To check its symmetry, we can use the following syntax. # Create matrix R R = matrix( data = c(1.00, 0.32, 0.69, 0.32, 1.00, 0.85, 0.69, 0.85, 1.00), nrow = 3 ) # Display R R [,1] [,2] [,3] [1,] 1.00 0.32 0.69 [2,] 0.32 1.00 0.85 [3,] 0.69 0.85 1.00 # Test for symmetry R == t(R) [,1] [,2] [,3] [1,] TRUE TRUE TRUE [2,] TRUE TRUE TRUE [3,] TRUE TRUE TRUE If the logical statement of equality evaluates as TRUE for all the elements, the matrix is symmetric. If the logical expression returns an non-conformable array error, or evaluates as FALSE for any of the elements, then it is not symmetric. 12.4.1 Skew Symmetric Matrices A matrix A is said to be skew symmetric if \\(\\mathbf{A} = -\\mathbf{A}^{\\intercal}\\).For example, the following matrix S is skew symmetric: \\[ \\underset{3\\times 3}{\\mathbf{S}} = \\begin{bmatrix} 0 &amp; 2 &amp; -3\\\\ -2 &amp; 0 &amp; 1 \\\\ 3 &amp; -1 &amp; 0 \\end{bmatrix} \\] In general, a matrix A is skew symmetric if, \\(\\mathbf{a}_{ij} = -\\mathbf{a}_{ji}\\). Note that if a matrix is skew symmetric, all of its diagonal elements must be zero. 12.5 Triangular Matrices A matrix is a triangular matrix if all of the elements above or below the main diagonal are zero. These are referred to as upper triangular matrices and lower triangular matrices, respectively. A matrix A is a lower triangular matrix if elements \\(a_{ij}=0\\) for \\(j&gt;i\\) (all the elements above the main diagonal are zero). For example the matrix L is lower triangular: \\[ \\underset{4\\times 4}{\\mathbf{L}} = \\begin{bmatrix} 6 &amp; \\color{red}{0} &amp; \\color{red}{0} &amp; \\color{red}{0}\\\\ -1 &amp; 5 &amp; \\color{red}{0} &amp; \\color{red}{0} \\\\ 0 &amp; 1 &amp; 2 &amp; \\color{red}{0} \\\\ 3 &amp; -2 &amp; 5 &amp; 7 \\end{bmatrix} \\] A matrix A is an upper triangular matrix if elements \\(a_{ij}=0\\) for \\(i&gt;j\\) (all the elements below the main diagonal are zero). For example the matrix U is lower triangular: \\[ \\underset{4\\times 4}{\\mathbf{U}} = \\begin{bmatrix} 3 &amp; -2 &amp; 5 &amp; 7\\\\ \\color{red}{0} &amp; 1 &amp; 2 &amp; 0 \\\\ \\color{red}{0} &amp; \\color{red}{0} &amp; -1 &amp; 5 &amp; \\\\ \\color{red}{0} &amp; \\color{red}{0} &amp; \\color{red}{0} &amp; 6\\\\ \\end{bmatrix} \\] 12.6 Exercises Consider the following matrices: \\[ \\mathbf{A} = \\begin{bmatrix}1 &amp; 5 &amp; -1 \\\\ 3 &amp; 2 &amp; -1 \\\\ 0 &amp; 1 &amp; 5 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}3 &amp; 1 &amp; 6 \\\\ 2 &amp; 0 &amp; 1 \\\\ -7 &amp; -1 &amp; 2 \\end{bmatrix} \\] List the diagonal elements of \\(\\mathbf{A}^\\intercal\\). Show/Hide Solution \\[ \\begin{split} B^\\intercal_{1,1}=1 \\\\[1em] B^\\intercal_{2,2}=2 \\\\[1em] B^\\intercal_{3,3}=5 \\\\[1em] \\end{split} \\] Create an identity matrix of order 4. Show/Hide Solution \\[ \\underset{4\\times4}{\\mathbf{I}} = \\begin{bmatrix}1 &amp; 0 &amp; 0&amp; 0\\\\0 &amp; 1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix} \\] Explain why the transpose of an upper triangular matrix will be a lower triangular matrix. Show/Hide Solution An upper triangular matrix, U, has elements \\(u_{ij}=0\\) for \\(i&gt;j\\) (all the elements below the main diagonal are zero). Taking the transpose of U, element \\(u_{ij}\\) becomes \\(u^\\intercal_{ji}\\). This means that in \\(\\mathbf{U}^\\intercal\\) the zero elements all be where \\(j&gt;i\\). Consider the following: \\[ \\mathbf{X} = \\begin{bmatrix} 10 &amp; 4 &amp; 6\\\\4 &amp; 10 &amp; 8\\\\6 &amp; 8 &amp; 9 \\end{bmatrix} \\qquad \\mathbf{Y} = \\begin{bmatrix}0 &amp; 1 &amp; 2\\\\-1 &amp; 0 &amp; 4\\\\-2 &amp; -4 &amp; 0 \\end{bmatrix} \\] Use R to show that X is symmetric. Show/Hide Solution # Create X X = matrix( data = c(10, 4, 6, 4, 10, 8, 6, 8, 9), byrow = TRUE, ncol = 3 ) # Check symmetry X == t(X) [,1] [,2] [,3] [1,] TRUE TRUE TRUE [2,] TRUE TRUE TRUE [3,] TRUE TRUE TRUE Use R to show that Y is skew symmetric. Show/Hide Solution # Create X Y = matrix( data = c(0, 1, 2, -1, 0, 4, -2, -4, 0), byrow = TRUE, ncol = 3 ) # Check symmetry Y == -t(Y) [,1] [,2] [,3] [1,] TRUE TRUE TRUE [2,] TRUE TRUE TRUE [3,] TRUE TRUE TRUE "],["properties-of-square-matrices.html", "Chapter 13 Properties of Square Matrices 13.1 Matrix Trace 13.2 Determinant 13.3 Matrix Inverse 13.4 Exercises", " Chapter 13 Properties of Square Matrices In this chapter, you will learn about several properties of square matrices encountered in statistical and psychometric applications. These properties are also useful for computation. 13.1 Matrix Trace The trace of a square matrix is the sum of the elements along the main diagonal.8 For example, consider the matrix Y: \\[ \\underset{3\\times 3}{\\mathbf{Y}} = \\begin{bmatrix} \\color{red}0 &amp; 1 &amp; 0 \\\\ 1 &amp; \\color{red}3 &amp; -8\\\\ 10 &amp; 4 &amp; \\color{red}{-2} \\end{bmatrix} \\] The trace of Y is \\(0 + 3 + -2 = 1\\). To compute the trace using R, we use the sum() and diag() functions. # Create matrix Y = matrix( data = c(0, 1, 0, 1, 3, -8, 10, 4, -2), byrow = TRUE, ncol = 3 ) # Compute trace of Y sum(diag(Y)) [1] 1 You can also use the tr() function from the psych library to compute the trace. library(psych) tr(Y) [1] 1 13.2 Determinant Every square matrix has a unique scalar value associated with it, called the determinant. The determinant, denoted by \\(\\left| \\mathbf{A} \\right|\\) or \\(\\mathrm{det}(\\mathbf{A})\\), has many uses in statistical and psychometric applications. For example, the determinant of a variance-covariance matrix provides information about the generalized variance of several variables. It is also useful for computing several multivariate statistics. We will illustrate the process for calculating the determinant for a \\(1 \\times 1\\), \\(2 \\times 2\\), and \\(3 \\times 3\\) matrix. Beyond a \\(3 \\times 3\\) matrix the determinant is more difficult to calculate, and those methods will not be discussed here. In practice, computation is used to find the determinant. 13.2.1 Determinant for a \\(1 \\times 1\\) matrix. The determinant of a \\(1 \\times 1\\) scalar matrix is equal to the scalar, namely \\(\\mathrm{det}\\big([a_1]\\big) = a_1\\). For example, consider the scalar matrix having an element of \\(-1\\): \\[ \\begin{vmatrix}-1\\end{vmatrix} = -1 \\] The determinant of this matrix is \\(-1\\). 13.2.2 Determinant for a \\(2 \\times 2\\) matrix. The determinant of the \\(2\\times 2\\) matrix A, given as \\[ \\underset{2\\times 2}{\\mathbf{A}} = \\begin{bmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{bmatrix} \\] is the scalar \\(a_{11}(a_{22}) - a_{12}(a_{21})\\). That is, we are taking the product of the elements along the main diagonal (shown in pink below) and subtracting the product of the elements on the off diagonal (shown in blue below). Figure 13.1: Procedure for finding the product terms in the determinant for a 2x2 matrix. The first term is computed by multiplying the elements along the pink arrow, and the second term is computed by multiplying the elements along the blue arrow. Color-coding the formula: \\[ \\begin{split} \\mathrm{det}(\\mathbf{A}) = ~&amp;\\color{#CC79A7}{\\overbrace{a_{11}(a_{22})}^\\mathrm{Term~1}} - \\color{#56B4E9}{ \\overbrace{a_{12}(a_{21})}^\\mathrm{Term~2} } \\end{split} \\] As an example, consider the matrix Y: \\[ \\mathbf{Y} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix} \\] The determinant of Y is: \\[ \\begin{split} \\mathrm{det}(\\mathbf{Y}) &amp;= \\begin{vmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{vmatrix} \\\\[2ex] &amp;= 1(4) - 2(3) \\\\[2ex] &amp;= -2 \\end{split} \\] 13.2.3 Determinant for a \\(3 \\times 3\\) matrix. The determinant of a \\(3\\times 3\\) matrix A, where \\[ \\underset{3\\times 3}{\\mathbf{A}} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix} \\] is defined as: \\[ \\begin{split} \\mathrm{det}(\\mathbf{A}) = &amp;a_{11}(a_{22})(a_{33}) + a_{12}(a_{23})(a_{31}) + a_{13}(a_{21})(a_{32}) -\\\\ &amp;a_{13}(a_{22})(a_{31}) - a_{11}(a_{23})(a_{32}) - a_{12}(a_{21})(a_{33}) \\end{split} \\] Although this looks complicated the procedure for finding this is easily described. Write out the matrix A and additionally append the first two columns to the right of the matrix. These additional columns are shown in red below. \\[ \\mathbf{A} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix} \\color{red}{\\begin{matrix}a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\\\ a_{31} &amp; a_{32} \\end{matrix}} \\] The first three product terms in the determinant are computed by: multiplying the elements found in the main diagonal (first product term), and the elements in the remaining two parallel diagonals (shown in pink below). Similarly, the second set of three products are found by multiplying the elements in the opposite diagonal and the elements in the two parallel diagonals (shown in blue below). Figure 13.2: Procedure for finding the product terms in the determinant for a 3x3 matrix. The first three terms computed by multiplying the elements along each of the pink arrows, and the second set of three terms are computed by multiplying the elements along each of the blue arrows. We add together the first three product terms and subtract the last three product terms. This gives us the determinant. \\[ \\begin{split} \\mathrm{det}(\\mathbf{A}) = ~&amp;\\color{#CC79A7}{\\overbrace{a_{11}(a_{22})(a_{33})}^\\mathrm{Term~1} + \\overbrace{a_{12}(a_{23})(a_{31})}^\\mathrm{Term~2} + \\overbrace{a_{13}(a_{21})(a_{32})}^\\mathrm{Term~3}} \\color{#56B4E9}{-}\\\\ &amp;\\color{#56B4E9}{\\underbrace{a_{13}(a_{22})(a_{31})}_\\mathrm{Term~4} - \\underbrace{a_{11}(a_{23})(a_{32})}_\\mathrm{Term~5} - \\underbrace{a_{12}(a_{21})(a_{33})}_\\mathrm{Term~6}} \\end{split} \\] As an example consider the following \\(3 \\times 3\\) matrix: \\[ \\underset{3\\times 3}{\\mathbf{X}} = \\begin{bmatrix} 3 &amp; 5 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ 3 &amp; 6 &amp; 4 \\end{bmatrix} \\] The determinant of X is: \\[ \\begin{split} \\mathrm{det}(\\mathbf{X}) &amp;= \\begin{vmatrix} 3 &amp; 5 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ 3 &amp; 6 &amp; 4 \\end{vmatrix} \\\\[2ex] &amp;= 3(2)(4) + 5(1)(3) + 0(1)(6) - 0(2)(3) - 3(1)(6) - 5(1)(4) \\\\[2ex] &amp;= 24 + 15 + 0 - 0 - 18 - 20 \\\\[2ex] &amp;= 1 \\end{split} \\] 13.2.4 Using R to Find the Determinant We can use the det() function to compute the determinant, using R. Below we use this function to compute the determinant for each of the matrices given in the examples above. # Create matrix Y = matrix( data = c(1, 2, 3, 4), byrow = TRUE, ncol = 2 ) # Find determinant det(Y) [1] -2 # Create matrix X = matrix( data = c(3, 5, 0, 1, 2, 1, 3, 6, 4), byrow = TRUE, ncol = 3 ) # Find determinant det(X) [1] 1 13.2.5 Properties of the Determinant There are several useful properties of the determinant. For each of these properties A and B are matrices and \\(\\lambda\\) is a scalar. If every element in a row (or column) of a matrix is zero, then the determinant of that matrix is zero. If two rows (or columns) of a matrix are swapped, the determinant changes sign. If two rows (or columns) of a matrix are identical, the determinant of that matrix is zero. \\(\\mathrm{det}(\\mathbf{A}) = \\mathrm{det}(\\mathbf{A}^\\intercal)\\) If \\(\\mathbf{B}=\\lambda\\mathbf{A}\\), then \\(\\mathrm{det}(\\mathbf{B}) = \\lambda\\big(\\mathrm{det}(\\mathbf{A})\\big)\\). For an \\(n \\times n\\) matrix A, \\(\\mathrm{det}(\\lambda\\mathbf{A}) = \\lambda^n\\big(\\mathrm{det}(\\mathbf{A})\\big)\\). If A and B are of the same order, \\(\\mathrm{det}(\\mathbf{A})\\mathrm{det}(\\mathbf{B}) = \\mathrm{det}(\\mathbf{AB})\\) 13.3 Matrix Inverse The inverse of a \\(n \\times n\\) matrix A, denoted \\(\\mathbf{A}^{-1}\\), is also a \\(n \\times n\\) matrix having the property that: \\[ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I} \\] That is, when we postmultiply (or premultiply) a square matrix by its inverse, we obtain the \\(n \\times n\\) identity matrix. Not all square matrices have an inverse. If a square matrix has an inverse we say that it is invertible or nonsingular. Typically, we will use the computer to find a matrix’s inverse. However, for the \\(2\\times2\\) matrix, we can describe a procedure that can be used to find the inverse. To find \\(\\mathbf{A}^{-1}\\) we: Compute the determinant of A. Create a new matrix, call it B where we swap the elements on the main diagonal of A and change the signs on the off diagonals elements. Multiply B by the reciprocal of the determinant. As an example, consider the following \\(2 \\times 2\\) matrix: \\[ \\mathbf{A} = \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 3 \\end{bmatrix} \\] The determinant of A is 5. Swapping the elements on the main diagonal of A and changing the signs on the off diagonals elements, we get: \\[ \\begin{bmatrix} 3 &amp; -1 \\\\ -1 &amp; 2 \\end{bmatrix} \\] To obtain the inverse, we multiply this converted matrix by the reciprocal of the determinant: \\[ \\begin{split} \\mathbf{A}^{-1} &amp;= \\frac{1}{5} \\begin{bmatrix} 3 &amp; -1 \\\\ -1 &amp; 2 \\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} \\frac{3}{5} &amp; -\\frac{1}{5} \\\\ -\\frac{1}{5} &amp; \\frac{2}{5} \\end{bmatrix} \\end{split} \\] If we postmultiply (or premultiply) this matric by A, we should get the \\(2 \\times 2\\) identity matrix. \\[ \\begin{split} \\mathbf{A}\\mathbf{A}^{-1} &amp;= \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 3 \\end{bmatrix}\\begin{bmatrix} \\frac{3}{5} &amp; -\\frac{1}{5} \\\\ -\\frac{1}{5} &amp; \\frac{2}{5} \\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\\\[5ex] \\mathbf{A}^{-1}\\mathbf{A} &amp;= \\begin{bmatrix} \\frac{3}{5} &amp; -\\frac{1}{5} \\\\ -\\frac{1}{5} &amp; \\frac{2}{5} \\end{bmatrix}\\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 3 \\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\end{split} \\] 13.3.1 Find the Inverse using R To find the inverse using R, we can use the solve() function. If you want fractional elements (rather than decimal elements) you can use the fractions() function from the {MASS} package to convert the decimal output to fractions. # Create matrix A = matrix( data = c(2, 1, 1, 3), byrow = TRUE, ncol = 2 ) # Find inverse solve(A) [,1] [,2] [1,] 0.6 -0.2 [2,] -0.2 0.4 # Find inverse (output as fractions) MASS::fractions(solve(A)) [,1] [,2] [1,] 3/5 -1/5 [2,] -1/5 2/5 If you get a message that the system is “exactly singular” or “computationally singular,” the matrix does not have an inverse (or it cannot be computed). In this case, we would say that A is not invertible or is singular. 13.3.2 Properties of the Inverse There are several useful properties of the matrix inverse. If \\(\\mathrm{det}(\\mathbf{A})=0\\), then A is singular and \\(\\mathbf{A}^{-1}\\) does not exist. If the inverse exists for A, there is only one inverse. If the inverse exists for A, the inverse also exists for \\(\\mathbf{A}^\\intercal\\) \\((\\mathbf{A}^{-1})^{-1} = \\mathbf{A}\\) \\((\\mathbf{A}^\\intercal)^{-1} = (\\mathbf{A}^{-1})^\\intercal\\) \\((\\mathbf{AB})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\\) \\((\\mathbf{A})^{-n} = (\\mathbf{A}^{-1})^n\\) If \\(\\lambda\\) is a non-zero scalar, \\((\\lambda\\mathbf{A})^{-1} = \\frac{1}{\\lambda}(\\mathbf{A}^{-1})\\) The inverse of a nonsingular diagonal matrix D is also a diagonal matrix. Moreover, the diagonal elements of \\(\\mathbf{D}^{-1}\\) are the reciprocals of the diagonal elements of D. The inverse of a nonsingular symmetric matrix is also a symmetric matrix. 13.4 Exercises For each of the following matrices, calculate (by-hand) the trace, determinant, and inverse. Then use R to check your work. \\(\\mathbf{A} = \\begin{bmatrix}2 &amp; 4 \\\\3 &amp; 6 \\end{bmatrix}\\) Show/Hide Solution Trace: \\(2+6=8\\) Determinant: \\(2(6) - 3(4) = 0\\) Inverse: Since the determinant is 0, A is singular and does not have an inverse. # Create A A = matrix(data = c(2, 4, 3, 6), byrow = TRUE, ncol = 2) # Compute trace sum(diag(A)) [1] 8 # Compute determinant det(A) [1] 0 # Compute inverse solve(A) Error in solve.default(A): Lapack routine dgesv: system is exactly singular: U[2,2] = 0 \\(\\mathbf{B} = \\begin{bmatrix}2 &amp; -1 \\\\3 &amp; 3 \\end{bmatrix}\\) Show/Hide Solution Trace: \\(2+3=5\\) Determinant: \\(2(3) - 3(-1) = 9\\) Inverse: \\(\\frac{1}{9}\\begin{bmatrix}3 &amp; 1 \\\\-3 &amp; 2 \\end{bmatrix} = \\begin{bmatrix}\\frac{1}{3} &amp; \\frac{1}{9} \\\\-\\frac{1}{3} &amp; \\frac{2}{9} \\end{bmatrix}\\) # Create B B = matrix(data = c(2, -1, 3, 3), byrow = TRUE, ncol = 2) # Compute trace sum(diag(B)) [1] 5 # Compute determinant det(B) [1] 9 # Compute inverse MASS::fractions(solve(B)) [,1] [,2] [1,] 1/3 1/9 [2,] -1/3 2/9 For each of the following matrices, calculate (by-hand) the trace, and determinant. Then use R to check your work. Also use R to find the inverse. \\(\\mathbf{C} = \\begin{bmatrix}2 &amp; -1 &amp; 3 \\\\3 &amp; 3 &amp; 4\\\\1 &amp; 2 &amp; 2 \\end{bmatrix}\\) Show/Hide Solution Trace: \\(2+3+2=7\\) Determinant: \\(2(3)(2) + (-1)(4)(1) + 3(3)(2) - 1(3)(3) - 2(4)(2) - 2(3)(-1) = 7\\) # Create C C = matrix(data = c(2, -1, 3, 3, 3, 4, 1, 2, 2), byrow = TRUE, ncol = 3) # Compute trace sum(diag(C)) [1] 7 # Compute determinant det(C) [1] 7 # Compute inverse MASS::fractions(solve(C)) [,1] [,2] [,3] [1,] -2/7 8/7 -13/7 [2,] -2/7 1/7 1/7 [3,] 3/7 -5/7 9/7 \\(\\mathbf{D} = \\begin{bmatrix}2 &amp; -1 &amp; 3 \\\\3 &amp; 3 &amp; 4\\\\5 &amp; 2 &amp; 7 \\end{bmatrix}\\) Show/Hide Solution Trace: \\(2+3+7=12\\) Determinant: \\(2(3)(7) + (-1)(4)(5) + 3(3)(2) - 5(3)(3) - 2(4)(2) - 7(3)(-1) = 0\\) # Create D D = matrix(data = c(2, -1, 3, 3, 3, 4, 5, 2, 7), byrow = TRUE, ncol = 3) # Compute trace sum(diag(D)) [1] 12 # Compute determinant det(D) [1] 1.199041e-14 # Compute inverse MASS::fractions(solve(D)) Error in solve.default(D): system is computationally singular: reciprocal condition number = 3.72373e-17 Note: When you used R to compute the determinant of D you did not get 0; the determinant was computed as \\(1.20 \\times 10^{-14}=0.000000000000012\\). This is because R uses a computational method for computing the determinant called LU decomposition. We will examine this in a later chapter. Since rectangular matrices also have a main diagonal, we can also compute the trace of a rectangular matrix. However, in statistical and psychometric applications (e.g., principle components analysis), the trace is primarily computed on square matrices.↩︎ "],["eigenvalues-and-eigenvectors.html", "Chapter 14 Eigenvalues and Eigenvectors 14.1 Eigenvalues 14.2 Eigenvectors 14.3 Using R to Find Eigenvalues and Eigenvectors", " Chapter 14 Eigenvalues and Eigenvectors In this chapter, you will learn about eigenvalues and eigenvectors. Eigenvalues (a.k.a., characteristic roots) are scalars that are associated with linear systems of equations. Each eigenvalue has a corresponding vector, an eigenvector, associated with it. Eigenvalues and eigenvectors play a role in many statistical applications, featuring prominently in some methods of decomposition and principal component analysis (PCA). 14.1 Eigenvalues Let A be a \\(p \\times p\\) square matrix and I be a \\(p \\times p\\) identity matrix. Eigenvalues are the set of non-zero scalars \\(\\lambda_1,\\lambda_2, \\lambda_3,\\ldots,\\lambda_p\\) that satisfy, \\[ \\mathrm{det}\\bigg(\\mathbf{A} - \\lambda \\mathbf{I}\\bigg) = 0 \\] Consider an example, \\[ \\underset{2\\times 2}{\\mathbf{A}} = \\begin{bmatrix} -3 &amp; 5 \\\\ 4 &amp; -2 \\\\ \\end{bmatrix} \\] Our goal is to find the values of \\(\\lambda\\) that satisfy \\[ \\mathrm{det}\\bigg(\\begin{bmatrix} -3 &amp; 5 \\\\ 4 &amp; -2 \\\\ \\end{bmatrix} - \\lambda\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix}\\bigg) = 0 \\] Using matrix algebra, \\[ \\begin{split} \\mathrm{det}\\bigg(\\begin{bmatrix} -3 &amp; 5 \\\\ 4 &amp; -2 \\\\ \\end{bmatrix} - \\lambda\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix}\\bigg) &amp;= 0 \\\\[2ex] \\mathrm{det}\\bigg(\\begin{bmatrix} -3 &amp; 5 \\\\ 4 &amp; -2 \\\\ \\end{bmatrix} - \\begin{bmatrix} \\lambda &amp; 0 \\\\ 0 &amp; \\lambda \\\\ \\end{bmatrix}\\bigg) &amp;= 0 \\\\[2ex] \\mathrm{det}\\begin{bmatrix} -3 - \\lambda &amp; 5 \\\\ 4 &amp; -2 - \\lambda \\\\ \\end{bmatrix} &amp;= 0 \\\\[2ex] (-3 - \\lambda)(-2 - \\lambda) - 20 &amp;= 0 \\end{split} \\] Computing the determinant, distributing the first set of terms, subtracting 20 we obtain: \\[ \\begin{split} (-3 - \\lambda)(-2 - \\lambda) - 20 &amp;= 0 \\\\[1em] 6 + 3\\lambda + 2\\lambda + \\lambda^2 -20 &amp;= 0 \\\\[1em] \\lambda^2 + 5\\lambda - 14 &amp;= 0 \\\\[1em] \\end{split} \\] Factoring the left-hand side, \\[ (\\lambda + 7)(\\lambda - 2) = 0 \\] Solving for \\(\\lambda\\), we find that \\(\\lambda = -7\\) and \\(\\lambda = 2\\). We could also have applied the quadratic formula to solve this for \\(\\lambda\\). Remember the quadratic formula? The roots of the polynomial \\(Ax^2+Bx+C\\) are computed using: \\[ x = \\frac{-B\\pm \\sqrt{B^2-4AC}}{2A} \\] Quick recap at Khan Academy if you need it. Now that we have the eigenvalues we can double-check that \\(\\mathbf{A} - \\lambda \\mathbf{I}\\) is singular. (I will skip this here, but plug in the values for \\(\\lambda\\), one at a time, and ensure that the determinant is zero.) The equation you solved, \\[ \\mathrm{det}\\big(\\mathbf{A} - \\lambda \\mathbf{I}\\big)=0 \\] is referred to as the characteristic equation. Solving the characteristic equation gives the eigenvalues. Sometimes the eigenvalues are referred to as the characteristic roots of matrix A. 14.2 Eigenvectors If \\(\\lambda\\) is an eigenvalue of matrix A, then it is possible to find a vector v (an eigenvector) that satisfies \\[ \\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v} \\] In our previous example, A was a \\(2\\times2\\) matrix, so v will be a \\(2\\times1\\) vector to make the matrix multiplication work. \\[ \\underset{2\\times2}{\\mathbf{A}}~\\underset{2\\times1}{\\mathbf{v}} = \\lambda \\underset{2\\times1}{\\mathbf{v}} \\] Notice that we can also re-arrange the terms of this expression: \\[ \\begin{split} \\mathbf{A}\\mathbf{v} &amp;= \\lambda \\mathbf{v} \\\\[2ex] \\mathbf{A}\\mathbf{v} - \\lambda \\mathbf{v} &amp;= 0\\\\[2ex] \\big(\\mathbf{A} - \\lambda \\mathbf{I}\\big) \\mathbf{v} &amp;= 0\\\\[2ex] \\end{split} \\] In order to obtain a nonzero solution for v, the determinant of \\(\\mathbf{A} - \\lambda \\mathbf{I}\\) needs to be zero. This is why the eigenvalues are the \\(\\lambda\\)-values that make this nonsingular. 14.2.1 Finding the Elements of v We can use matrix algebra to solve for the elements of vector v using each eigenvalue seperately. \\[ \\begin{split} \\mathbf{A}\\mathbf{v} &amp;= \\lambda \\mathbf{v} \\\\[1em] \\begin{bmatrix} -3 &amp; 5 \\\\ 4 &amp; -2 \\\\ \\end{bmatrix}\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\end{bmatrix} &amp;= -7\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\end{bmatrix}\\\\[1em] \\end{split} \\] This produces a system of two equations with two unknowns: \\[ \\begin{split} -3v_1 + 5v_2 &amp;= -7v_1 \\\\ 4v_1 - 2v_2 &amp;= -7v_2 \\end{split} \\] Simplifying this, we get \\[ \\begin{split} 4v_1 + 5v_2 &amp;= 0 \\\\ 4v_1 + 5v_2 &amp;= 0 \\end{split} \\] The homogeneous set of equations means that there are an infinite number of solutions. The general solution here is to express one variable (say \\(v_2\\)) as a function of the other. \\[ \\begin{split} v_1 &amp;= \\theta \\\\[0.5em] v_2 &amp;= -\\frac{4}{5}\\theta \\end{split} \\] Any set of \\(v_1\\) and \\(v_2\\) in which \\(v_2 = -\\frac{4}{5}v_1\\) will satisfy this set of equations. 14.2.2 Normalized Eigenvectors Although there are an infinite number of solutions, one that is particularly nice is that whose a sum of squared values is equal to 1, that is \\(\\mathbf{v}^\\intercal\\mathbf{v}=1\\). This is referred to as normalizing the eigenvector. Normalized eigenvectors are denoted as e. \\[ \\begin{split} \\mathbf{e}^\\intercal \\mathbf{e} &amp;= 1 \\\\[2ex] e_1^2 + e_2^2 &amp;= 1 \\\\[2ex] \\theta^2 + (-\\frac{4}{5}\\theta)^2 &amp;= 1 \\\\[2ex] \\frac{41}{25}\\theta^2 &amp;= 1 \\\\[2ex] \\theta^2 &amp;= \\frac{25}{41} \\\\[2ex] \\theta &amp;= \\sqrt{\\frac{25}{41}} \\\\[2ex] &amp;=\\frac{5}{\\sqrt{41}} \\end{split} \\] Which implies that, \\[ \\begin{split} e_1 &amp;= \\frac{5}{\\sqrt{41}} \\\\[1em] e_2 &amp;= -\\frac{4}{\\sqrt{41}} \\end{split} \\] And the normalized eigenvector corresponding to the eigenvalue of \\(-7\\) is: \\[ \\mathbf{e}_1x = \\begin{bmatrix}\\frac{5}{\\sqrt{41}} \\\\ -\\frac{4}{\\sqrt{41}}\\end{bmatrix} \\] We could verify this by ensuring that the the equation \\(\\mathbf{A}\\mathbf{e}=\\lambda\\mathbf{e}\\) holds: \\[ \\begin{bmatrix} -3 &amp; 5 \\\\ 4 &amp; -2 \\\\ \\end{bmatrix}\\begin{bmatrix}\\frac{5}{\\sqrt{41}} \\\\ -\\frac{4}{\\sqrt{41}}\\end{bmatrix} = -7\\begin{bmatrix}\\frac{5}{\\sqrt{41}} \\\\ -\\frac{4}{\\sqrt{41}}\\end{bmatrix} \\] We can follow the same process for the second eigenvector which corresponds to the eigenvalue of 2. This produces an eigenvector of: \\[ \\mathbf{e}_2 = \\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{bmatrix} \\] Which can again be verified. 14.3 Using R to Find Eigenvalues and Eigenvectors We can use the eigen() function to find the eigenvalues and eigenvectors for a matrix. The syntax to find these for our example matrix A is: # Create A A = matrix( data = c(-3, 4, 5, -2), nrow = 2 ) # Compute eigenvalues and eigenvectors eigen(A) eigen() decomposition $values [1] -7 2 $vectors [,1] [,2] [1,] -0.7808688 -0.7071068 [2,] 0.6246950 -0.7071068 The eigenvectors for each of the eigenvalues are presented in the columns of the matrix given in the `\\(vector\\) component of the output. For example, the eigenvector associated with the eigenvalue of \\(-7\\) is presented in the first column of the matrix, and that associated with the eigenvalue of 2 is presented in the second column of the matrix. Note that the signs for all elements of any particular eigenvector can be switched since the vector would represent the same vector space. Thus the second eigenvector outputted here has negative values. Together, the eigenvalues and eigenvectors make up the eigenstructure of matrix A. In our example, the eigensructure of A is: \\[ \\begin{split} \\lambda_1 &amp;= -7 \\qquad &amp;\\mathbf{e}_1 = \\begin{bmatrix}\\frac{5}{\\sqrt{41}} \\\\ -\\frac{4}{\\sqrt{41}}\\end{bmatrix}\\\\[2ex] \\lambda_2 &amp;= 2 \\qquad &amp;\\mathbf{e}_2 = \\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{bmatrix} \\\\[2ex] \\end{split} \\] "],["statistical-application-sscp-variancecovariance-and-correlation-matrices.html", "Chapter 15 Statistical Application: SSCP, Variance–Covariance, and Correlation Matrices 15.1 Deviation Scores 15.2 SSCP Matrix 15.3 Correlation Matrix 15.4 Standardized Scores", " Chapter 15 Statistical Application: SSCP, Variance–Covariance, and Correlation Matrices In this chapter, we will provide an example of how matrix operations are used in statistics to compute a sum of squares and cross-product (SSCP) matrix. To do this we will consider the following data set: Table 15.1: Example set of education data. ID SAT GPA Self-Esteem IQ 1 560 3.0 11 112 2 780 3.9 10 143 3 620 2.9 19 124 4 600 2.7 7 129 5 720 3.7 18 130 6 380 2.4 13 82 The data matrix (omitting ID values) could be conceived of as a \\(6 \\times 4\\) matrix: \\[ \\mathbf{X} = \\begin{bmatrix} 560 &amp; 3.0 &amp; 11 &amp; 112 \\\\ 780 &amp; 3.9 &amp; 10 &amp; 143 \\\\ 620 &amp; 2.9 &amp; 19 &amp; 124 \\\\ 600 &amp; 2.7 &amp; 7 &amp; 129 \\\\ 720 &amp; 3.7 &amp; 18 &amp; 130 \\\\ 380 &amp; 2.4 &amp; 13 &amp; 82 \\end{bmatrix} \\] 15.1 Deviation Scores Since so much of statistics is dependent on the use of deviation scores, we will compute a matrix of means, where each column corresponds to the mean of the corresponding column mean of X, and subtract that from the original matrix X. \\[ \\begin{split} \\mathbf{D} &amp;= \\mathbf{X} - \\mathbf{M} \\\\[2ex] &amp;= \\begin{bmatrix} 560 &amp; 3.0 &amp; 11 &amp; 112 \\\\ 780 &amp; 3.9 &amp; 10 &amp; 143 \\\\ 620 &amp; 2.9 &amp; 19 &amp; 124 \\\\ 600 &amp; 2.7 &amp; 7 &amp; 129 \\\\ 720 &amp; 3.7 &amp; 18 &amp; 130 \\\\ 380 &amp; 2.4 &amp; 13 &amp; 82 \\end{bmatrix} - \\begin{bmatrix} 610 &amp; 3.1 &amp; 13 &amp; 120 \\\\ 610 &amp; 3.1 &amp; 13 &amp; 120 \\\\ 610 &amp; 3.1 &amp; 13 &amp; 120 \\\\ 610 &amp; 3.1 &amp; 13 &amp; 120 \\\\ 610 &amp; 3.1 &amp; 13 &amp; 120 \\\\ 610 &amp; 3.1 &amp; 13 &amp; 120 \\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} -50 &amp; -0.1 &amp; -2 &amp; -8 \\\\ 170 &amp; 0.8 &amp; -3 &amp; 23 \\\\ 10 &amp; -0.2 &amp; 6 &amp; 4 \\\\ -10 &amp; -0.4 &amp; -6 &amp; 9 \\\\ 110 &amp; 0.6 &amp; 5 &amp; 10 \\\\ -230 &amp; -0.7 &amp; 0 &amp; -38 \\end{bmatrix} \\end{split} \\] Using R, the following commands produce the deviation matrix. # Create X X = matrix( data = c(560, 3.0, 11, 112, 780, 3.9, 10, 143, 620, 2.9, 19, 124, 600, 2.7, 7, 129, 720, 3.7, 18, 130, 380, 2.4, 13, 82), byrow = TRUE, ncol = 4 ) # Create a ones column vector with six elements ones = rep(1, 6) # Compute M (mean matrix) M = ones %*% t(ones) %*% X * (1/6) M [,1] [,2] [,3] [,4] [1,] 610 3.1 13 120 [2,] 610 3.1 13 120 [3,] 610 3.1 13 120 [4,] 610 3.1 13 120 [5,] 610 3.1 13 120 [6,] 610 3.1 13 120 # Compute deviation matrix (D) D = X - M D [,1] [,2] [,3] [,4] [1,] -50 -0.1 -2 -8 [2,] 170 0.8 -3 23 [3,] 10 -0.2 6 4 [4,] -10 -0.4 -6 9 [5,] 110 0.6 5 10 [6,] -230 -0.7 0 -38 15.1.1 Creating the Mean Matrix Here we used matrix operations to compute the mean matrix rather than built-in R functions. Taking a ones column vector of n elements and post-multiplying by its transpose produces a \\(n \\times n\\) matrix where all the elements are one; in our case we get a \\(6 \\times 6\\) ones matrix. ones %*% t(ones) [,1] [,2] [,3] [,4] [,5] [,6] [1,] 1 1 1 1 1 1 [2,] 1 1 1 1 1 1 [3,] 1 1 1 1 1 1 [4,] 1 1 1 1 1 1 [5,] 1 1 1 1 1 1 [6,] 1 1 1 1 1 1 This ones matrix is then post-multiplied by X. Since we are post-multiplyinga ones matrix, the resultin matrix will have the same elements in every row. Moreover these elements will be the column sums of X. ones %*% t(ones) %*% X [,1] [,2] [,3] [,4] [1,] 3660 18.6 78 720 [2,] 3660 18.6 78 720 [3,] 3660 18.6 78 720 [4,] 3660 18.6 78 720 [5,] 3660 18.6 78 720 [6,] 3660 18.6 78 720 # Compute column sums colSums(X) [1] 3660.0 18.6 78.0 720.0 Finally, since we want the means, we multiply by the scalar \\(\\frac{1}{n}\\), where n is the number of rows. This gets us the mean matrix, all via matrix operations. ones %*% t(ones) %*% X * (1/6) [,1] [,2] [,3] [,4] [1,] 610 3.1 13 120 [2,] 610 3.1 13 120 [3,] 610 3.1 13 120 [4,] 610 3.1 13 120 [5,] 610 3.1 13 120 [6,] 610 3.1 13 120 Using mathematical notation, the deviation matrix can be expressed as: \\[ \\begin{split} \\underset{6 \\times 4}{\\mathbf{D}} &amp;= \\underset{6 \\times 4}{\\mathbf{X}} - \\underset{6 \\times 4}{\\mathbf{M}} \\\\[2ex] &amp;= \\begin{bmatrix} X_{1_{1}} &amp; X_{2_{1}} &amp; X_{3_{1}} &amp; X_{4_{1}} \\\\ X_{1_{2}} &amp; X_{2_{2}} &amp; X_{3_{2}} &amp; X_{4_{2}} \\\\ X_{1_{3}} &amp; X_{2_{3}} &amp; X_{3_{3}} &amp; X_{4_{3}} \\\\ X_{1_{4}} &amp; X_{2_{4}} &amp; X_{3_{4}} &amp; X_{4_{4}} \\\\ X_{1_{5}} &amp; X_{2_{5}} &amp; X_{3_{5}} &amp; X_{4_{5}} \\\\ X_{1_{6}} &amp; X_{2_{6}} &amp; X_{3_{6}} &amp; X_{4_{6}} \\end{bmatrix} - \\begin{bmatrix} \\bar{X}_1 &amp; \\bar{X}_2 &amp; \\bar{X}_3 &amp; \\bar{X}_4 \\\\ \\bar{X}_1 &amp; \\bar{X}_2 &amp; \\bar{X}_3 &amp; \\bar{X}_4 \\\\ \\bar{X}_1 &amp; \\bar{X}_2 &amp; \\bar{X}_3 &amp; \\bar{X}_4 \\\\ \\bar{X}_1 &amp; \\bar{X}_2 &amp; \\bar{X}_3 &amp; \\bar{X}_4 \\\\ \\bar{X}_1 &amp; \\bar{X}_2 &amp; \\bar{X}_3 &amp; \\bar{X}_4 \\\\ \\bar{X}_1 &amp; \\bar{X}_2 &amp; \\bar{X}_3 &amp; \\bar{X}_4 \\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} (X_{1_{1}} - \\bar{X}_1) &amp; (X_{2_{1}} - \\bar{X}_2) &amp; (X_{3_{1}} - \\bar{X}_3) &amp; (X_{4_{1}} - \\bar{X}_4) \\\\ (X_{1_{2}} - \\bar{X}_1) &amp; (X_{2_{2}} - \\bar{X}_2) &amp; (X_{3_{2}} - \\bar{X}_3) &amp; (X_{4_{2}} - \\bar{X}_4) \\\\ (X_{1_{3}} - \\bar{X}_1) &amp; (X_{2_{3}} - \\bar{X}_2) &amp; (X_{3_{3}} - \\bar{X}_3) &amp; (X_{4_{3}} - \\bar{X}_4) \\\\ (X_{1_{4}} - \\bar{X}_1) &amp; (X_{2_{4}} - \\bar{X}_2) &amp; (X_{3_{4}} - \\bar{X}_3) &amp; (X_{4_{4}} - \\bar{X}_4) \\\\ (X_{1_{5}} - \\bar{X}_1) &amp; (X_{2_{5}} - \\bar{X}_2) &amp; (X_{3_{5}} - \\bar{X}_3) &amp; (X_{4_{5}} - \\bar{X}_4) \\\\ (X_{1_{6}} - \\bar{X}_1) &amp; (X_{2_{6}} - \\bar{X}_2) &amp; (X_{3_{6}} - \\bar{X}_3) &amp; (X_{4_{6}} - \\bar{X}_4) \\end{bmatrix} \\end{split} \\] 15.2 SSCP Matrix To obtain the sums of squares and cross products (SSCP) matrix, we can pre-multiply D by its transpose. \\[ \\begin{split} \\underset{4 \\times 4}{\\mathbf{SSCP}} &amp;= \\underset{4 \\times 6}{\\mathbf{D}}^\\intercal\\underset{6 \\times 4}{\\mathbf{D}} \\\\[2ex] &amp;= \\begin{bmatrix} (X_{1_{1}} - \\bar{X}_1) &amp; (X_{1_{2}} - \\bar{X}_1) &amp; (X_{1_{3}} - \\bar{X}_1) &amp; (X_{1_{4}} - \\bar{X}_1) &amp; (X_{1_{5}} - \\bar{X}_1) &amp; (X_{1_{6}} - \\bar{X}_1) \\\\ (X_{2_{1}} - \\bar{X}_2) &amp; (X_{2_{2}} - \\bar{X}_2) &amp; (X_{2_{3}} - \\bar{X}_2) &amp; (X_{2_{4}} - \\bar{X}_2) &amp; (X_{2_{5}} - \\bar{X}_2) &amp; (X_{2_{6}} - \\bar{X}_2) \\\\ (X_{3_{1}} - \\bar{X}_3) &amp; (X_{3_{2}} - \\bar{X}_3) &amp; (X_{3_{3}} - \\bar{X}_3) &amp; (X_{3_{4}} - \\bar{X}_3) &amp; (X_{3_{5}} - \\bar{X}_3) &amp; (X_{3_{6}} - \\bar{X}_3) \\\\ (X_{4_{1}} - \\bar{X}_4) &amp; (X_{4_{2}} - \\bar{X}_4) &amp; (X_{4_{3}} - \\bar{X}_4) &amp; (X_{4_{4}} - \\bar{X}_4) &amp; (X_{4_{5}} - \\bar{X}_4) &amp; (X_{4_{6}} - \\bar{X}_4) \\end{bmatrix} \\begin{bmatrix} (X_{1_{1}} - \\bar{X}_1) &amp; (X_{2_{1}} - \\bar{X}_2) &amp; (X_{3_{1}} - \\bar{X}_3) &amp; (X_{4_{1}} - \\bar{X}_4) \\\\ (X_{1_{2}} - \\bar{X}_1) &amp; (X_{2_{2}} - \\bar{X}_2) &amp; (X_{3_{2}} - \\bar{X}_3) &amp; (X_{4_{2}} - \\bar{X}_4) \\\\ (X_{1_{3}} - \\bar{X}_1) &amp; (X_{2_{3}} - \\bar{X}_2) &amp; (X_{3_{3}} - \\bar{X}_3) &amp; (X_{4_{3}} - \\bar{X}_4) \\\\ (X_{1_{4}} - \\bar{X}_1) &amp; (X_{2_{4}} - \\bar{X}_2) &amp; (X_{3_{4}} - \\bar{X}_3) &amp; (X_{4_{4}} - \\bar{X}_4) \\\\ (X_{1_{5}} - \\bar{X}_1) &amp; (X_{2_{5}} - \\bar{X}_2) &amp; (X_{3_{5}} - \\bar{X}_3) &amp; (X_{4_{5}} - \\bar{X}_4) \\\\ (X_{1_{6}} - \\bar{X}_1) &amp; (X_{2_{6}} - \\bar{X}_2) &amp; (X_{3_{6}} - \\bar{X}_3) &amp; (X_{4_{6}} - \\bar{X}_4) \\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} \\sum (X_{1_{i}} - \\bar{X}_1)^2 &amp; \\sum (X_{1_{i}} - \\bar{X}_1)(X_{2_{i}} - \\bar{X}_2) &amp; \\sum (X_{1_{i}} - \\bar{X}_1)(X_{3_{i}} - \\bar{X}_3) &amp; \\sum (X_{1_{i}} - \\bar{X}_1)(X_{4_{i}} - \\bar{X}_4) \\\\ \\sum (X_{1_{i}} - \\bar{X}_1)(X_{2_{i}} - \\bar{X}_2) &amp; \\sum (X_{2_{i}} - \\bar{X}_2)^2 &amp; \\sum (X_{2_{i}} - \\bar{X}_2)(X_{3_{i}} - \\bar{X}_3) &amp; \\sum (X_{2_{i}} - \\bar{X}_2)(X_{4_{i}} - \\bar{X}_4) \\\\ \\sum (X_{1_{i}} - \\bar{X}_1)(X_{3_{i}} - \\bar{X}_3) &amp; \\sum (X_{2_{i}} - \\bar{X}_2)(X_{3_{i}} - \\bar{X}_3) &amp; \\sum (X_{3_{i}} - \\bar{X}_3)^2 &amp; \\sum (X_{3_{i}} - \\bar{X}_3)(X_{4_{i}} - \\bar{X}_4) \\\\ \\sum (X_{1_{i}} - \\bar{X}_1)(X_{4_{i}} - \\bar{X}_4) &amp; \\sum (X_{2_{i}} - \\bar{X}_2)(X_{4_{i}} - \\bar{X}_4) &amp; \\sum (X_{3_{i}} - \\bar{X}_3)(X_{4_{i}} - \\bar{X}_4) &amp; \\sum (X_{4_{i}} - \\bar{X}_4)^2 \\end{bmatrix} \\end{split} \\] The diagonal of the SSCP matrix contains the sums of squared deviations for each of the variables (columns) represented in X. For example, the element in the first row and first column of the SSCP matrix (\\(\\mathbf{SSCP}_{11}\\)) is the sum of squared deviations for the SAT variable. The off-diagonal elements in the SSCP matrix are the cross-products of the deviation scores between the different variables. That is: \\[ \\mathbf{SSCP} = \\begin{bmatrix} \\mathrm{SS}_1 &amp; \\mathrm{CP}_{12} &amp; \\mathrm{CP}_{13} &amp; \\mathrm{CP}_{14} \\\\ \\mathrm{CP}_{21} &amp; \\mathrm{SS}_2 &amp; \\mathrm{CP}_{23} &amp; \\mathrm{CP}_{24} \\\\ \\mathrm{CP}_{21} &amp; \\mathrm{CP}_{32} &amp; \\mathrm{SS}_3 &amp; \\mathrm{CP}_{34} \\\\ \\mathrm{CP}_{41} &amp; \\mathrm{CP}_{42} &amp; \\mathrm{CP}_{43} &amp; \\mathrm{SS}_4 \\end{bmatrix} \\] Also note that the SSCP matrix is square and symmetric. Using R: # Compute SSCP matrix SSCP = t(D) %*% D # View SSCP matrix SSCP [,1] [,2] [,3] [,4] [1,] 96600 370.0 260 14100.0 [2,] 370 1.7 2 47.4 [3,] 260 2.0 110 -33.0 [4,] 14100 47.4 -33 2234.0 The SSCP matrix is a scalar multiple of the variance–covariance matrix (\\(\\boldsymbol{\\Sigma}\\)). Namely, \\[ \\mathbf{SSCP} = n (\\boldsymbol{\\Sigma}) \\] That is if me multiply the SSCP matrix by \\(\\frac{1}{n}\\) we obtain the variance–covariance matrix. Mathematically, \\[ \\begin{split} \\underset{4 \\times 4}{\\boldsymbol{\\Sigma}} &amp;= \\frac{1}{n}(\\underset{4 \\times 4}{\\mathbf{SSCP}}) \\\\[2ex] &amp;= \\frac{1}{n} \\begin{bmatrix} \\sum (X_{1_{i}} - \\bar{X}_1)^2 &amp; \\sum (X_{1_{i}} - \\bar{X}_1)(X_{2_{i}} - \\bar{X}_2) &amp; \\sum (X_{1_{i}} - \\bar{X}_1)(X_{3_{i}} - \\bar{X}_3) &amp; \\sum (X_{1_{i}} - \\bar{X}_1)(X_{4_{i}} - \\bar{X}_4) \\\\ \\sum (X_{1_{i}} - \\bar{X}_1)(X_{2_{i}} - \\bar{X}_2) &amp; \\sum (X_{2_{i}} - \\bar{X}_2)^2 &amp; \\sum (X_{2_{i}} - \\bar{X}_2)(X_{3_{i}} - \\bar{X}_3) &amp; \\sum (X_{2_{i}} - \\bar{X}_2)(X_{4_{i}} - \\bar{X}_4) \\\\ \\sum (X_{1_{i}} - \\bar{X}_1)(X_{3_{i}} - \\bar{X}_3) &amp; \\sum (X_{2_{i}} - \\bar{X}_2)(X_{3_{i}} - \\bar{X}_3) &amp; \\sum (X_{3_{i}} - \\bar{X}_3)^2 &amp; \\sum (X_{3_{i}} - \\bar{X}_3)(X_{4_{i}} - \\bar{X}_4) \\\\ \\sum (X_{1_{i}} - \\bar{X}_1)(X_{4_{i}} - \\bar{X}_4) &amp; \\sum (X_{2_{i}} - \\bar{X}_2)(X_{4_{i}} - \\bar{X}_4) &amp; \\sum (X_{3_{i}} - \\bar{X}_3)(X_{4_{i}} - \\bar{X}_4) &amp; \\sum (X_{4_{i}} - \\bar{X}_4)^2 \\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)^2}{n} &amp; \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)(X_{2_{i}} - \\bar{X}_2)}{n} &amp; \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)(X_{3_{i}} - \\bar{X}_3)}{n} &amp; \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)(X_{4_{i}} - \\bar{X}_4)}{n} \\\\ \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)(X_{2_{i}} - \\bar{X}_2)}{n} &amp; \\frac{\\sum (X_{2_{i}} - \\bar{X}_2)^2}{n} &amp; \\frac{\\sum (X_{2_{i}} - \\bar{X}_2)(X_{3_{i}} - \\bar{X}_3)}{n} &amp; \\frac{\\sum (X_{2_{i}} - \\bar{X}_2)(X_{4_{i}} - \\bar{X}_4)}{n} \\\\ \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)(X_{3_{i}} - \\bar{X}_3)}{n} &amp; \\frac{\\sum (X_{2_{i}} - \\bar{X}_2)(X_{3_{i}} - \\bar{X}_3)}{n} &amp; \\frac{\\sum (X_{3_{i}} - \\bar{X}_3)^2}{n} &amp; \\frac{\\sum (X_{3_{i}} - \\bar{X}_3)(X_{4_{i}} - \\bar{X}_4)}{n} \\\\ \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)(X_{4_{i}} - \\bar{X}_4)}{n} &amp; \\frac{\\sum (X_{2_{i}} - \\bar{X}_2)(X_{4_{i}} - \\bar{X}_4)}{n} &amp; \\frac{\\sum (X_{3_{i}} - \\bar{X}_3)(X_{4_{i}} - \\bar{X}_4)}{n} &amp; \\frac{\\sum (X_{4_{i}} - \\bar{X}_4)^2}{n} \\end{bmatrix} \\end{split} \\] The diagonal of the \\(\\boldsymbol{\\Sigma}\\) matrix contains the variances for each of the variables (columns) represented in X. For example, the element in the first row and first column of \\(\\boldsymbol{\\Sigma}\\) (\\(\\boldsymbol{\\Sigma}_{11}\\)) is the variance for the SAT variable. The off-diagonal elements in \\(\\boldsymbol{\\Sigma}\\) are the covariances between the different variables. That is: \\[ \\boldsymbol{\\Sigma} = \\begin{bmatrix} \\mathrm{Var}(X_1) &amp; \\mathrm{Cov}(X_1,X_2) &amp; \\mathrm{Cov}(X_1,X_3) &amp; \\mathrm{Cov}(X_1,X_4) \\\\ \\mathrm{Cov}(X_2,X_1) &amp; \\mathrm{Var}(X_2) &amp; \\mathrm{Cov}(X_2,X_3) &amp; \\mathrm{Cov}(X_2,X_4) \\\\ \\mathrm{Cov}(X_3,X_1) &amp; \\mathrm{Cov}(X_3,X_2) &amp; \\mathrm{Var}(X_3) &amp; \\mathrm{Cov}(X_3,X_4) \\\\ \\mathrm{Cov}(X_4,X_1) &amp; \\mathrm{Cov}(X_4,X_2) &amp; \\mathrm{Cov}(X_4,X_3) &amp; \\mathrm{Var}(X_4) \\end{bmatrix} \\] Similar to the SSCP matrix, \\(\\boldsymbol{\\Sigma}\\) is also a square, symmetric matrix. Using R: # Compute SIGMA matrix SIGMA = SSCP * (1/6) # View SIGMA matrix SIGMA [,1] [,2] [,3] [,4] [1,] 16100.00000 61.6666667 43.3333333 2350.0000 [2,] 61.66667 0.2833333 0.3333333 7.9000 [3,] 43.33333 0.3333333 18.3333333 -5.5000 [4,] 2350.00000 7.9000000 -5.5000000 372.3333 The var() function can also be applied to X directly to obtain the \\(\\boldsymbol{\\Sigma}\\) matrix. Note that the var() function uses \\(\\frac{1}{n-1}\\) rather than \\(\\frac{1}{n}\\) as the scalar multiple to obtain the variance–covariance matrix. # Obtain SIGMA directly var(X) [,1] [,2] [,3] [,4] [1,] 19320 74.00 52.0 2820.00 [2,] 74 0.34 0.4 9.48 [3,] 52 0.40 22.0 -6.60 [4,] 2820 9.48 -6.6 446.80 # Check SSCP * (1/5) [,1] [,2] [,3] [,4] [1,] 19320 74.00 52.0 2820.00 [2,] 74 0.34 0.4 9.48 [3,] 52 0.40 22.0 -6.60 [4,] 2820 9.48 -6.6 446.80 15.3 Correlation Matrix We can convert \\(\\boldsymbol{\\Sigma}\\) to a correlation matrix by standardizing it; that is dividing each element by its corresponding standard deviation. This is equivalent to pre- and post-multiplying \\(\\boldsymbol{\\Sigma}\\) by a scaling matrix, S, a diagonal matrix with elements equal to the reciprocal of the standard deviations of each of the variables: \\[ \\mathbf{S} = \\begin{bmatrix} \\frac{1}{\\mathrm{SD}(X_1)} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{\\mathrm{SD}(X_2)} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{1}{\\mathrm{SD}(X_3)} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\frac{1}{\\mathrm{SD}(X_4)} \\end{bmatrix} \\] Employing R, we can obtain the scaling matrix by first pulling out the diagonal elements of \\(\\boldsymbol{\\Sigma}\\) (the variances) and then using those to create S. # Get variances V = diag(SIGMA) V [1] 1.610000e+04 2.833333e-01 1.833333e+01 3.723333e+02 # Create S S = diag(1 / sqrt(V)) S [,1] [,2] [,3] [,4] [1,] 0.007881104 0.000000 0.0000000 0.00000000 [2,] 0.000000000 1.878673 0.0000000 0.00000000 [3,] 0.000000000 0.000000 0.2335497 0.00000000 [4,] 0.000000000 0.000000 0.0000000 0.05182437 We can now pre- and post-multiply \\(\\boldsymbol{\\Sigma}\\) by S to obtain the correlation matrix, R. That is, \\[ \\mathbf{R} = \\mathbf{S}\\boldsymbol{\\Sigma}\\mathbf{S} \\] Since both S and \\(\\boldsymbol{\\Sigma}\\) are \\(4 \\times 4\\) matrices, R will also be a \\(4 \\times 4\\) matrix. Moreover, R will also be both square and symmetric. R = S %*% SIGMA %*% S R [,1] [,2] [,3] [,4] [1,] 1.00000000 0.9130377 0.07976061 0.95981817 [2,] 0.91303768 1.0000000 0.14625448 0.76915222 [3,] 0.07976061 0.1462545 1.00000000 -0.06656961 [4,] 0.95981817 0.7691522 -0.06656961 1.00000000 Again, the cor() function could be applied directly to X, noting that the SSCP matrix would be scaled by \\(\\frac{1}{n-1}\\). 15.4 Standardized Scores Standardized scores (z-scores) can be computed for the original values in X by post-multiplying the deviation matrix (D) by the same scaling matrix, S. \\[ \\mathbf{Z} = \\mathbf{DS} \\] # Compute standardized scores Z = D %*% S Z [,1] [,2] [,3] [,4] [1,] -0.39405520 -0.1878673 -0.4670994 -0.4145950 [2,] 1.33978769 1.5029383 -0.7006490 1.1919605 [3,] 0.07881104 -0.3757346 1.4012981 0.2072975 [4,] -0.07881104 -0.7514691 -1.4012981 0.4664193 [5,] 0.86692145 1.1272037 1.1677484 0.5182437 [6,] -1.81265393 -1.3150710 0.0000000 -1.9693261 We can also use Z to compute the correlation matrix, \\[ \\mathbf{R} = \\frac{\\mathbf{Z}^\\intercal\\mathbf{Z}}{n} \\] # Compute R t(Z) %*% Z * (1/6) [,1] [,2] [,3] [,4] [1,] 1.00000000 0.9130377 0.07976061 0.95981817 [2,] 0.91303768 1.0000000 0.14625448 0.76915222 [3,] 0.07976061 0.1462545 1.00000000 -0.06656961 [4,] 0.95981817 0.7691522 -0.06656961 1.00000000 "],["basis-vectors-and-matrices.html", "Chapter 16 Basis Vectors and Matrices 16.1 An Example 16.2 Changing Basis Vectors 16.3 Converting Non-Standard Coordinates to the Standard Basis 16.4 Final Thoughts", " Chapter 16 Basis Vectors and Matrices In this chapter, you will learn about basis vectors and basis matrices and how they are used to define the dimensionality of a coordinate space. You will also learn how to transition to different coordinate systems by changing the basis vectors. 16.1 An Example In the chapter Vectors, we learned that a vector could be represented in a coordinate system where each element in the vector corresponds to a distance along one of the reference axes defining the coordinate system. For example, the vector: \\[ \\mathbf{a} = \\begin{bmatrix} -3 \\\\ 4 \\end{bmatrix} \\] has a distance of \\(-3\\) on the first reference axis (R1) and a distance of 4 on the second reference axis (R2). This vector and the reference axes are shown in Figure 16.1. The reference axes (R1 and R2) define a two-dimensional coordinate space. Figure 16.1: Plot showing vector a (in teal) in the R1–R2 dimensional space. The two-dimensional coordinate system in our example, the Cartesian coordinate system, is defined by two orthogonal vectors called the basis vectors because they form the basis for the two-dimensional space. (These are shown in black in Figure 16.1.) More specifically, the dimensions of a system are determined by the number of independent vectors within that system. For example, in a three-dimensional coordinate space there are three independent vectors (e.g., that form the x, y, and z axes). The span of the basis vectors create what we think of as the axes for that system. In our example, we have two basis vectors whose spans create R1 and R2. There are many possible sets of basis vectors whose spans would create R1 and R2, but the standard basis is the set of elementary vectors, in two-dimensional space: \\[ \\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\qquad\\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\] Here the two basis vectors are orthogonal and have unit length. We refer to vectors that have these properties as orthonormal. We can represent any vector in the space as a linear combination of a set of scalars and the basis vectors. For example consider the vector: \\[ \\mathbf{a} = \\begin{bmatrix} -3 \\\\ 4 \\end{bmatrix} \\] We can also express a as: \\[ \\begin{split} \\mathbf{a} &amp;= -3 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 4 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\\\[2em] &amp;= -3\\mathbf{e}_1 + 4\\mathbf{e}_2 \\end{split} \\] In general a vector a is can be expressed relative to the basis vectors as: \\[ \\mathbf{a} = a_1(\\mathbf{e}_1) + a_2(\\mathbf{e}_2) + a_3(\\mathbf{e}_3) + \\dots + a_n(\\mathbf{e}_n) \\] where \\(\\mathbf{e}_i\\) is the ith orthonormal basis vector. 16.2 Changing Basis Vectors There is nothing sacred about the Cartesian coordinate system. We could also define the two-dimensional space using a different set of orthonormal basis vectors. For example consider the basis vectors: \\[ \\mathbf{b}_1 = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2} \\end{bmatrix} \\qquad\\mathbf{b}_2 = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{bmatrix} \\] These basis vectors still are orthonormal—they are orthogonal and their length is one. However, they are not elementary vectors. Not all orthonormal vectors are elementary vectors. R1 and R2 are still defined as the spans of these vectors, but now the vector a (which had the coordinates \\(-3, 4\\) in the system defined by the standard basis) has a different set of coordinates in the new basis. We can see this in 16.2. Figure 16.2: Plot showing vector a (in teal) in the R1–R2 dimensional space with basis vectors b1 and b2. The coordinates in the new basis for a is: \\[ \\mathbf{a}=\\begin{bmatrix} -4.95 \\\\ 0.707 \\end{bmatrix} \\] To find the coordinates of a vector a under a non-standard basis, we can express a using the new basis vectors as: \\[ \\begin{split} \\mathbf{a} &amp;= a_1(\\mathbf{b}_1) + a_2(\\mathbf{b}_2) \\\\[2ex] &amp;= a_1\\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2} \\end{bmatrix} + a_2\\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{bmatrix} \\end{split} \\] Then, since a has coordinates (\\(-3,4\\)) in the standard system, we can re-write this as: \\[ \\begin{split} \\begin{bmatrix}-3 \\\\ 4 \\end{bmatrix} &amp;= a_1\\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2} \\end{bmatrix} + a_2\\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} \\frac{\\sqrt{2}}{2} &amp; \\frac{\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2} &amp; \\frac{\\sqrt{2}}{2} \\end{bmatrix}\\begin{bmatrix}a_1 \\\\ a_2\\end{bmatrix} \\end{split} \\] Now we need to solve for the values of \\(a_1\\) and \\(a_2\\). Note the basis vectors form the columns of the basis matrix, and we can solve for the values of \\(a_1\\) and \\(a_2\\) by pre-multiplying both sides by the inverse of the basis matrix. # Create basis matrix B = matrix( data = c(sqrt(2)/2, -sqrt(2)/2, sqrt(2)/2, sqrt(2)/2), nrow = 2 ) # Create a a = matrix( data = c(-3, 4), nrow = 2 ) # Solve for the values of the new coordinates solve(B) %*% a [,1] [1,] -4.9497475 [2,] 0.7071068 16.2.1 Non-Orthonormal Basis There is no specification that the basis vectors need to be orthogonal, or have length of one. For example consider the basis vectors: \\[ \\mathbf{b}_1 = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{bmatrix} \\qquad\\mathbf{b}_2 = \\begin{bmatrix} 1.8 \\\\ 0.2 \\end{bmatrix} \\] Here the basis vectors are no longer orthogonal. We refer to non-orthogonal vectors as oblique. The second basis vector also has a length which is no longer one. In this basis, the vector a (which had the coordinates \\(-3, 4\\) in the system defined by the standard basis) has yet a different set of coordinates; seen in 16.3. Figure 16.3: Plot showing vector a (in teal) in the R1–R2 dimensional space with the oblique basis vectors b1 and b2. To find the coordinates in the oblique basis, we solve \\[ \\begin{bmatrix}-3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} &amp; 1.8 \\\\ \\frac{\\sqrt{2}}{2} &amp; 0.2 \\end{bmatrix} \\begin{bmatrix}a_1 \\\\ a_2\\end{bmatrix} \\] As long as the basis matrix is conformable, we can solve this for \\(a_1\\) and \\(a_2\\). # Create basis matrix B = matrix( data = c(sqrt(2)/2, sqrt(2)/2, 1.8, 0.2), nrow = 2 ) # Create a a = matrix( data = c(-3, 4), nrow = 2 ) # Solve for the values of the new coordinates solve(B) %*% a [,1] [1,] 6.894291 [2,] -4.375000 The coordinates for a in the new basis is: \\[ \\mathbf{a} = \\begin{bmatrix}6.894 \\\\ -4.375\\end{bmatrix} \\] 16.3 Converting Non-Standard Coordinates to the Standard Basis We can, given a set of coordinates for a vector in a non-standard basis system, transform this to the coordinates of a vector in the standrd basis system. For example, consider the vector a, which has coordinates defined in the basis given by B: \\[ \\mathbf{a} = \\begin{bmatrix}3 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} &amp; 1.8 \\\\ \\frac{\\sqrt{2}}{2} &amp; 0.2 \\end{bmatrix} \\] What are the coordinates of this vector in the standard basis system? 16.3 shows this graphically. Figure 16.4: Plot showing vector a (in teal) in the R1–R2 non-standard basis defined by b1 and b2. The coordinates in the standard basis are found by determining the projection onto the span of the standard basis vectors (i.e., the R1 and R2 axes in the standard basis). To find the coordinates of a in the standard basis system, we need to find the projections onto the spans of the standard basis vectors. To do this, we can use the same equation we have been using to convert coordinates to a non-standard basis. Recall, this equation is: \\[ \\mathbf{a}_{[\\mathrm{S}]} = \\mathbf{B}_{[\\mathrm{NS}]}\\mathbf{a}_{[\\mathrm{NS}]} \\] where \\(\\mathbf{a}_{[\\mathrm{S}]}\\) is the vector a in the standard basis, \\(\\mathbf{a}_{[\\mathrm{NS}]}\\) is the vector a in the non-standard basis, and \\(\\mathbf{B}_{[\\mathrm{NS}]}\\) is the basis matrix defining the non-standard system. To determine the coordinates for a in the standard system, we simply postmultiply the non-standard basis matrix by the vector a in the non-standard system. # Create basis matrix B = matrix( data = c(sqrt(2)/2, sqrt(2)/2, 1.8, 0.2), nrow = 2 ) # Create a (non-standard) a = matrix( data = c(3, 2), nrow = 2 ) # Find a (standard) B %*% a [,1] [1,] 5.72132 [2,] 2.52132 The coordinates for a in the standard basis sytem is: \\[ \\mathbf{a} = \\begin{bmatrix}5.721 \\\\ 2.521\\end{bmatrix} \\] 16.4 Final Thoughts Although we have been working in two dimensions so it is easier to show these ideas visually, basis vectors and matrices can, of course, be expanded to define coordinate systems in higher dimensions. For example, to define the standard three-dimensional space, we use the following basis vectors: \\[ \\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\qquad\\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\qquad\\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] which can be expressed as the basis matrix B: \\[ \\mathbf{B} = \\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] In general, to define an n-dimensional space, we need n basis vectors of dimension \\(n \\times 1\\), which create an \\(n \\times n\\) basis matrix. This implies that the basis matrix will be square. When we have a linear combinations that take on the form: \\[ \\underset{n \\times 1}{\\mathbf{y}} = \\underset{n \\times n}{\\mathbf{B}} ~\\underset{n \\times 1}{\\mathbf{x}} \\] where y and x are vectors and B is a square matrix, then y is the projection of the vector x (which is defined using the basis matrix B) onto the set of standard basis vectors. "],["quadratic-form-of-a-matrix.html", "Chapter 17 Quadratic Form of a Matrix 17.1 Linear Forms 17.2 Bilinear Form 17.3 Quadratic Form 17.4 Example: Quadratic Form 17.5 Positive Definite Matrices", " Chapter 17 Quadratic Form of a Matrix 🚧  🚧  🚧  🚧  🚧  🚧  🚧  🚧  🚧 UNDER CONSTRUCTION 🚧  🚧  🚧  🚧  🚧  🚧  🚧  🚧  🚧 In this chapter, you will learn about the quadratic forms of a matrix. The quadratic forms of a matrix comes up often in statistical applications. For example the sum of squares can be expressed in quadratic form. Similarly the SSCP, covariance matrix, and correlation matrix are also examples of the quadratic form of a matrix. Before we introduce the quadratic form of a matrix, we first examine the linear and bilinear forms of a matrix. 17.1 Linear Forms You are already quite familiar with the linear form of a matrix. The linear form of a matrix is simply a linear mapping of that matrix. In scalar algebraic notation, we might write: \\[ f(x) = a_1x_1 + a_2x_2 + a_3x_3 + \\ldots + a_nx_n \\] It is linear since the x-values are all to the first degree. We can, equivalently use matrix notation to express this as: \\[ f(\\mathbf{x}) = \\mathbf{a}^\\intercal\\mathbf{x} \\] where, \\[ \\mathbf{a}^\\intercal = \\begin{bmatrix} a_1 &amp; a_2 &amp; a_3 &amp; \\ldots &amp; a_n \\end{bmatrix} \\qquad \\mathrm{and} \\qquad \\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\] 17.2 Bilinear Form Bilinear form of a matrix extends the linear form by including two variables, x and y. For example, if we had: \\[ \\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\qquad \\mathbf{y}=\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} \\] Then the bilinear form written in scalar algebra is: \\[ f(x,y) = a_{11}x_1y_1 + a_{21}x_2y_1 + a_{31}x_3y_1 + a_{12}x_1y_2 + a_{22}x_2y_2 + a_{32}x_3y_2 \\] The linear part of bilinear implies that both variables are to the first power. And, only one x and one y appear in each term. Using matrix notation, we can write: \\[ f(\\mathbf{x},\\mathbf{y}) = \\mathbf{x}^\\intercal\\mathbf{A}\\mathbf{y} \\] where \\[ \\mathbf{A} = \\begin{bmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\\\ a_{31} &amp; a_{32} \\end{bmatrix} \\] We refer to the bilinear form of matrix A. Note that there is more than one bilinear form for A; by changing the values in the x and y vectors, we can obtain many different bilinear mappings. 17.3 Quadratic Form The quadratic form is a special case of the bilinear form in which \\(\\mathbf{x}=\\mathbf{y}\\). In this case we replace y with x so that we create terms with the different combinations of x: \\[ f(x,x) = a_{11}x_1y_1 + a_{21}x_2y_1 + a_{31}x_3y_1 + a_{12}x_1y_2 + a_{22}x_2y_2 + a_{32}x_3y_2 \\] A, simply means a linear function of a set of variables given in a vector x. Consider the following transformation of a square matrix A: \\[ \\mathbf{x}^\\intercal\\mathbf{Ax} \\] where A is a \\(k \\times k\\) matrix and x is a \\(k \\times 1\\) vector. This transformations is referred to as the quadratic transformation or the quadratic form of A. 17.4 Example: Quadratic Form Consider the following square matrix A: \\[ \\mathbf{A} = \\begin{bmatrix} -3 &amp; 5 \\\\ 4 &amp; -2 \\\\ \\end{bmatrix} \\] We can compute the quadratic form by using the vector \\[ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2\\end{bmatrix} \\] Then, \\[ \\begin{split} \\mathbf{x}^\\intercal\\mathbf{Ax} &amp;= \\begin{bmatrix} x_1 &amp; x_2\\end{bmatrix}\\begin{bmatrix} -3 &amp; 5 \\\\ 4 &amp; -2 \\\\ \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix} x_1 &amp; x_2\\end{bmatrix}\\begin{bmatrix} -3x_1 + 5x_2 \\\\ 4x_1 -2x_2 \\end{bmatrix} \\\\[2ex] &amp;= x_1(-3x_1 + 5x_2) + x_2(4x_1 -2x_2) \\\\[2ex] &amp;= -3x_1^2 + 5x_1x_2 + 4x_1x_2 -2x_2^2 \\\\[2ex] &amp;= -3x_1^2 + 9x_1x_2 -2x_2^2 \\end{split} \\] By looking at the exponents in the final expression, you can see why this is called a quadratic form or transformation of A. 17.5 Positive Definite Matrices Positive definite matrices come up a lot in statistical applications. For example, nonsingular correlation matrices and covariance matrices are positive definite matrices. A symmetric matrix is said to be positive definite if all of its eigenvalues are positive. An alternative definition is that a symmetric matrix is positive definite if pre-multiplying and post-multiplying it by the same vector always gives a positive number as a result, independently of how we choose the vector. Mathematically, A is positive definite if: \\[ \\mathbf{v}^\\intercal \\mathbf{Av} &gt; 0 \\] These two definitions are equivalent. "],["systems-of-equations.html", "Chapter 18 Systems of Equations 18.1 Solving Systems of Equations with Matrix Algebra 18.2 Linear Dependence and Independence 18.3 Singularity 18.4 Rank of a Matrix 18.5 Rank and the Solution of Systems of Equations 18.6 Exercises", " Chapter 18 Systems of Equations One thing that matrix algebra is really useful for is solving systems of equations. For example, consider the following system of two equations: \\[ \\begin{split} 3x + 4y &amp;= 10 \\\\[2ex] 2x + 3y &amp;= 7 \\end{split} \\] The set of equations is referred to as a system of equations, and “solving” this system of equations is akin to determining the values of x and y that satisfy all equations in the system. In an algebra class, you may have learned to solve these equations by graphing them, using substitution, or elimination. Based on these methods we would find that \\(x=2\\) and \\(y=1\\). Since this system has a solution, we say the system of equations is consistent. If this were the only set of x- and y-values that could simultaneously solve all of the equations, then this consistent system is said to have a unique solution. In other systems of equations, there may be an infinite number of x- and y-values that provide solutions (referred to as a dependent system of equations) or no x- and y-values that offer a solution (an inconsistent system of equations). Graphically, a consistent system of equations with a unique solution intersect at a unique point. In our example, the two lines defined by the equations would intersect at the point \\((2,1)\\). If the system of equations were dependent, the two equations would form the same line. For example, \\[ \\begin{split} x + y &amp;= 6 \\\\[2ex] 2x + 2y &amp;= 12 \\end{split} \\] In this example, every \\((x,y)\\) value that lies on the two lines is a potential solution for the system; and there are an infinite number of these solutions. In an inconsistent system of equations, the two lines do not intersect; they are parallel.9 For example, the following system of equations is inconsistent: \\[ \\begin{split} x + y &amp;= 5 \\\\[2ex] x + y &amp;= 10 \\end{split} \\] These two lines are parallel (do not intersect), so there is not any values of x and y that satisfy both equations. Note that the second equation in the system has been changed disproportionately from the first; namely the left-side of the first equation was multiplied by one to obtain the left-side of the second equation and the right-side of the first equation was multiplied by two. Algebraically, inconsistent systems share this characteristic that one side of the equation has been disproportionately changed from the other side. 18.1 Solving Systems of Equations with Matrix Algebra We can also use matrix algebra to solve systems of equations. To do this, we will re-visit the earlier system of equations: \\[ \\begin{split} 3x + 4y &amp;= 10 \\\\[2ex] 2x + 3y &amp;= 7 \\end{split} \\] We first need to write this system of equations using matrices. Because there are two equations, we can initially formulate each side of the equality as a \\(2 \\times 1\\) matrix: \\[ \\begin{bmatrix} 3x + 4y \\\\ 2x + 3y \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 7 \\end{bmatrix} \\] Now, we can re-write the left-hand side of the equality as the product of two matrices: \\[ \\begin{bmatrix} 3 &amp; 4 \\\\ 2 &amp; 3 \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 7 \\end{bmatrix} \\] Note that the elements of the “left” (premultiplied) matrix on the left-hand side of the equality are composed of the four coefficients that make up the system of equations. The “right” (postmultiplied) matrix on the left-hand side of the equality is a column matrix composed of the unknowns in the original system of equations. The matrix on the right-hand side of the equality is a column matrix made up of the equality values in the original system of equations. Note that the matrix multiplication on the left-hand side of the equality is defined and produces the appropriately dimensioned matrix on the right-hand side of the equality. \\[ \\underset{2\\times 2}{\\begin{bmatrix} 3 &amp; 4 \\\\ 2 &amp; 3 \\end{bmatrix}}\\underset{2\\times 1}{\\begin{bmatrix} x \\\\ y \\end{bmatrix}} = \\underset{2\\times 1}{\\begin{bmatrix} 10 \\\\ 7 \\end{bmatrix}} \\] If we use a more general notation, \\[ \\underset{n\\times n}{\\mathbf{A}}~\\underset{n\\times 1}{\\mathbf{X}} = \\underset{n\\times 1}{\\mathbf{C}} \\] where n is the number of equations (and unknowns) in the system, A is a matrix representing the coefficients associated with each of the unknowns in the system equations, and C is the matrix representing the scalar equality values. To solve this general equation, we premultiply both sides of the equation by \\(\\mathbf{A}^{-1}\\). \\[ \\mathbf{A}^{-1}\\mathbf{A}\\mathbf{X} = \\mathbf{A}^{-1}\\mathbf{C} \\] This will result in \\[ \\begin{split} (\\mathbf{A}^{-1}\\mathbf{A})\\mathbf{X} &amp;= \\mathbf{A}^{-1}\\mathbf{C} \\\\[2ex] \\mathbf{I}\\mathbf{X} &amp;= \\mathbf{A}^{-1}\\mathbf{C} \\\\[2ex] \\mathbf{X} &amp;= \\mathbf{A}^{-1}\\mathbf{C} \\end{split} \\] Thus, we can solve for the elements in X (x and y) by premultiplying C by the inverse of A. In our example, \\[ \\begin{split} \\overset{\\mathbf{A}}{\\begin{bmatrix} 3 &amp; 4 \\\\ 2 &amp; 3 \\end{bmatrix}}\\overset{\\mathbf{X}}{\\begin{bmatrix} x \\\\ y \\end{bmatrix}} &amp;= \\overset{\\mathbf{C}}{\\begin{bmatrix} 10 \\\\ 7 \\end{bmatrix}} \\\\[2ex] \\overset{\\mathbf{A}^{-1}}{\\begin{bmatrix} 3 &amp; -4 \\\\ -2 &amp; 3 \\end{bmatrix}}\\overset{\\mathbf{A}}{\\begin{bmatrix} 3 &amp; 4 \\\\ 2 &amp; 3 \\end{bmatrix}}\\overset{\\mathbf{X}}{\\begin{bmatrix} x \\\\ y \\end{bmatrix}} &amp;= \\overset{\\mathbf{A}^{-1}}{\\begin{bmatrix} 3 &amp; -4 \\\\ -2 &amp; 3 \\end{bmatrix}}\\overset{\\mathbf{C}}{\\begin{bmatrix} 10 \\\\ 7 \\end{bmatrix}} \\\\[2ex] \\overset{\\mathbf{I}}{\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}}\\overset{\\mathbf{X}}{\\begin{bmatrix} x \\\\ y \\end{bmatrix}} &amp;= \\overset{\\mathbf{A}^{-1}\\mathbf{C}}{\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}} \\\\[2ex] \\overset{\\mathbf{X}}{\\begin{bmatrix} x \\\\ y \\end{bmatrix}} &amp;= \\overset{\\mathbf{A}^{-1}\\mathbf{C}}{\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}} \\\\[2ex] \\end{split} \\] Because the two matrices here, \\(\\mathbf{X}\\) and \\(\\mathbf{A}^{-1}\\mathbf{C}\\), are equal, all of the elements in the same position are equal. Thus \\(x=2\\) and \\(y=1\\). 18.1.1 Solving this System of Equations Using R We can use R to carry out this matrix algebra. # Create A A = matrix( data = c(3, 2, 4, 3), nrow = 2 ) # Create C C = matrix( data = c(10, 7), nrow = 2 ) # Solve for X solve(A) %*% C [,1] [1,] 2 [2,] 1 What happens when we try to solve for a consistent system of equations that have an infinite number of solutions? Using our example from above: # Create A A = matrix( data = c(1, 2, 1, 2), nrow = 2 ) # Create C C = matrix( data = c(6, 12), nrow = 2 ) # Solve for X solve(A) %*% C Error in solve.default(A): Lapack routine dgesv: system is exactly singular: U[2,2] = 0 We cannot solve this system of equations because the inverse of A is singular (the determinant of A is zero). What happens when we try to solve for a dependent system of equations? Using our example from above: # Create A A = matrix( data = c(1, 1, 1, 1), nrow = 2 ) # Create C C = matrix( data = c(5, 10), nrow = 2 ) # Solve for X solve(A) %*% C Error in solve.default(A): Lapack routine dgesv: system is exactly singular: U[2,2] = 0 We also cannot solve this system of equations because the inverse of A is singular (the determinant of A is zero). 18.2 Linear Dependence and Independence We recognize that when a redundant equation is present in a system of equations, the matrix of coefficients (A) is singular, the determinant is zero. This is one definition of a linearly dependent system of equations. When none of the equations in a system are redundant with any other equation within the system, the system of equations is linearly independent. Consider a vector z where \\(\\mathbf{z} = a\\mathbf{x} + b\\mathbf{y}\\). That is, z is a linear function of the vectors x and y. (We could also say z is linearly dependent on x and y.) When we have used vector addition or vector subtraction, we were actually creating linear combinations of vectors where the scalars were all equal to \\(+1\\) or \\(–1\\). For example, \\(\\mathbf{z} = \\mathbf{x} + \\mathbf{y}\\) is mathematically equivalent to \\(\\mathbf{z} = 1\\mathbf{x} + 1\\mathbf{y}\\). In this case, the coefficients for x and y are both \\(+1\\); \\(a=1\\) and \\(b=1\\). Formally, a set of vectors, \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots \\mathbf{a}_n\\), is linearly dependent if there exists a set of scalars (not all zero) where: \\[ c_1\\mathbf{a}_1 + c_2 \\mathbf{a}_2 + \\ldots + c_n\\mathbf{a}_n = 0 \\] If no set of scalars exist to satisfy this condition, the set of vectors is linearly independent. For the above condition to be true, at least one of the vectors must be redundant with another, it can be expressed as a linear combination of the others. Linear dependence implies redundancy among at least two of the vectors. For example consider the following three vectors: \\[ \\mathbf{x} = \\begin{bmatrix}4\\\\6\\end{bmatrix} \\qquad \\mathbf{y}= \\begin{bmatrix}2\\\\1\\end{bmatrix} \\qquad \\mathbf{z}= \\begin{bmatrix}6\\\\11\\end{bmatrix} \\] These three vectors can be written as: \\[ \\begin{split} 2\\mathbf{x} - 1 \\mathbf{y} -1 \\mathbf{z} &amp;= 0 \\\\[2ex] 2\\begin{bmatrix}4\\\\6\\end{bmatrix} - 1 \\begin{bmatrix}2\\\\1\\end{bmatrix} - 1 \\begin{bmatrix}6\\\\11\\end{bmatrix} &amp;= 0 \\end{split} \\] Since we have a set of scalars \\(\\{2,-1,-1\\}\\) that satisfy this equation where none of the scalars are zero, this system of equations is linearly dependent. 18.3 Singularity Recall that in a singular matrix, the determinant if the matrix is zero. A matrix that is singular implies that one or more of the rows (or columns) is linearly dependent; there exists a set of scalars that satisfies our definition. Earlier we saw that: \\[ \\mathbf{A} = \\begin{bmatrix}1 &amp; 1 \\\\ 2 &amp; 2\\end{bmatrix} \\] was singular. Namely, \\[ \\begin{split} \\vert\\mathbf{A}\\vert &amp;= 1(2) - 2(1) \\\\[2ex] &amp;= 0 \\end{split} \\] We can also find scalars \\(c_1\\) and \\(c_2\\) such that the row vectors (or column vectors) of A can be written as \\(c_1\\mathbf{a}_1 + c_2 \\mathbf{a}_2 = 0\\). Using the two row vectors: \\[ \\begin{split} 2\\begin{bmatrix}1\\\\1\\end{bmatrix} - 1 \\begin{bmatrix}2\\\\2\\end{bmatrix} &amp;= 0 \\\\[2ex] 2\\mathbf{a}_1 - 1 \\mathbf{a}_2 &amp;= 0 \\end{split} \\] In general, when A is singular the rows are linearly dependent. The columns are similarly linearly dependent when A is singular. There is a theorem in linear algebra that suggests that any n-element vector can be written as a linear combination of n linearly independent n-element vectors. Finding such vectors can be difficult, but we rely on this theorem in statistics. 18.4 Rank of a Matrix The column rank of a matrix describes the dimensionality of the column space for that matrix, and the row rank describes the dimensionality of the row space for that matrix. It turns out that the column and row ranks of a matrix are the same, so we just refer to the dimensionality, or rank of a matrix. Because dimensionality is related to the independence of the rows/columns, the rank of a matrix is also the number of rows or columns that are linearly independent: \\[ \\mathrm{Rank}(\\mathbf{A}) = \\begin{cases}\\mathrm{number~of~linearly~independent~rows~of~}\\mathbf{A} \\\\ \\mathrm{number~of~linearly~independent~columns~ of~}\\mathbf{A}\\end{cases} \\] For an \\(n \\times p\\) dimensional matrix A, the maximum possible rank is the smaller value of n and p. That is, \\[ 0 \\leq \\mathrm{Rank}(\\underset{n\\times p}{\\mathbf{A}}) \\leq \\min(n,p) \\] The rank of a matrix would be zero only if the matrix were a null matrix. For all other matrices, the minimum possible rank would be one. When all of the row and column vectors in a matrix are linearly independent the matrix is said to be of full rank. That is, when the rank of A is the maximum possible rank, it is of full rank. When the rank of A is smaller than both n and p, the matrix is said to be of deficient rank. This implies that at least one of the row (or column) vectors is linearly dependent. It turns out only a square matrix can be of full rank—rectangular matrices will always have a rank less than either n or p. For example, consider the following matrices: \\[ \\mathbf{A} = \\begin{bmatrix}3 &amp; 5 \\\\ 1 &amp; 2\\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}1 &amp; 3 &amp; 5 \\\\ 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 5 &amp; 8\\end{bmatrix} \\] The rank of A is 2, since the two ro vectors (or the two column vectors) are linearly independent. Since 2 is the maximum possible rank of a \\(2 \\times 2\\) matrix, we consider A to be of full rank. Matrix B, on the other hand, is of deficient rank. This is because the rank of B is 2, which is less than the maximum possible rank of 3 for a \\(3\\times 3\\) matrix. This means that two of the three rows (or columns) of B are linearly independent. It turns out the third row of B is a linear combination of the first and second rows: \\[ \\overset{\\mathrm{Row~3}}{\\begin{bmatrix}2 &amp; 5 &amp; 8\\end{bmatrix}} = \\overset{\\mathrm{Row~1}}{\\begin{bmatrix}1 &amp; 3 &amp; 5\\end{bmatrix}} + \\overset{\\mathrm{Row~2}}{\\begin{bmatrix}1 &amp; 2 &amp; 3\\end{bmatrix}} \\] We can compute the rank of a matrix by using the rankMatrix() function from the Matrix package. # Create A A = matrix( data = c(3, 5, 1, 2), byrow = TRUE, ncol = 2 ) # Compute rank of A Matrix::rankMatrix(A) [1] 2 attr(,&quot;method&quot;) [1] &quot;tolNorm2&quot; attr(,&quot;useGrad&quot;) [1] FALSE attr(,&quot;tol&quot;) [1] 4.440892e-16 # Create B B = matrix( data = c(1, 3, 5, 1, 2, 3, 2, 5, 8), byrow = TRUE, ncol = 3 ) # Compute rank of B Matrix::rankMatrix(B) [1] 2 attr(,&quot;method&quot;) [1] &quot;tolNorm2&quot; attr(,&quot;useGrad&quot;) [1] FALSE attr(,&quot;tol&quot;) [1] 6.661338e-16 The determinant of a matrix also gives us insight into whether a matrix is of full rank. Singular matrices (determinant = 0) are of deficient rank, while non-singular matrices are of full rank. # Compute determinant det(A) [1] 1 # Compute determinant det(B) [1] 0 All of this implies that if A is non-singular (\\(\\lvert \\mathbf{A} \\rvert \\neq 0\\)), then: A has an inverse; \\(\\mathrm{rank}(\\mathbf{A}) = n\\), or A is full rank; All rows of A are linearly independent; and All columns of A are linearly independent. 18.5 Rank and the Solution of Systems of Equations Remember, our goal was to use matrix algebra to solve a system of equations. In order to do this we need to evaluate the independence of the system of equations. Consider the following system of equations: \\[ \\begin{split} x_1 + x_2 + x_3 &amp;= 5 \\\\[2ex] 2x_1 – 1x_2 + 6x_3 &amp;= 12 \\\\[2ex] x_1 + 3x_2 + 5x_3 &amp;= 17 \\end{split} \\] The coefficient matrix (\\(\\mathbf{A}\\)) and the augmented coefficient matrix (\\(\\mathbf{A}\\vert \\mathbf{c}\\)) are: \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; – 1 &amp; 6\\\\ 1 &amp; 3 &amp; 5 \\end{bmatrix} \\qquad \\mathbf{A}\\vert \\mathbf{c} = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 5\\\\ 2 &amp; – 1 &amp; 6 &amp; 12\\\\ 1 &amp; 3 &amp; 5 &amp; 17 \\end{bmatrix} \\] Note that the augmented coefficient matrix is simply the coefficient matrix with an appended column that include the solutions from the system of equations (the c vector from \\(\\mathbf{Ax}=\\mathbf{c}\\)). A theorem in linear algebra states that a system of equations is independent (consistent) if the rank of the coefficient matrix is the same as the rank of the augmented coefficient matrix, \\(\\mathrm{rank}(\\mathbf{A}) = \\mathrm{rank}(\\mathbf{A}\\vert \\mathbf{c})\\). Examining the rank of A, we find: Computing the rank of A: # Create A A = matrix( data = c(1, 1, 1, 2, -1, 6, 1, 3, 5), byrow = TRUE, ncol = 3 ) # Create A|y A_y = matrix( data = c(1, 1, 1, 5, 2, -1, 6, 12, 1, 3, 5, 17), byrow = TRUE, ncol = 4 ) # Compute rank of A Matrix::rankMatrix(A) [1] 3 attr(,&quot;method&quot;) [1] &quot;tolNorm2&quot; attr(,&quot;useGrad&quot;) [1] FALSE attr(,&quot;tol&quot;) [1] 6.661338e-16 # Compute rank of A|y Matrix::rankMatrix(A_y) [1] 3 attr(,&quot;method&quot;) [1] &quot;tolNorm2&quot; attr(,&quot;useGrad&quot;) [1] FALSE attr(,&quot;tol&quot;) [1] 8.881784e-16 The rank of A is 3, which means A is of full rank. This also implies that A is non-singular (\\(\\lvert \\mathbf{A}\\rvert \\neq 0\\)), and that A has an inverse. Furthermore, since the rank of \\(\\mathbf{A}\\vert \\mathbf{c}\\) is also 3, the system of equations is consistent (linearly independent). This means that we can solve the equations using: \\[ \\begin{split} \\mathbf{x} &amp;= \\mathbf{A}^{-1}\\mathbf{y}\\\\[2ex] \\begin{bmatrix} x_1 \\\\ x_2\\\\ x_3 \\end{bmatrix} &amp;= \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; – 1 &amp; 6\\\\ 1 &amp; 3 &amp; 5 \\end{bmatrix}^{-1} \\begin{bmatrix} 5 \\\\ 12\\\\ 17 \\end{bmatrix} \\end{split} \\] Computing this, we find \\(x_1 = 1\\), \\(x_2=2\\), and \\(x_3=2\\). # Create y y = matrix( data = c(5, 12, 17), ncol = 1 ) # Solve system of equations solve(A) %*% y [,1] [1,] 1 [2,] 2 [3,] 2 Another theorem of linear algebra provides a way to evaluate whether a consistent set of equations has an infinite number of solutions. A consistent system of n equations in n unknowns has a unique solution if the rank of the coefficient matrix is equal to its order, that is \\(\\mathrm{rank}(\\mathbf{A}) = n\\). In the example, there are three equations and three unknowns and the rank of the coefficient matrix, A, is 3. The solution we have computed is unique. Another way to state this is: A consistent system of equations where A is of order n has a unique solution if and only if \\(\\mathbf{A}^{-1}\\) exists. This is important in applications where we have the same number of unknowns as equations. When the number of equations equals the number of unknowns, and \\(\\lvert \\mathbf{A} \\rvert \\neq 0\\), there is a unique solution. That is, when A is full rank, a unique solution exists for \\(\\mathbf{Ax} = \\mathbf{y}\\). This is true because: If \\(\\lvert \\mathbf{A} \\rvert \\neq 0\\), then \\(\\mathrm{rank}(\\mathbf{A}) = n\\). If \\(\\mathrm{rank}(\\mathbf{A} \\vert \\mathbf{c}) = \\mathrm{rank}(\\mathbf{A})\\), the equations are consistent. If the equations are consistent and \\(\\mathrm{rank}(\\mathbf{A}) = n\\), there is a unique solution. Figure 18.1 shows a flowchart that you can use to determine whether a system of equations is consistent or not, and if so, whether there is a unique solution or whether it has an infinite number of solutions. Figure 18.1: Flowchart to determine whether a system of equations is consistent or not, and how many solutions exist. 18.6 Exercises Consider the following matrices: \\[ \\mathbf{A} = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 1 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}1 &amp; 2 &amp; 3\\\\4 &amp; 5 &amp; 6\\\\7 &amp; 8 &amp; 9 \\end{bmatrix} \\qquad \\mathbf{C} = \\begin{bmatrix}2 &amp; 3 &amp; 8 \\\\15 &amp; 5 &amp; 9\\\\6 &amp; 9 &amp; 24 \\end{bmatrix} \\] Determine which of the matrices (A, B, and C) have linearly independent rows and which have linearly dependent rows. Show/Hide Solution All rows in A are linearly independent, since it has a rank of 3. The rank of B is 2, which implies that there is a linearly dependent row. There are a number of other ways to represent this dependency. For example, \\(\\mathrm{Row~3} = 2(\\mathrm{Row~2}) - 1(\\mathrm{Row~1})\\). The rank of C is also 2, which implies that there is a linearly dependent row. There are a number of other ways to represent this dependency. For example, \\(\\mathrm{Row~3} = 3(\\mathrm{Row~2}) + 0(\\mathrm{Row~1})\\). Consider the following system of equations: \\[ \\begin{split} 2(x_1) + x_2 &amp;= 6 \\\\[2ex] x_1 + 3(x_2) &amp;= 8 \\end{split} \\] 2. Write out the coefficient matrix (A) and the augmented coefficient matrix (\\(\\mathbf{A} \\vert \\mathbf{c}\\)). Show/Hide Solution \\[ \\mathbf{A} = \\begin{bmatrix}2 &amp; 1 \\\\ 1 &amp; 3 \\end{bmatrix} \\qquad \\mathbf{A} \\vert \\mathbf{c} = \\begin{bmatrix}2 &amp; 1 &amp; 6 \\\\ 1 &amp; 3 &amp; 8 \\end{bmatrix} \\] Use the ranks of \\(\\mathbf{A}\\) and \\(\\mathbf{A} \\vert \\mathbf{c}\\) to determine whether the system of equations is consistent. Show/Hide Solution The ranks of the two matrices are: \\(\\mathrm{Rank}(\\mathbf{A}) = 2\\) \\(\\mathrm{Rank}(\\mathbf{A} \\vert \\mathbf{c}) = 2\\) Since the rank of the coefficient matrix and the augmented coefficient matrix are equal, the system of equations is consistent. Based on the rank of the coefficient matrix, will there be a unique solution to the system of equations? Show/Hide Solution Since A is of full rank, the solution will be unique. Use R to solve the system of equations. Show/Hide Solution # Create A A = matrix( data = c(2, 1, 1, 3), ncol = 2 ) # Create y y = matrix( data = c(6, 8), ncol = 1 ) # Solve system of equations solve(A) %*% y [,1] [1,] 2 [2,] 2 Consider the following system of equations: \\[ \\begin{split} 25(x_1) + 5(x_2) + x_3 &amp;= 106.8 \\\\[2ex] 64(x_1) + 8(x_2) + x_3 &amp;= 177.2 \\\\[2ex] 89(x_1) + 13(x_2) + 2(x_3) &amp;= 284.0 \\\\[2ex] \\end{split} \\] 6. Use R to compute the determinant of the coefficient matrix. Based on the determinant, will there be a unique solution to the system of equations? Show/Hide Solution # Create A A = matrix( data = c(25, 5, 1, 64, 8, 1, 89, 13, 2), byrow = TRUE, ncol = 3 ) # Compute determinant det(A) [1] 0 Since A is singular, there is not a unique solution to the system of equations. (The rank of the coefficient matrix and the augmented coefficient matrix are both 2, so the system is consistent. And, since there is no unique solution, this means there are an infinite number of solutions.) Another way to think about dependent systems of equations is that they are redundant.↩︎ "],["statistical-appplication-estimating-regression-coefficients.html", "Chapter 19 Statistical Appplication: Estimating Regression Coefficients 19.1 Regression Model 19.2 Estimating the Regression Coefficients 19.3 The \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) Matrix", " Chapter 19 Statistical Appplication: Estimating Regression Coefficients In this chapter, you will learn about how matrix algebra is used to compute regression coefficients. 19.1 Regression Model Recall the model equation we use in linear regression: \\[ Y_i = \\beta_0 + \\beta_1(X_{1_i}) + \\beta_2(X_{2_i}) + \\ldots + \\beta_k(X_{k_i}) + \\epsilon_i \\] where the response variable (Y) is represented as a linear function of the set of predictors \\(X_1,X_2,\\ldots,X_k\\)) and a residual (\\(\\epsilon\\)). Equation terms with an i subscript vary across subjects. Terms without an i subscript are the same (fixed) across subjects. Recall that this notation is a mathematical expression of the n subject-specific equations: \\[ \\begin{split} Y_1 &amp;= \\beta_0 + \\beta_1(X_{1_1}) + \\beta_2(X_{2_1}) + \\ldots + \\beta_k(X_{k_1}) + \\epsilon_1 \\\\ Y_2 &amp;= \\beta_0 + \\beta_1(X_{1_2}) + \\beta_2(X_{2_2}) + \\ldots + \\beta_k(X_{k_2}) + \\epsilon_2 \\\\ Y_3 &amp;= \\beta_0 + \\beta_1(X_{1_3}) + \\beta_2(X_{2_3}) + \\ldots + \\beta_k(X_{k_3}) + \\epsilon_3 \\\\ \\vdots &amp;~ ~~~~~\\vdots ~~~~~~~~~~\\vdots~~~~~~~~~~~~~~~~~~~\\vdots~~~~~~~~~~~~~~\\vdots~~~~~~~~~~~~~\\vdots~~~~~~~~~~~\\vdots \\\\ Y_n &amp;= \\beta_0 + \\beta_1(X_{1_n}) + \\beta_2(X_{2_n}) + \\ldots + \\beta_k(X_{k_n}) + \\epsilon_n \\end{split} \\] These can be arranged into a set of vectors and matrices, \\[ \\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ \\vdots \\\\ Y_n\\end{bmatrix} = \\begin{bmatrix}\\beta_0(1) + \\beta_1(X_{1_1}) + \\beta_2(X_{2_1}) + \\ldots + \\beta_k(X_{k_1}) \\\\ \\beta_0(1) + \\beta_1(X_{1_2}) + \\beta_2(X_{2_2}) + \\ldots + \\beta_k(X_{k_2}) \\\\ \\beta_0(1) + \\beta_1(X_{1_3}) + \\beta_2(X_{2_3}) + \\ldots + \\beta_k(X_{k_3}) \\\\ \\vdots \\\\ \\beta_0(1) + \\beta_1(X_{1_n}) + \\beta_2(X_{2_n}) + \\ldots + \\beta_k(X_{k_n})\\end{bmatrix} + \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n\\end{bmatrix} \\] We can re-write this as, \\[ \\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ \\vdots \\\\ Y_n\\end{bmatrix} = \\begin{bmatrix}1 &amp; X_{1_1} &amp; X_{2_1} &amp; \\ldots &amp; X_{k_1} \\\\ 1 &amp; X_{1_2} &amp; X_{2_2} &amp; \\ldots &amp; X_{k_2} \\\\ 1 &amp; X_{1_3} &amp; X_{2_3} &amp; \\ldots &amp; X_{k_3} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; X_{1_n} &amp; X_{2_n} &amp; \\ldots &amp; X_{k_n}\\end{bmatrix} \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\\\beta_2 \\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}+ \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n\\end{bmatrix} \\] Naming these vectors and matrices, we can use matrix notation to compactly write the regression model as, \\[ \\underset{n \\times 1}{\\mathbf{y}} = \\underset{n \\times k}{\\mathbf{X}}~\\underset{k \\times 1}{\\mathbf{b}} + \\underset{n \\times 1}{\\mathbf{e}} \\] In the equation above, y is a vector of the outcome values, X is a matrix referred to the design matrix (a.k.a., the model matrix, the data matrix), b is a vector of regression coefficients, and e is a vector of the residuals. 19.2 Estimating the Regression Coefficients When we fit a regression model to a dataset, one of the goals is to estimate the regression coefficients, \\(\\beta_0, \\beta_1, \\beta_2,\\ldots,\\beta_k\\). Based on the regression equation, \\[ \\mathbf{Y} = \\mathbf{Xb} + \\boldsymbol{\\epsilon} \\] our goal is to solve for terms in the b vector. (Note that here (and moving forward) the dimensions of each matrix/vector have been omitted when we write the regression model.) How this is done, depends on the estimation method used. For the rest of this chapter, we will assume that Ordinary Least Squares (OLS) regression is being used to estimate the coefficients. In OLS estimation, we want to find the coefficient values that produce the smallest sum of squared residuals. To do this, we first re-write the regression equation to isolate the error vector: \\[ \\mathbf{e} = \\mathbf{y} - \\mathbf{Xb} \\] The sum of squared residual can be expressed in matrix notation as \\(\\mathbf{e}^{\\intercal}\\mathbf{e}\\). This implies: \\[ \\mathbf{e}^{\\intercal}\\mathbf{e} = (\\mathbf{y} - \\mathbf{Xb})^{\\intercal} (\\mathbf{y} - \\mathbf{Xb}) \\] Using the rules of transposes and expanding the right-hand side, we get, \\[ \\mathbf{y}^{\\intercal}\\mathbf{y} - \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y} - \\mathbf{y}^{\\intercal}\\mathbf{X}\\mathbf{b} + \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{X}\\mathbf{b} \\] Each of these terms is a \\(1\\times 1\\) matrix10, which implies that each term is equal to its transpose. We will re-write the third term \\(\\mathbf{y}^{\\intercal}\\mathbf{X}\\mathbf{b}\\) as its transpose \\(\\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y}\\). Re-writing, we get: \\[ \\mathbf{y}^{\\intercal}\\mathbf{y} - \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y} - \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y} + \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{X}\\mathbf{b} \\] Combining the two middle terms, \\[ \\mathbf{y}^{\\intercal}\\mathbf{y} - 2\\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y} + \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{X}\\mathbf{b} \\] To find the values for the elements in b that minimize the equation, we differentiate this expression with respect to b. \\[ \\frac{\\delta}{\\delta\\mathbf{b}}~\\mathbf{y}^{\\intercal}\\mathbf{y} - 2\\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y} + \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{X}\\mathbf{b} \\] Although calculus, especially calculus on matrices, is beyond the scope of this book, Fox (2009) gives the interested reader some mathematical background on optimization (i.e., minimizing). For now you just need to understand we can optimize a function by computing its derivative, setting the derivative equal to 0, and solving for any remaining unknowns. Differentiating this we get \\[ -2\\mathbf{X}^{\\intercal}\\mathbf{y} + 2\\mathbf{X}^{\\intercal}\\mathbf{Xb} \\] We set this equal to zero and solve for b. \\[ \\begin{split} -2\\mathbf{X}^{\\intercal}\\mathbf{y} + 2\\mathbf{X}^{\\intercal}\\mathbf{Xb} &amp;= 0 \\\\[2ex] 2\\mathbf{X}^{\\intercal}\\mathbf{Xb} &amp;= 2\\mathbf{X}^{\\intercal}\\mathbf{y} \\\\[2ex] \\mathbf{X}^{\\intercal}\\mathbf{Xb} &amp;= \\mathbf{X}^{\\intercal}\\mathbf{y} \\end{split} \\] This representation of the OLS equations: \\[ \\mathbf{X}^{\\intercal}\\mathbf{Xb} = \\mathbf{X}^{\\intercal}\\mathbf{y} \\] is referred to as the Normal Equations. It is the basis for many methods of soving for b. To isolate b we pre-multiply both sides of the equation by \\((\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\). \\[ \\begin{split} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}(\\mathbf{X}^{\\prime}\\mathbf{X})\\mathbf{b} &amp;= (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}(\\mathbf{X}^{\\intercal}\\mathbf{y}) \\\\[2ex] \\mathbf{I}\\mathbf{b} &amp;= (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y} \\\\[2ex] \\mathbf{b} &amp;= (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y} \\end{split} \\] The vector of regression coefficients can be obtain from: \\[ \\mathbf{b} = (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y} \\] This implies that the vector of regression coefficients can be obtained directly through manipulation of the design matrix and the vector of outcomes. In other words, the OLS coefficients is a direct function of the data. 19.2.1 Example Using Data We will use the following toy data set to illustrate how regression is carried out via matrix algebra. Table 19.1: Example set of education data. ID SAT GPA Self-Esteem IQ 1 560 3.0 11 112 2 780 3.9 10 143 3 620 2.9 19 124 4 600 2.7 7 129 5 720 3.7 18 130 6 380 2.4 13 82 Say we wanted to fit a regression model using SAT and Self-Esteem to predict variation in GPA. We can estimate the regression coefficients by creating a design matrix (X), the vector of outcomes (y), and then using matrix algebra. # Create vector of outcomes y = c(3.0, 3.9, 2.9, 2.7, 3.7, 2.4) # Create design matrix X = matrix( data = c( rep(1, 6), 560, 780, 620, 600, 720, 380, 11, 10, 19, 7, 18, 13 ), ncol = 3 ) # View X X [,1] [,2] [,3] [1,] 1 560 11 [2,] 1 780 10 [3,] 1 620 19 [4,] 1 600 7 [5,] 1 720 18 [6,] 1 380 13 # Estimate coefficients b = solve(t(X) %*% X) %*% t(X) %*% y b [,1] [1,] 0.659213517 [2,] 0.003805501 [3,] 0.009186998 Let’s compare this to the estimates given in the lm() function. # Create data frame d = data.frame( GPA = c(3.0, 3.9, 2.9, 2.7, 3.7, 2.4), SAT = c(560, 780, 620, 600, 720, 380), Self = c(11, 10, 19, 7, 18, 13) ) # Fit model lm.1 = lm(GPA ~ 1 + SAT + Self, data = d) # View coefficients coef(lm.1) (Intercept) SAT Self 0.659213517 0.003805501 0.009186998 The matrix algebra and lm() function produce identical coefficient estimates. 19.3 The \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) Matrix In computing estimates, the matrix that must be inverted is \\(\\mathbf{X}^\\intercal\\mathbf{X}\\). In simple regression (with a single predictor) this is: \\[ \\begin{split} \\mathbf{X}^\\intercal\\mathbf{X} &amp;= \\begin{bmatrix}1 &amp; 1 &amp; 1 &amp; \\ldots &amp; 1\\\\ X_1 &amp; X_2 &amp; X_3 &amp; \\ldots &amp; X_n\\end{bmatrix} \\begin{bmatrix}1 &amp; X_1\\\\ 1 &amp; X_2\\\\ 1 &amp; X_3\\\\ \\vdots &amp; \\vdots\\\\ 1&amp; X_n\\end{bmatrix} \\\\[2ex] &amp;= \\begin{bmatrix}n &amp; \\sum X_i\\\\ \\sum X_i &amp; \\sum X_i^2\\end{bmatrix} \\end{split} \\] The determninant can then be found as: \\[ \\begin{split} \\begin{vmatrix}n &amp; \\sum X_i\\\\ \\sum X_i &amp; \\sum X_i^2\\end{vmatrix} &amp;= n\\sum X_i^2 - \\bigg(\\sum X_i\\bigg)^2 \\\\[2ex] &amp;= n\\sum X_i^2 - (n\\bar{X})^2 \\\\[2ex] &amp;= n \\bigg(\\sum X_i^2 - n\\bar{X}^2\\bigg) \\\\[2ex] &amp;= n \\sum(X_i - \\bar{X})^2 \\end{split} \\] This will be a positive value as long as there is variation in X, which implies that the inverse of \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) should exist. Note that when there is very little variation in X, it may be that the inverse is computationally singular. When the design matrix expands to include more than one predictor, this is also the case. The inverse of the \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) matrix can be computed as long as there is variation in the predictors and no one predictor is a linear combination of the other columns in the design matrix. References "],["introduction-to-matrix-decompostion.html", "Chapter 20 Introduction to Matrix Decompostion", " Chapter 20 Introduction to Matrix Decompostion Matrix decomposition is a method of reducing or factoring a matrix into a set of product matrices. Another name for this is matrix factorization. Working with these product matrices often makes it easier to carry out more complex matrix operations (e.g., computing an inverse). In many ways, matrix decomposition is similar to factoring scalars. For example, we can factor the scalar 36 as: \\[ 36 = 12 \\times 3 \\] We could also have used the following factorizations: \\[ \\begin{split} 36 &amp;= 9 \\times 4 \\\\[0.5em] 36 &amp;= 18 \\times 2 \\\\[0.5em] 36 &amp;= 6 \\times 3 \\times 2 \\\\[0.5em] 36 &amp;= 2 \\times 2 \\times 3 \\times 3 \\\\[0.5em] \\end{split} \\] There are potentially multiple ways to factor a scalar, and in different applications, some of these factorizations may prove more useful than others. For example, the last factorization in the example is referred to as the prime factorization (as the factors of 36 are all prime numbers) and is useful for some mathematical applications. The same is true of matrix decompoosition. There are many methods of matrix decomposition. Depending on the application, some of these methods are more useful than others. In the following chapters, we will explore a few of the more common decomposition methods, including LU decomposition, QR decomposition, singular value decomposition, Cholsky decomposition, and eigen decomposition. "],["lu-decompostion.html", "Chapter 21 LU Decompostion 21.1 An Example of LU Decomposition 21.2 Solving Systems of Equations with the LU Decomposition", " Chapter 21 LU Decompostion One common method of matrix decomposition is LU decomposition. Commonly used to computationally solve systems of equations, and to find the determinant of a matrix, LU decomposition decomposes a square matrix into a pair of triangular matrices to more easily carry out Gaussian elimination. It is based on a theorem in linear algebra, which states: Any nonsingular, square matrix, A, can be written as the product of two triangular matrices, L and U, of the same order such that matrix L is a lower-triangular matrix (all elements above the main diagonal are 0) and U is an upper-triangular matrix (all elements below the main diagonal are 0). \\[ \\begin{split} \\underset{n\\times n}{\\mathbf{A}} &amp;= \\underset{n\\times n}{\\mathbf{L}}~ \\underset{n\\times n}{\\mathbf{U}} \\\\[2ex] &amp;= \\begin{bmatrix} l_{11} &amp; 0 &amp; 0 &amp; \\ldots &amp; 0\\\\ l_{21}&amp; l_{22} &amp; 0 &amp; \\ldots &amp; 0\\\\ l_{31}&amp; l_{32} &amp; l_{33} &amp; \\ldots &amp; 0\\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp; \\ddots &amp; \\vdots \\\\ l_{n1} &amp; l_{n2} &amp; l_{n3} &amp; \\ldots &amp; l_{nn} \\end{bmatrix}\\begin{bmatrix} u_{11} &amp; u_{12} &amp; u_{13} &amp; \\ldots &amp; u_{1n}\\\\ 0 &amp; u_{22} &amp; u_{23} &amp; \\ldots &amp; u_{2n}\\\\ 0 &amp; 0 &amp; u_{33} &amp; \\ldots &amp; u_{3n}\\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; u_{nn} \\end{bmatrix} \\end{split} \\] The goal of LU decomposition would be to find the values for each of the non-zero elements in L and U. 21.1 An Example of LU Decomposition Consider the following \\(2 \\times 2\\) matrix: \\[ \\underset{2\\times 2}{\\mathbf{A}} = \\begin{bmatrix} 5 &amp; 1 \\\\ -4 &amp; 2 \\\\ \\end{bmatrix} \\] Then LU decomposition would define A as the product of a lower- and upper-triangular matrix: \\[ \\begin{split} \\underset{2\\times 2}{\\mathbf{A}} &amp;= \\underset{2\\times 2}{\\mathbf{L}}~\\underset{2\\times 2}{\\mathbf{U}}\\\\[1em] \\begin{bmatrix} 5 &amp; 1 \\\\ -4 &amp; 2 \\\\ \\end{bmatrix} &amp;= \\begin{bmatrix} l_{11} &amp; 0 \\\\ l_{21}&amp; l_{22} \\\\ \\end{bmatrix}\\begin{bmatrix} u_{11} &amp; u_{12} \\\\ 0 &amp; u_{22} \\\\ \\end{bmatrix} \\end{split} \\] To determine the elements of L and U, we can write out the system of equations based on the matrix algebra, we get: \\[ \\begin{split} 5 &amp;= l_{11}(u_{11}) + 0(0) \\\\ 1 &amp;= l_{11}(u_{12}) + 0(u_{22})\\\\ -4 &amp;= l_{21}(u_{11}) + l_{22}(0)\\\\ 2 &amp;= l_{21}(u_{12}) + l_{22}(u_{22})\\\\ \\end{split} \\] Unfortunately there are more unknowns (6) than equations (4) which means the system is underdetermined. To find a unique solution, we need to add additional constraints on the system. One way to do this is to require L to be a unit triangular matrix (i.e. elements on the main diagonal are ones). In our example, \\(l_{11} = l_{22} = 1\\), and our system of equations becomes: \\[ \\begin{split} 5 &amp;= 1(u_{11}) + 0(0) \\\\ 1 &amp;= 1(u_{12}) + 0(u_{22})\\\\ -4 &amp;= l_{21}(u_{11}) + 1(0)\\\\ 2 &amp;= l_{21}(u_{12}) + 1(u_{22})\\\\ \\end{split} \\] This system of equations is uniquely solvable (i.e., four equations; four unknowns). Solving for these unknowns, we find: \\[ \\begin{split} l_{21} &amp;= -0.8\\\\ u_{11} &amp;= 5 \\\\ u_{12} &amp;= 1\\\\ u_{22} &amp;= 2.8\\\\ \\end{split} \\] Thus, the LU decomposition of A is, \\[ \\begin{bmatrix} 5 &amp; 1 \\\\ -4 &amp; 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 \\\\ -0.8 &amp; 1 \\\\ \\end{bmatrix}\\begin{bmatrix} 5 &amp; 1 \\\\ 0 &amp; 2.8 \\\\ \\end{bmatrix} \\] Checking our work in R, we find the solution holds. # Create L L = matrix( data = c(1, -0.8, 0, 1), nrow = 2 ) # Create U U = matrix( data = c(5, 0, 1, 2.8), nrow = 2 ) # Product L %*% U [,1] [,2] [1,] 5 1 [2,] -4 2 The determinant of the decomposed matrix (A) can be computed from finding the product of the diagonal elements of both the L and U matrices from the LU decomposition. From the example, the determinant of A is: \\[ \\begin{split} \\mathrm{det}(\\mathbf{A}) &amp;= 1 \\times 1 \\times 5 \\times 2.8 \\\\[2ex] &amp;= 14 \\end{split} \\] This is the same result as taking \\(5(2) - (-4)(1) = 14\\). The det() function uses this method for computing the determinant of a matrix. We can carry out LU decomposition using the lu.decomposition() function from the matrixcalc package. This function outputs a list with the L and U matrices. # Create A A = matrix( data = c(5, -4, 1, 2), nrow = 2 ) # Load matrixcalc library library(matrixcalc) # PLU decomposition lu_decomp = lu.decomposition(A) # View results lu_decomp $L [,1] [,2] [1,] 1.0 0 [2,] -0.8 1 $U [,1] [,2] [1,] 5 1.0 [2,] 0 2.8 Here the results are, \\[ \\begin{split} {\\mathbf{L}} &amp;= \\begin{bmatrix} 1 &amp; 0 \\\\ -0.8 &amp; 1 \\\\ \\end{bmatrix} \\\\[1em] {\\mathbf{U}} &amp;= \\begin{bmatrix} 5 &amp; 1 \\\\ 0 &amp; 2.8 \\\\ \\end{bmatrix} \\end{split} \\] To double-check that the decomposition worked, we can compute LU and see if we re-obtain A. We use the object name along with the $ notation to access each element of the output. # Compute LU lu_decomp$L %*% lu_decomp$U [,1] [,2] [1,] 5 1 [2,] -4 2 There are other methods of LU decomposition, including pivoting methods (PLU decomposition), which are more applicable in practice. There is also LDU decomposition in which A is decomposed into unit triangular matrices L and U, and diagonal matrix D. 21.2 Solving Systems of Equations with the LU Decomposition Imagine if we wanted to solve a set of simultaneous equations to compute unknown values of B using our matrix A, and known values for Y, say, \\[ \\mathbf{A}\\mathbf{X}=\\mathbf{Y} \\] To find the elements of X we would need the inverse of our matrix A. However, we know have an alternative solution from the LU decomposition. Since \\(\\mathbf{A}=\\mathbf{L}\\mathbf{U}\\), we can re-write our simultaneuous equations as: \\[ \\mathbf{L}\\mathbf{U}\\mathbf{X}=\\mathbf{Y} \\] Pre-multiplying this by L inverse, we get \\[ \\begin{split} \\mathbf{L}^{-1}\\mathbf{L}\\mathbf{U}\\mathbf{X}&amp;=\\mathbf{L}^{-1}\\mathbf{Y} \\\\[0.5em] \\mathbf{U}\\mathbf{X}&amp;=\\mathbf{L}^{-1}\\mathbf{Y} \\end{split} \\] Let us call the right-hand side of this equation Z. This gives us two sets of equations related to Z: \\[ \\begin{split} \\mathbf{U}\\mathbf{X} &amp;= \\mathbf{Z} \\\\[0.5em] \\mathbf{L}^{-1}\\mathbf{Y} &amp;= \\mathbf{Z} \\end{split} \\] The second equation, we can also express (after pre-multiplying by L) as \\(\\mathbf{L}\\mathbf{Z} = \\mathbf{Y}\\). Thus we now have the two equations that are now based on the matrices L and U from our decomposition. \\[ \\begin{split} \\mathbf{U}\\mathbf{X} &amp;= \\mathbf{Z} \\\\[0.5em] \\mathbf{L}\\mathbf{Z} &amp;= \\mathbf{Y} \\end{split} \\] Remember, the goal was to solve for X, so to do this, we first solve the second equation, \\(\\mathbf{L}\\mathbf{Z} = \\mathbf{Y}\\), for Z (which is unknown), and then use that to solve the first equation for X. Let’s see it in action using our example. To do so, let’s solve this set of simultaneous equations: \\[ \\underset{\\mathbf{A}}{\\begin{bmatrix} 5 &amp; 1 \\\\ -4 &amp; 2 \\\\ \\end{bmatrix}}\\underset{\\mathbf{X}}{\\begin{bmatrix} x_{11} \\\\ x_{21} \\\\ \\end{bmatrix}} = \\underset{\\mathbf{Y}}{\\begin{bmatrix} 1 \\\\ -3 \\\\ \\end{bmatrix}} \\] 21.2.1 Step 1: Solve for Z \\[ \\begin{split} \\mathbf{L}\\mathbf{Z} &amp;= \\mathbf{Y} \\\\[0.5em] \\begin{bmatrix} 1 &amp; 0 \\\\ -0.8 &amp; 1 \\\\ \\end{bmatrix}\\begin{bmatrix} z_{11} \\\\ z_{21} \\\\ \\end{bmatrix} &amp;= \\begin{bmatrix} 1 \\\\ -3 \\\\ \\end{bmatrix} \\end{split} \\] The two equations from this multiplication are: \\[ \\begin{split} z_{11} &amp;= 1 \\\\[0.5em] -0.8z_{11} + z_{21} &amp;= -3 \\end{split} \\] The triangular matrix makes this really easy to solve these equations for the two elements of Z. Since solving these equations boils down to substituting values into each subsequent equation, this method of solving the equations is referred to as forward substitution. Here, \\[ \\begin{split} z_{11}&amp;=1 \\\\[2ex] z_{21}&amp;=-2.2 \\end{split} \\] We can now use these values of Z in the second equation. 21.2.2 Step 2: Solve for X \\[ \\begin{split} \\mathbf{U}\\mathbf{X} &amp;= \\mathbf{Z} \\\\[0.5em] \\begin{bmatrix} 5 &amp; 1 \\\\ 0 &amp; 2.8 \\\\ \\end{bmatrix} \\begin{bmatrix} x_{11} \\\\ x_{21} \\\\ \\end{bmatrix} &amp;= \\begin{bmatrix} 1 \\\\ -2.2 \\\\ \\end{bmatrix} \\end{split} \\] The two equations from this multiplication are: \\[ \\begin{split} 5(x_{11}) + x_{21} &amp;= 1 \\\\[0.5em] 2.8(x_{21}) &amp;= -2.2 \\end{split} \\] Again, the triangular matrix makes this really easy to solve these equations for the two elements of X. Solving these equations boils down to substituting values from later equations into each of the earlier equations. Hence, this method of solving the equations is referred to as back substitution. After carrying out the algebra, \\[ \\mathbf{B} \\approx\\begin{bmatrix} 0.36 \\\\ -0.79 \\\\ \\end{bmatrix} \\] The exciting thing here is that we have solved for X without ever finding the inverse of the A matrix! It turns out that this is also a much more computationally efficient method to solve systems of equations. (Even though it maybe didn’t feel like it when we used a \\(2 \\times 2\\) matrix.) 21.2.3 Using R to Solve the Two Equations We can also use R to solve the two equations, \\(\\mathbf{LZ}=\\mathbf{Y}\\) and \\(\\mathbf{UX}=\\mathbf{Z}\\). The function forwardsolve() (forward substitution) can be used to solve the equation \\(\\mathbf{LZ}=\\mathbf{Y}\\), and more generally, any equation where a lower-triangular matrix is being pre-multiplied by a matrix of unknown elements to produce another known matrix. This function takes a lower-triangular matrix as its first argument and the solution matrix as its second argument. The syntax to solve the following equation for the elements of Z is given below. \\[ \\begin{split} \\mathbf{L}\\mathbf{Z} &amp;= \\mathbf{Y} \\\\[0.5em] \\begin{bmatrix} 1 &amp; 0 \\\\ -0.8 &amp; 1 \\\\ \\end{bmatrix}\\begin{bmatrix} z_{11} \\\\ z_{21} \\\\ \\end{bmatrix} &amp;= \\begin{bmatrix} 1 \\\\ -3 \\\\ \\end{bmatrix} \\end{split} \\] # Create L L = matrix( data = c(1, -0.8, 0, 1), nrow = 2 ) # Create Y Y = matrix( data = c(1, -3), nrow = 2 ) # Solve for Z Z = forwardsolve(L, Y) # View Z Z [,1] [1,] 1.0 [2,] -2.2 To solve an upper-triangular system of equations—one in which an upper-triangular matrix is being pre-multiplied by a matrix of unknown elements to produce another known matrix—we use the backsolve() function (backward substitution). This function takes an upper-triangular matrix as its first argument and the solution matrix as its second argument. The syntax to solve the following equation for the elements of X is given below. \\[ \\begin{split} \\mathbf{U}\\mathbf{X} &amp;= \\mathbf{Z} \\\\[0.5em] \\begin{bmatrix} 5 &amp; 1 \\\\ 0 &amp; 2.8 \\\\ \\end{bmatrix} \\begin{bmatrix} x_{11} \\\\ x_{21} \\\\ \\end{bmatrix} &amp;= \\begin{bmatrix} 1 \\\\ -2.2 \\\\ \\end{bmatrix} \\end{split} \\] # Create L U = matrix( data = c(5, 0, 1, 2.8), nrow = 2 ) # Solve for X X = backsolve(U, Z) # View X X [,1] [1,] 0.3571429 [2,] -0.7857143 These functions can also take as arguments the lower- or upper-triangular matrices produced from the lu() function. For example: # Solve for Z Z = forwardsolve(lu_decomp$L, Y) # Solve for X X = backsolve(lu_decomp$U, Z) # View X X [,1] [1,] 0.3571429 [2,] -0.7857143 "],["statistical-application-estimating-regression-coefficients-with-lu-decomposition.html", "Chapter 22 Statistical Application: Estimating Regression Coefficients with LU Decomposition", " Chapter 22 Statistical Application: Estimating Regression Coefficients with LU Decomposition In OLS regression our goal is to estimate regression coefficients (b) from a data matrix (X) and vector of outcomes (y). To do this we want to solve the following equation for b: \\[ (\\mathbf{X}^{\\intercal}\\mathbf{X})\\mathbf{b} = \\mathbf{X}^{\\intercal}\\mathbf{y} \\] Pre-multiplying both sides by \\((\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\) gives us our conventional solution for b, namely \\(\\mathbf{b}=(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y}\\). However, this requires us to find the inverse of \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\). Unfortunately, the computational functions that are used to compute inverses of matrices sometimes end up being numerically unstable. For example, consider the following simulated data set of \\(n=50\\) cases: # Number of cases n = 50 # Create 50 x-values evenly spread b/w 1 and 500 x = seq(from = 1, to = 500, len = n) # Create X matrix X = cbind(1, x, x^2, x^3) # Create b matrix b = matrix( data = c(1, 1, 1, 1), nrow = 4 ) # Create vector of y-values set.seed(1) y = X %*% b + rnorm(n, mean = 0, sd = 1) # Find determinant of (X^T)X det(t(X) %*% X) [1] 3.113748e+32 Here we have simulated data using the following statistical model: \\[ y_i = 1 + 1(x_i) + 1(x_i^2) + 1(x_i^3) + e_i \\qquad \\mathrm{where~} e_i\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,1) \\] The determinant of \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) is not zero (although it is close to zero), so we should be able to find an inverse. What happens if we try to use solve() to find the inverse of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix to obtain the regression coefficient estimates? Here crossprod(X) is equivalent to t(X) %*% X, and crossprod(X, y) is equivalent to t(X) %*% y. # Try to compute b solve(crossprod(X)) %*% crossprod(X, y) Error in solve.default(crossprod(X)): system is computationally singular: reciprocal condition number = 2.93617e-17 The standard R function for inverse is computationally singular. To see why this happens, we can take a closer look at the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix. Here we will examine these values: # Set the number of digits options(digits = 4) # Compute X^T(X) matrix crossprod(X) x 5.000e+01 1.253e+04 4.217e+06 1.597e+09 x 1.253e+04 4.217e+06 1.597e+09 6.454e+11 4.217e+06 1.597e+09 6.454e+11 2.716e+14 1.597e+09 6.454e+11 2.716e+14 1.176e+17 Note the difference of several orders of magnitude. On a computer, we have a limited range of numbers. This makes some numbers behave like 0, when we also have to consider very large numbers. This in turn leads to what is essentially division by 0, which produces errors. 22.0.1 Estimating Regression Coefficients Using LU Decomposition Remember, our goal is to solve the following for b. \\[ (\\mathbf{X}^{\\intercal}\\mathbf{X})\\mathbf{b} = \\mathbf{X}^{\\intercal}\\mathbf{y} \\] This is exactly the type of problem that LU decomposition can help solve. Note that the first “term,” \\((\\mathbf{X}^{\\intercal}\\mathbf{X})\\), is a square matrix. The right-hand side of the equation is known from the data, and we want to solve for the elements of b. This is the exact format of \\(\\mathbf{AX}=\\mathbf{Y}\\). So rather than trying to find the inverse of \\((\\mathbf{X}^{\\intercal}\\mathbf{X})\\), we can carry out the decomposition on the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix to produce an L and U matrix that we can then use to solve for b. # LU decomposition library(matrixcalc) lu_decomp = lu.decomposition(crossprod(X)) # View results lu_decomp $L [,1] [,2] [,3] [,4] [1,] 1.000e+00 0 0.0 0 [2,] 2.505e+02 1 0.0 0 [3,] 8.435e+04 501 1.0 0 [4,] 3.195e+07 227105 751.5 1 $U [,1] [,2] [,3] [,4] [1,] 50 12525 4.217e+06 1.597e+09 [2,] 0 1079851 5.410e+08 2.452e+11 [3,] 0 0 1.863e+10 1.400e+13 [4,] 0 0 1.953e-03 3.095e+14 Now we can solve for Z in the equation \\(\\mathbf{LZ}=\\mathbf{Y}\\), but remembering that here, the right-hand side of this equation is \\(\\mathbf{X}^{\\intercal}\\mathbf{y}\\), so we are solving for Z in: \\[ \\mathbf{LZ}=\\mathbf{X}^{\\intercal}\\mathbf{y} \\] In our example, \\[ \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 2.640052 \\times 10^{-3} &amp; 1 &amp; 0 &amp; 0\\\\ 7.840596 \\times 10^{-6} &amp; 7.919771 \\times 10^{-3} &amp; 1 &amp; 0\\\\ 3.129978 \\times 10^{-8} &amp; 7.211598 \\times 10^{-5} &amp; 2.495756 \\times 10^{-2} &amp; 1 \\end{bmatrix}\\begin{bmatrix} z_{11} \\\\ z_{21} \\\\ z_{31} \\\\ z_{41} \\\\ \\end{bmatrix} = \\begin{bmatrix} 1.601685 \\times 10^{9}\\\\ 6.470034 \\times 10^{11}\\\\ 2.722570 \\times 10^{14}\\\\ 1.178380 \\times 10^{17}\\\\ \\end{bmatrix} \\] Then we can solve the equation: \\[ \\mathbf{Ub}=\\mathbf{Z} \\] which in our example is: \\[ \\begin{bmatrix} 1.597455 \\times 10^{9} &amp; 6.454018 \\times 10^{11} &amp; 2.7161 \\times 10^{14} &amp; 1.175658 \\times 10^{17}\\\\ 1 &amp; -1.064388 \\times 10^{8} &amp; -7.166250 \\times 10^{10} &amp; -3.876978 \\times 10^{13}\\\\ 1 &amp; 1 &amp; 3.542182 \\times 10^{7} &amp; 3.066370 \\times 10^{10}\\\\ 1 &amp; 1 &amp; 1 &amp; -5.169923 \\times 10^{7} \\end{bmatrix}\\begin{bmatrix} b_{11} \\\\ b_{21} \\\\ b_{31} \\\\ b_{41} \\\\ \\end{bmatrix} = \\begin{bmatrix} 1.601685 \\times 10^{9}\\\\ 6.469992 \\times 10^{11}\\\\ 2.722518 \\times 10^{14}\\\\ 1.178312 \\times 10^{17} \\end{bmatrix} \\] # Solve for Z Z = forwardsolve(lu_decomp$L, crossprod(X,y)) # Solve for b b = backsolve(lu_decomp$U, Z) # View b b [,1] [1,] 0.9038 [2,] 1.0066 [3,] 1.0000 [4,] 1.0000 We have successfully estimated the coefficients, which are all near 1. Note that fitting the model using lm() we obtain the same coefficients. # Fit model using columns of the X matrix lm.1 = lm(y ~ 1 + X[ , 2] + X[ , 3] + X[ , 4]) # View coefficientw coef(lm.1) (Intercept) X[, 2] X[, 3] X[, 4] 0.9038 1.0066 1.0000 1.0000 The lm() function (and most other statistical software) uses decomposition to solve for coefficients rather than inverting the \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) matrix. Although the estimates here are the same as those from LU decomposition, the lm() function actually uses QR decomposition. We will examine this method in the next chapter. "],["cholesky-decompostion.html", "Chapter 23 Cholesky Decompostion 23.1 Example of Cholesky Decomposition 23.2 Cholesky Decomposition using R 23.3 Statistical Application: Estimating Regression Coefficents with Cholesky Decomposition", " Chapter 23 Cholesky Decompostion Another decomposition method is Cholesky (or Choleski) decomposition. The Cholesky decomposition method—used in statistical applications from nonlinear optimization, to Monte Carlo simulation methods, to Kalman filtering—is much more computationally efficient than the LU method. The Cholesky method decomposes a symmetric, positive definite matrix A into the product of two matrices, \\(\\mathbf{L}\\) (a lower-triangular matrix) and \\(\\mathbf{L}^*\\) (the conjugate transpose of \\(\\mathbf{L}\\)).11 \\[ \\underset{n \\times n}{\\mathbf{A}} = \\mathbf{LL}^* \\] The conjugate transpose is computed by taking the transpose of a matrix and then finding the complex conjugate of each element in the matrix. To understand what a complex conjugate is, we first remind you of the idea of a complex number. Remember that all real and imaginary numbers are complex numbers which can be expressed as \\(a+bi\\), where a is the real part of the number and b is the imaginary part of the number, and i is the square root of \\(-1\\). For example the number 2 can be expressed as \\(2 + 0i\\). Note: The complex conjugate of a real number is just the real number, since \\(a+0i = a-0i=a\\). The complex conjugate of a number is itself a complex number that has the exact same real part and an imaginary part equal in magnitude but opposite in sign. For example the complex conjugate for the number \\(3 + 2i\\) is \\(3 - 2i\\). As an example, say that matrix A was \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 3+i &amp; -2+3i \\\\ 0 &amp; 4 &amp; 0-i \\\\ \\end{bmatrix} \\] The conjugate transpose of A, symbolized as \\(\\mathbf{A}^*\\), can be found by first computing the transpose of A, and then finding the complex conjugate of each element in the transpose. \\[ \\mathbf{A}^{\\intercal} = \\begin{bmatrix} 1 &amp; 0 \\\\ 3+i0 &amp; 4 \\\\ -2+3i &amp; 0-i \\end{bmatrix} \\] And, \\[ \\mathbf{A}^{*} = \\begin{bmatrix} 1 &amp; 0 \\\\ 3-i0 &amp; 4 \\\\ -2-3i &amp; 0+i \\end{bmatrix} \\] Many texts just present the notation for the Cholesky decomposition as \\(\\mathbf{A}=\\mathbf{LL}^\\intercal\\), rather than \\(\\mathbf{A}=\\mathbf{LL}^*\\). This is because they make the further assumption that all the elements in A are real numbers. In that case, \\(\\mathbf{L}^*=\\mathbf{L}^\\intercal\\). We will make that assumption and use that notation moving forward. 23.1 Example of Cholesky Decomposition Say we wanted to compute a Cholsky decomposition on a \\(2 \\times 2\\) symmetric matrix: \\[ \\underset{2\\times 2}{\\mathbf{A}} = \\begin{bmatrix} 5 &amp; -4 \\\\ -4 &amp; 5 \\\\ \\end{bmatrix} \\] The Cholesky decomposition would factor A into the following: \\[ \\begin{split} \\underset{2\\times 2}{\\mathbf{A}} &amp;= \\underset{2\\times 2}{\\mathbf{L}}~\\underset{2\\times 2}{\\mathbf{L}^\\intercal}\\\\[1em] \\begin{bmatrix} 5 &amp; -4 \\\\ -4 &amp; 5 \\\\ \\end{bmatrix} &amp;= \\begin{bmatrix} l_{11} &amp; 0 \\\\ l_{21} &amp; l_{22} \\\\ \\end{bmatrix} \\begin{bmatrix} l_{11} &amp; l_{21} \\\\ 0 &amp; l_{22} \\\\ \\end{bmatrix} \\end{split} \\] Carrying out the matrix algebra we have the following three unique equations: \\[ \\begin{split} 5 &amp;= l_{11}(l_{11}) \\\\[0.5em] -4 &amp;= l_{11}(l_{21}) \\\\[0.5em] 5 &amp;= l_{21}(l_{21}) + l_{22}(l_{22}) \\end{split} \\] Since we have three equations with three unknowns we can solve for each element in L. \\[ \\begin{split} l_{11} &amp;= \\sqrt{5} \\approx 2.24\\\\[0.5em] l_{21} &amp;= \\dfrac{-4}{\\sqrt{5}} \\approx -1.79 \\\\[0.5em] l_{22} &amp;= \\sqrt{1.8} \\approx 1.34 \\end{split} \\] So the Cholsky decomposition is, \\[ \\begin{bmatrix} 5 &amp; -4 \\\\ -4 &amp; 5 \\\\ \\end{bmatrix} = \\begin{bmatrix} \\sqrt{5} &amp; 0 \\\\ \\dfrac{-4}{\\sqrt{5}} &amp; \\sqrt{1.8} \\\\ \\end{bmatrix} \\begin{bmatrix} \\sqrt{5} &amp; \\dfrac{-4}{\\sqrt{5}} \\\\ 0 &amp; \\sqrt{1.8} \\\\ \\end{bmatrix} \\] Or using the approximations, \\[ \\begin{bmatrix} 5 &amp; -4 \\\\ -4 &amp; 5 \\\\ \\end{bmatrix} \\approx \\begin{bmatrix} 2.24 &amp; 0 \\\\ -1.79 &amp; 1.34 \\\\ \\end{bmatrix} \\begin{bmatrix} 2.24 &amp; -1.79 \\\\ 0 &amp; 1.34 \\\\ \\end{bmatrix} \\] 23.2 Cholesky Decomposition using R We can use the chol() function to compute the Cholesky decomposition. For example to carry out the Cholesky decomposition on A form the previous section, we would use the following syntax: # Create A A = matrix( data = c(5, -4, -4, 5), nrow = 2 ) # Cholesky decomposition cholesky_decomp = chol(A) # View results cholesky_decomp [,1] [,2] [1,] 2.236 -1.789 [2,] 0.000 1.342 Note that the output from chol() is actually the transpose matrix, \\(\\mathbf{L}^\\intercal\\). To obtain L, we need to take the transpose of this output. We can also verify that the decomposition worked. # Obtain L t(cholesky_decomp) [,1] [,2] [1,] 2.236 0.000 [2,] -1.789 1.342 # Check results L(L^T) = A t(cholesky_decomp) %*% cholesky_decomp [,1] [,2] [1,] 5 -4 [2,] -4 5 🚧 CAUTION: The chol() function does not check for symmetry. If you use the function to decompose a nonsymmetric matrix, the results will likely be meaningless. For example, consider the following nonsymmetric matrix: \\[ \\mathbf{A} = \\begin{bmatrix}5 &amp; 1\\\\ -4 &amp; 2\\end{bmatrix} \\] Computing the decomposition, we get: # Create A A = matrix( data = c(5, -4, 1, 2), nrow = 2 ) # Cholsky decomposition chol(A) [,1] [,2] [1,] 2.236 0.4472 [2,] 0.000 1.3416 Checking the matrix multiplication we do not get back to the original matrix, A: # Check results t(chol(A)) %*% chol(A) [,1] [,2] [1,] 5 1 [2,] 1 2 23.3 Statistical Application: Estimating Regression Coefficents with Cholesky Decomposition We can again use the the OLS solution to compute the elements of b by solving the system of equations represented in: \\[ (\\mathbf{X}^{\\intercal}\\mathbf{X})\\mathbf{b} = \\mathbf{X}^{\\intercal}\\mathbf{y} \\] So long as \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) is positive definite, we can use Cholesky decomposition to solve for the elements of b using the same two-step process we did for LU decomposition. Namely, \\[ \\begin{split} &amp;\\mathrm{Solve~~}\\mathbf{LZ} &amp;= \\mathbf{X}^{\\intercal}\\mathbf{y}~~\\mathrm{for~}\\mathbf{Z} \\\\[2ex] &amp;\\mathrm{Solve~~}\\mathbf{L}^\\intercal\\mathbf{b} &amp; =\\mathbf{Z}~~\\mathrm{for~}\\mathbf{b} \\end{split} \\] Using the same simulated data as the example from last chapter, consider the following simulated data set of \\(n=50\\) cases: # Number of cases n = 50 # Create 50 x-values evenly spread b/w 1 and 500 x = seq(from = 1, to = 500, len = n) # Create X matrix X = cbind(1, x, x^2, x^3) # Create b matrix b = matrix( data = c(1, 1, 1, 1), nrow = 4 ) # Create vector of y-values set.seed(1) y = X %*% b + rnorm(n, mean = 0, sd = 1) We can again use forwardsolve() and backsolve() to help solve the two equations. # Obtain L^T and L L_T = chol(t(X)%*%X) L = t(L_T) # Solve for Z Z = forwardsolve(L, crossprod(X,y)) # Solve for b b = backsolve(L_T, Z) # View b b [,1] [1,] 0.9038 [2,] 1.0066 [3,] 1.0000 [4,] 1.0000 There are also variations on the Cholesky decomposition method, including LDL and LDL\\(^\\intercal\\) decomposition. In these methods, D is a diagonal matrix.↩︎ "],["qr-decompostion.html", "Chapter 24 QR Decompostion 24.1 QR Decomposition using R 24.2 Statistical Application: Estimating Regression Coefficents with QR Decomposition", " Chapter 24 QR Decompostion Another decomposition method is QR decomposition. One advantage of QR decomposition over LU decomposition is that this method does not require that the decomposition be carried out on a square matrix. QR decomposition results in factoring matrix A (having independent columns) into the product of two matrices, namely Q and R: . \\[ \\underset{m\\times n}{\\mathbf{A}} = \\underset{m\\times m}{\\mathbf{Q}}~ \\underset{m\\times n}{\\mathbf{R}} \\] where Q is an orthogonal matrix12 and R is an upper-triangular matrix. 24.1 QR Decomposition using R Although there is a way to hand-calculate the matrices Q and R (e.g., using the Gram-Schmidt process), we will rely on computation. To carry out a QR decomposition in R, we will use the qr() function which factors the matrix and returns a list of output related to the QR decomposition. To extract the actual Q and R matrices from this output, we will use the qr.Q() and qr.R() functions, respectively. # Create matrix A A = matrix( data = c(5, -4, 1, 2), nrow = 2 ) # Carry out QR decomposition qr_decomp = qr(A) # View Q matrix qr.Q(qr_decomp) [,1] [,2] [1,] -0.7809 0.6247 [2,] 0.6247 0.7809 # View R matrix qr.R(qr_decomp) [,1] [,2] [1,] -6.403 0.4685 [2,] 0.000 2.1864 We find: \\[ \\begin{split} \\mathbf{A} &amp;= \\mathbf{QR} \\\\[2ex] \\begin{bmatrix}5 &amp; 1 \\\\ -4 &amp; 2\\end{bmatrix} &amp;= \\begin{bmatrix}-0.7809 &amp; 0.6247 \\\\ 0.6247 &amp; 0.7809\\end{bmatrix}\\begin{bmatrix}-6.403 &amp; 0.4685 \\\\ 0 &amp; 2.1864\\end{bmatrix} \\end{split} \\] You can see that R is an upper-triangular matrix, and we can check that Q is orthonormal by testing whether \\(\\mathbf{Q}^{\\intercal} = \\mathbf{Q}^{-1}\\). # Q^T t(qr.Q(qr_decomp)) [,1] [,2] [1,] -0.7809 0.6247 [2,] 0.6247 0.7809 # Q^-1 solve(qr.Q(qr_decomp)) [,1] [,2] [1,] -0.7809 0.6247 [2,] 0.6247 0.7809 We can also check to see that the product of the decomposed matrices is A. # A = QR? qr.Q(qr_decomp) %*% qr.R(qr_decomp) [,1] [,2] [1,] 5 1 [2,] -4 2 24.2 Statistical Application: Estimating Regression Coefficents with QR Decomposition We can use the two decomposed matrices, Q and R, to compute the elements of b in the OLS solution, solving the system of equations represented in: \\[ (\\mathbf{X}^{\\intercal}\\mathbf{X})\\mathbf{b} = \\mathbf{X}^{\\intercal}\\mathbf{y} \\] Since QR decomposition works on non-square matrices, we can decompose X as: \\[ \\mathbf{X} = \\mathbf{QR} \\] Then the OLS equation can be written as: \\[ \\begin{split} (\\mathbf{QR})^{\\intercal}\\mathbf{QR}\\mathbf{b} &amp;= (\\mathbf{QR})^{\\intercal}\\mathbf{y} \\\\[2ex] \\mathbf{R}^{\\intercal}\\mathbf{Q}^{\\intercal}\\mathbf{QR}\\mathbf{b} &amp;= \\mathbf{R}^{\\intercal}\\mathbf{Q}^{\\intercal}\\mathbf{y} \\end{split} \\] Since Q is orthonormal, and \\(\\mathbf{QQ}^\\intercal = \\mathbf{Q}^\\intercal\\mathbf{Q}=\\mathbf{I}\\), then this reduces to: \\[ \\begin{split} \\mathbf{R}^{\\intercal}\\mathbf{I}\\mathbf{R}\\mathbf{b} &amp;= \\mathbf{R}^{\\intercal}\\mathbf{Q}^{\\intercal}\\mathbf{y} \\\\[2ex] \\mathbf{R}^{\\intercal}\\mathbf{R}\\mathbf{b} &amp;= \\mathbf{R}^{\\intercal}\\mathbf{Q}^{\\intercal}\\mathbf{y} \\\\[2ex] \\mathbf{R}\\mathbf{b} &amp;= \\mathbf{Q}^{\\intercal}\\mathbf{y} \\end{split} \\] Since R is an upper-triangular matrix, we can use back substitution to solve for the elements of b. Using the same simulated data as the example from last chapter, consider the following simulated data set of \\(n=50\\) cases: # Number of cases n = 50 # Create 50 x-values evenly spread b/w 1 and 500 x = seq(from = 1, to = 500, len = n) # Create X matrix X = cbind(1, x, x^2, x^3) colnames(X) &lt;- c(&quot;Intercept&quot;, &quot;x&quot;, &quot;x2&quot;, &quot;x3&quot;) # Create b matrix b = matrix( data = c(1, 1, 1, 1), nrow = 4 ) # Create vector of y-values set.seed(1) y = X %*% b + rnorm(n, mean = 0, sd = 1) We can use QR decomposition to solve for b. # Carry out QR decomposition QR = qr(X) # Solve for b backsolve(qr.R(QR), t(qr.Q(QR)) %*% y) [,1] [1,] 0.9038 [2,] 1.0066 [3,] 1.0000 [4,] 1.0000 QR decomposition is how the lm() function computes the regression coefficients. The columns of an orthogonal matrix are orthogonal unit vectors and it has the property that \\(\\mathbf{Q}^{T}\\mathbf{Q}=\\mathbf{I}\\) or that \\(\\mathbf{Q}^{\\intercal} = \\mathbf{Q}^{-1}\\).↩︎ "],["spectral-decompostion.html", "Chapter 25 Spectral Decompostion 25.1 Solving Systems of Equations with Spectral Decomposition", " Chapter 25 Spectral Decompostion Spectral decomposition (a.k.a., eigen decomposition) is used primarily in principal components analysis (PCA). This method decomposes a square matrix, A, into the product of three matrices: \\[ \\underset{n\\times n}{\\mathbf{A}} = \\underset{n\\times n}{\\mathbf{P}}~ \\underset{n\\times n}{\\mathbf{D}}~ \\underset{n\\times n}{\\mathbf{P}^{\\intercal}} \\] where, P is a n-dimensional square matrix whose ith column is the ith eigenvector of A, and D is a n-dimensional diagonal matrix whose diagonal elements are composed of the eigenvalues of A. That is, the spectral decomposition is based on the eigenstructure of A. 25.0.1 An Example Recall that in a previous chapter we used the following \\(2 \\times 2\\) matrix as an example: \\[ \\mathbf{A} = \\begin{bmatrix} -3 &amp; 5 \\\\ 4 &amp; -2 \\\\ \\end{bmatrix} \\] The eigenstructure for this matrix was: \\[ \\begin{split} \\lambda_1 &amp;= -7 \\qquad &amp;\\mathbf{e}_1 = \\begin{bmatrix}\\frac{5}{\\sqrt{41}} \\\\ -\\frac{4}{\\sqrt{41}}\\end{bmatrix}\\\\[2ex] \\lambda_2 &amp;= 2 \\qquad &amp;\\mathbf{e}_2 = \\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{bmatrix} \\\\[2ex] \\end{split}\\] The P and D matrices of the spectral decomposition are composed of the eigenvectors and eigenvalues, respectively. \\[ \\begin{split} \\mathbf{P} &amp;= \\begin{bmatrix}\\frac{5}{\\sqrt{41}} &amp; \\frac{1}{\\sqrt{2}} \\\\ -\\frac{4}{\\sqrt{41}} &amp; \\frac{1}{\\sqrt{2}}\\end{bmatrix} \\\\[2ex] \\mathbf{D} &amp;= \\begin{bmatrix}7 &amp; 0 \\\\ 0 &amp; -2\\end{bmatrix} \\end{split} \\] Recall also that the eigen() function provided the eigenvalues and eigenvectors for an inputted square matrix. The eigenvectors were outputted as columns in a matrix, so, the $vector output from the function is, in fact, outputting the matrix P. The eigen() function is actually carrying out the spectral decomposition! We can use this output to verify the decomposition by computing whether \\(\\mathbf{PDP}^{-1}=\\mathbf{A}\\). # Create A A = matrix( data = c(-3, 4, 5, -2), nrow = 2 ) # Compute eigenvalues and eigenvectors spec_decomp = eigen(A) # Create P P = spec_decomp$vectors # Create D D = diag(spec_decomp$values) # Verify the decomposition P %*% D %*% solve(P) [,1] [,2] [1,] -3 5 [2,] 4 -2 Of note, when A is symmetric, then the P matrix will be orthogonal; \\(\\mathbf{P}^{-1}=\\mathbf{P}^\\intercal\\). We can illustrate this by an example: # Create symmetric matrix A A = matrix( data = c(1, 4, 2, 4, 1, 3, 2, 3, 1), nrow = 3 ) # Compute eigenvalues and eigenvectors spec_decomp = eigen(A) # Create P P = spec_decomp$vectors # Inverse of P solve(P) [,1] [,2] [,3] [1,] -0.5844 -0.6346 -0.5058 [2,] -0.5449 -0.1550 0.8240 [3,] 0.6013 -0.7572 0.2552 # Transpose of P t(P) [,1] [,2] [,3] [1,] -0.5844 -0.6346 -0.5058 [2,] -0.5449 -0.1550 0.8240 [3,] 0.6013 -0.7572 0.2552 This is a useful property since it means that the inverse of P is easy to compute. 25.1 Solving Systems of Equations with Spectral Decomposition We can use spectral decomposition to more easily solve systems of equations. For example, in OLS estimation, our goal is to solve the following for b. \\[ (\\mathbf{X}^{\\intercal}\\mathbf{X})\\mathbf{b} = \\mathbf{X}^{\\intercal}\\mathbf{y} \\] Since \\((\\mathbf{X}^{\\intercal}\\mathbf{X})\\) is a square, symmetric matrix, we can decompose it into \\(\\mathbf{PDP}^\\intercal\\). Thus, \\[ \\mathbf{PDP}^{\\intercal}\\mathbf{b} = \\mathbf{X}^{\\intercal}\\mathbf{y} \\] Solving for b, we find: \\[ \\begin{split} \\big(\\mathbf{PDP}^{\\intercal}\\big)^{-1}\\mathbf{PDP}^{\\intercal}\\mathbf{b} &amp;= \\big(\\mathbf{PDP}^{\\intercal}\\big)^{-1} \\mathbf{X}^{\\intercal}\\mathbf{y} \\\\[2ex] \\mathbf{b} &amp;= (\\mathbf{P}^\\intercal)^{-1}\\mathbf{D}^{-1}\\mathbf{P}^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y} \\\\[2ex] &amp;= \\mathbf{P} \\mathbf{D}^{-1}\\mathbf{P}^\\intercal\\mathbf{X}^{\\intercal}\\mathbf{y} \\end{split} \\] The orthogonal P matrix makes this computationally easier to solve. Moreover, since D is a diagonal matrix, \\(\\mathbf{D}^{-1}\\) is also easy to compute. Namely, \\(\\mathbf{D}^{-1}\\) is also diagonal with elements on the diagonal equal to \\(\\frac{1}{\\lambda_i}\\). Consider our ongoing simulated data: # Number of cases n = 50 # Create 50 x-values evenly spread b/w 1 and 500 x = seq(from = 1, to = 500, len = n) # Create X matrix X = cbind(1, x, x^2, x^3) # Create b matrix b = matrix( data = c(1, 1, 1, 1), nrow = 4 ) # Create vector of y-values set.seed(1) y = X %*% b + rnorm(n, mean = 0, sd = 1) We start by using spectral decomposition to decompose \\(\\mathbf{X}^\\intercal\\mathbf{X}\\). # Spectral decomposition of (X^T)X spec_decomp = eigen(t(X) %*% X) # Create P and D^{-1} P = spec_decomp$vectors D_inv = diag(1/spec_decomp$values) Now we can carry out the matrix algebra to compute b. # Compute b P %*% D_inv %*% t(P) %*% t(X) %*% y [,1] [1,] 0.9278 [2,] 1.0059 [3,] 1.0000 [4,] 1.0000 "],["singular-value-decompostion.html", "Chapter 26 Singular Value Decompostion 26.1 SVD Using R", " Chapter 26 Singular Value Decompostion Singular value decomposition (SVD) is commonly used for data compression or variable reduction, and plays a large role in machine learning applications. SVD decomposes a matrix into the product of three matrices: \\[ \\underset{m\\times n}{\\mathbf{A}} = \\underset{m\\times n}{\\mathbf{U}}~ \\underset{n\\times n}{\\mathbf{D}}~ \\underset{n\\times n}{\\mathbf{V}^{\\intercal}} \\] where, U and V are an orthogonal matrices, and D is a diagonal matrix. From the properties of the transpose, we know that, \\[ \\mathbf{A}^{\\intercal} = \\mathbf{V}\\mathbf{D}^{\\intercal}\\mathbf{U}^{\\intercal} \\] And for mathematical convenience, we can take advantage that U and V are an orthogonal matrices (\\(\\mathbf{U}^{\\intercal}\\mathbf{U}=\\mathbf{V}^{\\intercal}\\mathbf{V}=\\mathbf{I}\\)) by expressing two equations: \\[ \\begin{split} \\mathbf{A}\\mathbf{A}^{\\intercal}\\mathbf{U} &amp;= \\mathbf{U}\\mathbf{S}\\mathbf{V}^{\\intercal}\\mathbf{V}\\mathbf{D}^{\\intercal}\\mathbf{U}^{\\intercal}\\mathbf{U} \\\\[0.5em] &amp;= \\mathbf{U}\\mathbf{D}^2 \\end{split} \\] And, \\[ \\begin{split} \\mathbf{A}^{\\intercal}\\mathbf{A}\\mathbf{V} &amp;= \\mathbf{V}\\mathbf{D}^{\\intercal}\\mathbf{U}^{\\intercal}\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal}\\mathbf{V}\\\\[0.5em] &amp;= \\mathbf{V}\\mathbf{D}^2 \\end{split} \\] These two equations are called eigenvalue equations, which show up all over the place in statistical work. These are easy to solve by computation. For example, we would solve the second eigenvalue equation to find V and D, and then find U by solving \\(\\mathbf{A}=\\mathbf{UDV}^\\intercal\\) for U, namely: \\[ \\mathbf{U} = \\mathbf{A}\\mathbf{V}\\mathbf{D}^{-1} \\] 26.1 SVD Using R In practice, we will use the svd() function in R to carry out singular value decomposition. # Create A A = matrix( data = c(5, -4, 1, 2), nrow = 2 ) # Singular value decomposition sv_decomp = svd(A) # View results sv_decomp $d [1] 6.422 2.180 $u [,1] [,2] [1,] -0.7630 0.6464 [2,] 0.6464 0.7630 $v [,1] [,2] [1,] -0.99659 0.08248 [2,] 0.08248 0.99659 The resulting decomposition is: \\[ \\begin{split} \\mathbf{U} &amp;= \\begin{bmatrix} -0.76 &amp; 0.65 \\\\ 0.65 &amp; 0.76 \\\\ \\end{bmatrix} \\\\[1em] \\mathbf{D} &amp;= \\begin{bmatrix} 6.42 &amp; 0 \\\\ 0 &amp; 2.18 \\\\ \\end{bmatrix} \\\\[1em] \\mathbf{V} &amp;= \\begin{bmatrix} -1.00 &amp; 0.08 \\\\ 0.08 &amp; 1.00 \\\\ \\end{bmatrix} \\end{split} \\] We can verify that \\(\\mathbf{A}=\\mathbf{USV}^\\intercal\\). Note that the svd() function outputs the diagonal elements of the D matrix, so we need to use the diag() function to create the actual D matrix. # Verify the results of the SVD sv_decomp$u %*% diag(sv_decomp$d) %*% t(sv_decomp$v) [,1] [,2] [1,] 5 1 [2,] -4 2 "],["important-matrices-in-regression.html", "Chapter 27 Important Matrices in Regression 27.1 Vector of Fitted Values 27.2 The H-Matrix (Hat Matrix) 27.3 Vector of Residuals", " Chapter 27 Important Matrices in Regression In this chapter, you will learn about important and useful matrices in regression applications. To illustrate these matrices, we will use the following toy data set to fit a regression model using SAT and Self-Esteem to predict variation in GPA. Table 27.1: Example set of education data. ID SAT GPA Self-Esteem IQ 1 560 3.0 11 112 2 780 3.9 10 143 3 620 2.9 19 124 4 600 2.7 7 129 5 720 3.7 18 130 6 380 2.4 13 82 27.1 Vector of Fitted Values Recall that the regression model is denoted as \\[ \\mathbf{y} = \\mathbf{Xb} + \\mathbf{e} \\] where y is the outcome vector, X is the design matrix, e is the vector of residuals, and b is the vector of coefficients estimated as, \\[ \\mathbf{b} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y} \\] We can compute the vector of fitted values using \\[ \\hat{\\mathbf{y}} = \\mathbf{Xb} \\] Here X has dimensions \\(n \\times k\\) and b has dimensions \\(k \\times 1\\). In both cases \\(k=p+1\\) where p is equal to the number of predictors in the model. This product gives the \\(n\\times 1\\) vector of fitted values, \\(\\hat{\\mathbf{y}}\\), which is, \\[ \\begin{bmatrix}\\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_n\\end{bmatrix} \\] # Create vector of outcomes y = c(3.0, 3.9, 2.9, 2.7, 3.7, 2.4) # Create design matrix X = matrix( data = c( rep(1, 6), 560, 780, 620, 600, 720, 380, 11, 10, 19, 7, 18, 13 ), ncol = 3 ) # Estimate coefficients b = solve(t(X) %*% X) %*% t(X) %*% y # Obtain vector of fitted values y_hat = X %*% b # View y_hat y_hat [,1] [1,] 2.891 [2,] 3.719 [3,] 3.193 [4,] 3.007 [5,] 3.565 [6,] 2.225 27.2 The H-Matrix (Hat Matrix) One useful matrix in regression is the hat matrix, or the H-matrix. To obtain the H-matrix we substitute the matrix formulation of the coefficient vector, b, into the equation to compute the fitted values, \\[ \\begin{split} \\mathbf{\\hat{y}} &amp;= \\mathbf{Xb} \\\\[2ex] &amp;= \\mathbf{X}(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y} \\end{split} \\] The set of products in this expression involving the X terms is referred to as H-matrix, that is: \\[ \\begin{split} \\mathbf{\\hat{y}} &amp;= \\underbrace{\\mathbf{X}(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}}_\\mathbf{H}\\mathbf{y} \\\\[2ex] &amp;= \\mathbf{Hy} \\end{split} \\] where, \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal} \\] Note that the H-matrix can be created completely from the design matrix and its transpose. # Create H H = X %*% solve(t(X) %*% X) %*% t(X) # View H H [,1] [,2] [,3] [,4] [,5] [,6] [1,] 0.22438 0.137685 0.059548 0.2738 0.02945 0.2751 [2,] 0.13769 0.575213 -0.004685 0.3380 0.21145 -0.2577 [3,] 0.05955 -0.004685 0.494122 -0.1608 0.43512 0.1767 [4,] 0.27379 0.338018 -0.160788 0.4941 -0.10178 0.1566 [5,] 0.02945 0.211446 0.435116 -0.1018 0.49437 -0.0686 [6,] 0.27515 -0.257678 0.176687 0.1566 -0.06860 0.7178 It also has the following properties: H is a square \\(n \\times n\\) matrix H is a symmetric matrix. H is an idempotent matrix. Idempotency is a mathematical property that implies certain operations can be repeatedly applied to a structure without changing it. An idempotent matrix is a matrix that can be post-multiplied by itself and the result is the originasl matrix. In our example, since H is idempotent, \\[ \\mathbf{HH} = \\mathbf{H} \\] We can verify this using R # Verify H is idempotent H %*% H [,1] [,2] [,3] [,4] [,5] [,6] [1,] 0.22438 0.137685 0.059548 0.2738 0.02945 0.2751 [2,] 0.13769 0.575213 -0.004685 0.3380 0.21145 -0.2577 [3,] 0.05955 -0.004685 0.494122 -0.1608 0.43512 0.1767 [4,] 0.27379 0.338018 -0.160788 0.4941 -0.10178 0.1566 [5,] 0.02945 0.211446 0.435116 -0.1018 0.49437 -0.0686 [6,] 0.27515 -0.257678 0.176687 0.1566 -0.06860 0.7178 As shown previously, we can also use the H-matrix to compute the fitted values, by post-multiplying H by the vector of outcomes: # Compute fitted values H %*% y [,1] [1,] 2.891 [2,] 3.719 [3,] 3.193 [4,] 3.007 [5,] 3.565 [6,] 2.225 In this computation, we can see that the fitted values can be expressed as linear combinations of the response vector Y using coefficients found in H. (This is why H is often referred to as the hat matrix.) For example, the first fitted value is computed as: \\[ \\begin{split} \\hat{y}_1 &amp;= 0.2244(3.0) + 0.1377(3.9) + 0.0595(2.9) + 0.2738(2.7) + 0.0294(3.7) + 0.2751(2.4) \\\\[2ex] &amp;= 2.891 \\end{split} \\] The H-matrix has many uses in regression. Aside from using it to compute the vectors of fitted values and residuals, it is also used to obtain leverage measures for each of the observations. Additionally, the trace of the H-matrix indicates the number of parameters (including the intercept) that are being estimated in the regression model. For example, in our regression model, we are estimating three parameters: \\(\\beta_0\\), \\(\\beta_{\\mathrm{SAT}}\\), and \\(\\beta_{\\mathrm{Self\\mbox{-}Esteem}}\\). # Compute trace of H sum(diag(H)) [1] 3 27.3 Vector of Residuals The \\(n\\times 1\\) vector of residuals, e, can be computed as, \\[ \\mathbf{e} = \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n\\end{bmatrix} = \\mathbf{y} - \\mathbf{\\hat{y}} \\] Doing a little substitution, \\[ \\begin{split} \\mathbf{e} &amp;= \\mathbf{y} - \\mathbf{\\hat{y}} \\\\ &amp;= \\mathbf{y} - \\mathbf{Hy} \\\\ &amp;= (\\mathbf{I} - \\mathbf{H}) \\mathbf{y} \\end{split} \\] Thus the residuals can also be expressed as linear combinations of the response vector y. The matrix \\((\\mathbf{I}-\\mathbf{H})\\) has some of the same properties as H, namely, it is Square, Symmetric, and Idempotent We can compute the residual vector using R # Create 6x6 identity matrix I = diag(6) # Compute the vector of residuals residuals = (I - H) %*% y # View vector of residuals residuals [,1] [1,] 0.1086 [2,] 0.1806 [3,] -0.2932 [4,] -0.3068 [5,] 0.1355 [6,] 0.1753 "],["sums-of-squares-in-regression.html", "Chapter 28 Sums of Squares in Regression 28.1 Sums of Squares 28.2 Sums of Squares as Functions of the Data", " Chapter 28 Sums of Squares in Regression In this chapter, you will learn about how matrix algebra is used to compute sum of square terms in regression. 28.1 Sums of Squares Recall that the regression model can be used to partition the total variation in the outcome into that which is explained by the regression model and that which is not explained by the model. To do this we compute sums of square terms such that: \\[ \\mathrm{SS}_{\\mathrm{Total}} = \\mathrm{SS}_{\\mathrm{Model}} + \\mathrm{SS}_{\\mathrm{Residual}} \\] where, algebraically, \\[ \\begin{split} \\mathrm{SS}_{\\mathrm{Total}} &amp;= \\sum(Y_i - \\bar{Y})^2\\\\[2ex] \\mathrm{SS}_{\\mathrm{Model}} &amp;= \\sum(\\hat{Y}_i - \\bar{Y})^2\\\\[2ex] \\mathrm{SS}_{\\mathrm{Residual}} &amp;= \\sum(Y_i - \\hat{Y}_i)^2 \\end{split} \\] We can also express each of these sums of squares using matrix notation. Remember that: \\[ \\mathbf{y} = \\hat{\\mathbf{y}} + \\mathbf{e} \\] To make this mathematically easier, we will subtract the mean vector from the outcome. \\[ \\mathbf{y} - \\bar{\\mathbf{y}} = \\hat{\\mathbf{y}} + \\mathbf{e} - \\bar{\\mathbf{y}} \\] Re-arranging the right-hand side of the equation: \\[ \\mathbf{y} - \\bar{\\mathbf{y}} = (\\hat{\\mathbf{y}}- \\bar{\\mathbf{y}}) + \\mathbf{e} \\] Now remember that the residuals and the fitted values are independent, which means that the vectors are orthogonal and the Pythagorean Theorem applies. That is: \\[ (\\mathbf{y} - \\bar{\\mathbf{y}})^2 = (\\hat{\\mathbf{y}}- \\bar{\\mathbf{y}})^2 + \\mathbf{e}^2 \\] Since these are vectors, squaring them creates dot products, which are sums. In this case, the sums of squares. That is, in matrix notation: \\[ \\begin{split} \\mathrm{SS}_{\\mathrm{Total}} &amp;= (\\mathbf{y} - \\bar{\\mathbf{y}})^2 \\\\[2ex] &amp;= (\\mathbf{y} - \\bar{\\mathbf{y}})^\\intercal(\\mathbf{y} - \\bar{\\mathbf{y}})\\\\[4ex] \\mathrm{SS}_{\\mathrm{Model}} &amp;= (\\hat{\\mathbf{y}}- \\bar{\\mathbf{y}})^2\\\\[2ex] &amp;= (\\hat{\\mathbf{y}}- \\bar{\\mathbf{y}})^\\intercal (\\hat{\\mathbf{y}}- \\bar{\\mathbf{y}}) \\\\[4ex] \\mathrm{SS}_{\\mathrm{Residual}} &amp;= \\mathbf{e}^2 \\\\[2ex] &amp;= \\mathbf{e}^\\intercal \\mathbf{e} \\end{split} \\] 28.1.1 Mean Centering the Outcome If the outcome, y, is mean centered—it is a mean deviation vector—then the math is simplified further. In this case, since the mean of a mean centered variable is zero, the quantity \\(\\mathbf{y} - \\bar{\\mathbf{y}}=\\mathbf{y} - \\mathbf{0} =\\mathbf{y}\\), and the total sum of squares is, simply, \\[ \\mathrm{SS}_{\\mathrm{Total}} = \\mathbf{y}^\\intercal\\mathbf{y} \\] Similarly, the model sum of squares also is simplified using a mean centered outcome to, \\[ \\mathrm{SS}_{\\mathrm{Model}} = \\hat{\\mathbf{y}}^\\intercal\\hat{\\mathbf{y}} \\] Finally, the residual sum of squares is \\[ \\mathrm{SS}_{\\mathrm{Residual}} = \\mathbf{e}^\\intercal\\mathbf{e} \\] This means that for mean centered data, \\[ \\begin{split} \\mathrm{SS}_{\\mathrm{Total}} &amp;= \\mathrm{SS}_{\\mathrm{Model}} + \\mathrm{SS}_{\\mathrm{Residual}} \\\\[2ex] \\mathbf{y}^\\intercal\\mathbf{y} &amp;= \\hat{\\mathbf{y}}^\\intercal\\hat{\\mathbf{y}} + \\mathbf{e}^\\intercal\\mathbf{e} \\end{split} \\] 28.2 Sums of Squares as Functions of the Data The model and residual sums of squares can also be written as products of the design matrix, X, and the vector of outcomes, y. To do this, we will make use of the relationships between \\(\\hat{\\mathbf{y}}\\), e, and the H-matrix. Remember that \\[ \\begin{split} \\hat{\\mathbf{y}} &amp;= \\mathbf{Hy} \\\\[2ex] \\mathbf{e} &amp;= (\\mathbf{I} - \\mathbf{H}) \\mathbf{y} \\end{split} \\] where \\(\\mathbf{H}=\\mathbf{X} (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\). We can re-write the sum of squared model using this formula, \\[ \\begin{split} \\mathrm{SS}_{\\mathrm{Model}} &amp;= \\hat{\\mathbf{y}}^\\intercal\\hat{\\mathbf{y}} \\\\[2ex] &amp;= (\\mathbf{Hy})^\\intercal\\mathbf{Hy} \\end{split} \\] Using the rules of transposes \\[ \\mathrm{SS}_{\\mathrm{Model}} = \\mathbf{y}^\\intercal\\mathbf{H}^\\intercal\\mathbf{Hy} \\] Also, remember that the H matrix is square and symmetric (\\(\\mathbf{H}=\\mathbf{H}^\\intercal\\)) and that it is idempotent (\\(\\mathbf{HH} = \\mathbf{H}\\)). This implies that \\(\\mathbf{HH}^\\intercal = \\mathbf{H}\\), and, \\[ \\begin{split} \\mathrm{SS}_{\\mathrm{Model}} &amp;= \\mathbf{y}^\\intercal\\mathbf{H}^\\intercal\\mathbf{Hy} \\\\[2ex] &amp;= \\mathbf{y}^\\intercal\\mathbf{Hy} \\\\[2ex] &amp;= \\mathbf{y}^\\intercal\\mathbf{X} (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\mathbf{y} \\end{split} \\] We can carry out a similar substitution and reduction to re-write the sum of squared residuals. \\[ \\begin{split} \\mathrm{SS}_{\\mathrm{Model}} &amp;= \\mathbf{y}^\\intercal\\mathbf{H}^\\intercal\\mathbf{Hy} \\\\[2ex] &amp;= \\mathbf{y}^\\intercal\\mathbf{Hy} \\\\[2ex] &amp;= \\mathbf{y}^\\intercal\\mathbf{X} (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\mathbf{y} \\end{split} \\] We can write \\(\\hat{\\mathbf{y}}=\\mathbf{Xb}\\), which means \\[ \\begin{split} \\mathrm{SS}_{\\mathrm{Model}} &amp;= (\\mathbf{Xb})^\\intercal\\mathbf{Xb} \\\\[2ex] &amp;= \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb} \\end{split} \\] Then, since \\(\\mathbf{b}=(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\\), we can substitute, getting, \\[ \\begin{split} \\mathrm{SS}_{\\mathrm{Model}} &amp;= \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb}\\\\[2ex] &amp;= \\bigg((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\\bigg)^\\intercal\\mathbf{X}^\\intercal\\mathbf{X}\\bigg((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\\bigg) \\end{split} \\] Then, using the rules of transposes and inverses, \\[ \\begin{split} \\mathrm{SS}_{\\mathrm{Model}} &amp;= \\bigg((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\\bigg)^\\intercal\\mathbf{X}^\\intercal\\mathbf{X}\\bigg((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\\bigg) \\\\[2ex] &amp;= \\mathbf{y}^\\intercal (\\mathbf{X}^\\intercal)^\\intercal \\bigg((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\bigg)^\\intercal \\mathbf{X}^\\intercal\\mathbf{X}(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y} \\\\[2ex] &amp;= \\mathbf{y}^\\intercal\\mathbf{X} \\bigg((\\mathbf{X}^\\intercal\\mathbf{X})^\\intercal\\bigg)^{-1} \\mathbf{I}\\mathbf{X}^\\intercal\\mathbf{y} \\\\[2ex] &amp;= \\mathbf{y}^\\intercal\\mathbf{X} (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\mathbf{y} \\end{split} \\] Finally, we can also rewrite the residual sum of squares, again, using the rules of matrix algebra. \\[ \\begin{split} \\mathrm{SS}_{\\mathrm{Residual}} &amp;= \\mathbf{e}^\\intercal\\mathbf{e} \\\\[2ex] &amp;= \\bigg((\\mathbf{I}-\\mathbf{H})\\mathbf{y}\\bigg)^\\intercal (\\mathbf{I}-\\mathbf{H})\\mathbf{y} \\\\[2ex] &amp;= \\mathbf{y}^\\intercal (\\mathbf{I}-\\mathbf{H})^\\intercal (\\mathbf{I}-\\mathbf{H})\\mathbf{y} \\\\[2ex] &amp;= \\mathbf{y}^\\intercal (\\mathbf{I}-\\mathbf{H})\\mathbf{y} \\\\[2ex] &amp;= \\mathbf{y}^\\intercal (\\mathbf{y}-\\mathbf{Hy}) \\\\[2ex] &amp;= \\mathbf{y}^\\intercal\\mathbf{y} - \\mathbf{y}^\\intercal\\mathbf{Hy} \\\\[2ex] &amp;= \\mathbf{y}^\\intercal\\mathbf{y} - \\mathbf{y}^\\intercal\\mathbf{X} (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\mathbf{y} \\end{split} \\] And, of course \\(\\mathrm{SS}_{\\mathrm{Total}}=\\mathbf{y}^\\intercal\\mathbf{y}\\). Each of the sums of squares is a function of the design matrix or the vector of outcomes. Don’t forget that for these computations, the vector of outcomes needs to be mean-centered! 28.2.1 Example Using R To illustrate these computations, we will again use the following toy data set to fit a regression model using SAT and Self-Esteem to predict variation in GPA. Table 28.1: Example set of education data. ID SAT GPA Self-Esteem IQ 1 560 3.0 11 112 2 780 3.9 10 143 3 620 2.9 19 124 4 600 2.7 7 129 5 720 3.7 18 130 6 380 2.4 13 82 Fitting the model and partitioning the variation (not shown), we find: \\(\\mathrm{SS}_{\\mathrm{Total}} = 1.7\\) \\(\\mathrm{SS}_{\\mathrm{Model}} = 1.426\\) \\(\\mathrm{SS}_{\\mathrm{Residual}} = 0.274\\) Using the matrix algebra, we can replicate these values. # Create vector of outcomes y = c(3.0, 3.9, 2.9, 2.7, 3.7, 2.4) # Mean-center y mc_y = y - mean(y) # Create design matrix X = matrix( data = c( rep(1, 6), 560, 780, 620, 600, 720, 380, 11, 10, 19, 7, 18, 13 ), ncol = 3 ) # Compute SS_total t(mc_y) %*% mc_y [,1] [1,] 1.7 # Compute SS_model t(mc_y) %*% X %*% solve(t(X) %*% X) %*% t(X) %*% mc_y [,1] [1,] 1.426 # Compute SS_residuals t(mc_y) %*% mc_y - t(mc_y) %*% X %*% solve(t(X) %*% X) %*% t(X) %*% mc_y [,1] [1,] 0.2736 "],["standard-errors-and-variance-estimates.html", "Chapter 29 Standard Errors and Variance Estimates 29.1 Residual Variance of the Model 29.2 Coefficient Variances and Covariances 29.3 Standard Errors for the Coefficients 29.4 Correlation Between the Coefficients 29.5 Inference: Model-Level 29.6 Inference: Coefficient-Level", " Chapter 29 Standard Errors and Variance Estimates In this chapter, you will learn about how matrix algebra is used to compute standard errors and variance estimates in regression. These allow us to compute confidence intervals and carry out hypothesis tests. To illustrate computations, we will again use the following toy data set to fit a regression model using SAT and Self-Esteem to predict variation in GPA. Table 29.1: Example set of education data. ID SAT GPA Self-Esteem IQ 1 560 3.0 11 112 2 780 3.9 10 143 3 620 2.9 19 124 4 600 2.7 7 129 5 720 3.7 18 130 6 380 2.4 13 82 29.1 Residual Variance of the Model Recall that the estimate of the residual variance (i.e., mean squared residuals) for the model is \\[ \\hat\\sigma^2_{\\epsilon} = \\frac{\\mathrm{SS}_{\\mathrm{Residuals}}}{\\mathrm{df}_{\\mathrm{Residuals}}} \\] Using matrices, the sum of squared error (\\(\\mathrm{SS}_{\\mathrm{Residuals}}\\)) is \\[ \\mathrm{SS}_{\\mathrm{Residuals}} = \\mathbf{e}^\\intercal\\mathbf{e} \\] The degrees-of-freedom associated with the residuals is \\(\\mathrm{df}_{\\mathrm{Residuals}} = n-k\\), where \\(k\\) is the number of coefficients (including the intercept) being estimated in the model. Recall that the value of \\(k\\) is the trace of the H-matrix. This implies, \\[ \\hat\\sigma^2_{\\epsilon} = \\frac{\\mathbf{e}^\\intercal\\mathbf{e}}{n - \\mathrm{tr}(\\mathbf{H})} \\] Using our toy example, # Create vector of outcomes y = c(3.0, 3.9, 2.9, 2.7, 3.7, 2.4) # Create design matrix X = matrix( data = c( rep(1, 6), 560, 780, 620, 600, 720, 380, 11, 10, 19, 7, 18, 13 ), ncol = 3 ) # Compute SS_residual b = solve(t(X) %*% X) %*% t(X) %*% y e = y - X %*% b ss_resid = t(e) %*% e # Compute df_residual H = X %*% solve(t(X) %*% X) %*% t(X) k = sum(diag(H)) df_resid = 6 - k # Compute estimate of error variance var_e = ss_resid / df_resid var_e [,1] [1,] 0.0912 If we take the sqaure root of this estimate, that gives us the residual standard error (RSE) of the model, a.k.a., the root mean square error (RMSE). # Find RSE/RMSE sqrt(var_e) [,1] [1,] 0.302 29.2 Coefficient Variances and Covariances In matrix applications, it is typical to indicate the error variances and covariances for the coefficients in a variance–covariance matrix. In this matrix the error variances for each of the coefficients is given along the main diagonal of the matrix. The covariances between the coefficients are given in the off-diagonal elements. For example, the variance–covariance matrix for a simple regression model is: \\[ \\boldsymbol{\\sigma^2_B} = \\begin{bmatrix}\\mathrm{Var}(b_0) &amp; \\mathrm{Cov}(b_0,b_1) \\\\ \\mathrm{Cov}(b_0,b_1) &amp; \\mathrm{Var}(b_1) \\end{bmatrix} \\] Examining the variance-covariance matrix of the coefficients, we find that it is a square, symmetric matrix with dimensions of \\(k \\times k\\). This matrix is defined as: \\[ \\boldsymbol{\\sigma^2_B} = \\sigma^2_{\\epsilon} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1} \\] In our toy example, # Variance-covariance matrix of the coefficients V_b = as.numeric(var_e) * solve(t(X) %*% X) V_b [,1] [,2] [,3] [1,] 0.4741276 -5.504e-04 -9.477e-03 [2,] -0.0005504 9.501e-07 -2.246e-06 [3,] -0.0094769 -2.246e-06 8.344e-04 We use as.numeric() to convert the estimate of the model-level error variance to a numeric scalar. The estimated variance for each of the coefficients are: \\(\\mathrm{Var}(\\hat\\beta_0) = 0.474\\) \\(\\mathrm{Var}(\\hat\\beta_{\\mathrm{SAT}}) = 0.00000095\\) \\(\\mathrm{Var}(\\hat\\beta_{\\mathrm{Self\\mbox{-}Esteem}}) = 0.00083\\) The covariances between these coefficients are also given in this matrix: \\(\\mathrm{Cov}(\\hat\\beta_0,\\hat\\beta_{\\mathrm{SAT}}) = -0.00055\\) \\(\\mathrm{Cov}(\\hat\\beta_0,\\hat\\beta_{\\mathrm{Self\\mbox{-}Esteem}}) = -0.0095\\) \\(\\mathrm{Cov}(\\hat\\beta_{\\mathrm{SAT}},\\hat\\beta_{\\mathrm{Self\\mbox{-}Esteem}}) = -0.0000022\\) In practice, the variance-covariance matrix of the regression coefficients can be obtained directly from R using the vcov() function. # Fit model lm.1 = lm(y ~ 1 + X[ , 2] + X[ , 3]) # Obtain variance-covariance matrix of the coefficients vcov(lm.1) (Intercept) X[, 2] X[, 3] (Intercept) 0.4741276 -5.504e-04 -9.477e-03 X[, 2] -0.0005504 9.501e-07 -2.246e-06 X[, 3] -0.0094769 -2.246e-06 8.344e-04 29.3 Standard Errors for the Coefficients While the variances are mathematically convenient, in applied work, the standard errors (SEs) of the coefficients are more commonly presented. Recall that the standard errors are given in the same metric as the coefficient estimates, and represent the uncertainty in that estimate. They are also used to construct test statistics (e.g., z or t) and confidence intervals for the estimates. The estimated standard error for each regression coefficient can be found by computing the square root of the variance estimates, that is: \\[ \\mathrm{SE}(\\beta_j) = \\sqrt{\\mathrm{Var}(\\beta_j)} \\] Thus, we can find the standard errors by computing the square roots of the diagonal elements in the variance–covariance matrix of the coefficients. # Compute SEs se = sqrt(diag(V_b)) # View SEs se [1] 0.6885692 0.0009747 0.0288855 29.4 Correlation Between the Coefficients Recall that correlation, which is a standardized covariance, is often times more interpretable than the covariance. We can obtain the correlation coefficient between two coefficients, \\(\\hat\\beta_j\\) and \\(\\hat\\beta_k\\), using \\[ \\mathrm{Cor}(b_j,b_k) = \\frac{\\mathrm{Cov}(b_j,b_k)}{\\sqrt{\\mathrm{Var}(b_j)\\times \\mathrm{Var}(b_k)}} \\] Using the values from our example, we can compute the correlation between each set of regression coefficients: \\[ \\begin{split} \\mathrm{Cor}(\\hat\\beta_0,\\hat\\beta_{\\mathrm{SAT}}) &amp;= \\frac{-0.00055}{\\sqrt{0.474 \\times 0.00000095}} &amp;= -0.820 \\\\[2ex] \\mathrm{Cor}(\\hat\\beta_0,\\hat\\beta_{\\mathrm{Self\\mbox{-}Esteem}}) &amp;= \\frac{-0.0095}{\\sqrt{0.474 \\times 0.00083}} &amp;= -0.479 \\\\[2ex] \\mathrm{Cor}(\\hat\\beta_{\\mathrm{SAT}},\\hat\\beta_{\\mathrm{Self\\mbox{-}Esteem}}) &amp;= \\frac{-0.0000022}{\\sqrt{0.00000095 \\times 0.00083}} &amp;= -0.078 \\end{split} \\] In R, if we have assigned the variance-covariance matrix to an object, we can use indexing to access the different elements to compute the correlation. # Compute correlation between b_0 and b_SAT V_b[1, 2] / sqrt(V_b[1, 1] * V_b[2, 2]) [1] -0.82 # Compute correlation between b_0 and b_selfesteem V_b[1, 3] / sqrt(V_b[1, 1] * V_b[3, 3]) [1] -0.4765 # Compute correlation between b_SAT and b_selfesteem V_b[2, 3] / sqrt(V_b[2, 2] * V_b[3, 3]) [1] -0.07976 The correlations indicates that the intercept is highly, negatively correlated with both the other coefficients. Whereas the effects of SAT and self-esteem seem to be mostly uncorrelated (i.e., independent). 29.5 Inference: Model-Level The model-level (i.e., omnibus) null hypothesis for the regression model \\[ Y_i = \\beta_0 + \\beta_1(X_{1_i}) + \\beta_2(X_{2_i}) + \\beta_3(X_{3_i}) + \\ldots + \\beta_k(X_{k_i}) + \\epsilon_i \\] is commonly expressed as: \\[ H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\ldots = \\beta_k = 0 \\] If we write the set of predictor coefficients as a vector, \\(\\boldsymbol{\\beta}^\\intercal = \\begin{bmatrix} \\beta_1 &amp; \\beta_2 &amp; \\beta_3 &amp; \\ldots &amp; \\beta_k\\end{bmatrix}\\), then we can write the model-level null hypothesis using vector notation: \\[ H_0: \\boldsymbol{\\beta} = 0 \\] Note that the we can express the vector of coefficients as either a column vector (\\(\\boldsymbol{\\beta}\\)) or as a row vector (\\(\\boldsymbol{\\beta}^\\intercal\\)). To test this we compute an F-statistic based on the ratio of the mean square for the model and that for the residuals. This is evaluted in an F-distribution having \\(k-1\\) and \\(n-k\\) degrees of freedom. (See here for a reminder about the model-level test.) Using our toy-example: # Compute MS_model mc_y = y - mean(y) # Mean-center y ss_model = t(mc_y) %*% X %*% solve(t(X) %*% X) %*% t(X) %*% mc_y ms_model = ss_model / (k - 1) ms_model [,1] [1,] 0.7132 # Compute MS_residual ss_resid = t(mc_y) %*% mc_y - t(mc_y) %*% X %*% solve(t(X) %*% X) %*% t(X) %*% mc_y ms_resid = ss_resid / (6 - k) ms_resid [,1] [1,] 0.0912 # Compute F F_stat = ms_model / ms_resid F_stat [,1] [1,] 7.82 # Compute p-value 1 - pf(F_stat, df1 = (k-1), df2 = (6-k)) [,1] [1,] 0.06456 29.6 Inference: Coefficient-Level The coefficient-level null hypotheses for the regression model are: \\[ \\begin{split} H_0: \\beta_j = 0 \\end{split} \\] for each j. To test this we compute a t-statistic based on the ratio of the coefficent estimate and that its associated standard error. This is evaluted in a t-distribution having \\(n-k\\) degrees of freedom. (See here for a reminder about the coefficient-level tests.) Using our toy-example: # Compute t for all three coefficients t_stat = b / se t_stat [,1] [1,] 0.9574 [2,] 3.9041 [3,] 0.3180 # Compute p-value 2 * pt(-abs(t_stat), df = (6-k), lower.tail = TRUE) [,1] [1,] 0.40901 [2,] 0.02984 [3,] 0.77130 We could also compute a confidence interval using, \\[ B_j \\pm t^*(\\mathrm{SE}_{B_j}) \\] where \\(B_j\\) is the jth coefficient, \\(\\mathrm{SE}_{B_j}\\) is the standard error associated with that coefficient, and \\(t^*\\) is the appropriate critical value. Using our toy example, # Obtain critical value for 95% CI t_star = abs(qt(.025, df = (6-k))) # Compute lower limit of CI b - t_star*se [,1] [1,] -1.5321211 [2,] 0.0007035 [3,] -0.0827395 # Compute upper limit of CI b + t_star*se [,1] [1,] 2.850548 [2,] 0.006908 [3,] 0.101113 "],["assumptions-of-the-regression-model.html", "Chapter 30 Assumptions of the Regression Model 30.1 Regression Assumptions 30.2 Expressing the Assumptions uising Matrix Algebra 30.3 Variance-Covariance Matrix of the Residuals 30.4 Regression Model: Revisited", " Chapter 30 Assumptions of the Regression Model In this chapter, you will learn about how matrix algebra is used to express and understand the distributional assumptions underlying the regression model. To illustrate computations, we will again use the following toy data set to fit a regression model using SAT and Self-Esteem to predict variation in GPA. Table 30.1: Example set of education data. ID SAT GPA Self-Esteem IQ 1 560 3.0 11 112 2 780 3.9 10 143 3 620 2.9 19 124 4 600 2.7 7 129 5 720 3.7 18 130 6 380 2.4 13 82 30.1 Regression Assumptions There are several distributional assumptions about the errors for the regression model, namely that the residuals (conditioned on each fitted value) are: Independent; Normally distributed; Average residual is 0; and Constant variance. Mathematically, we express this as part of the model as: \\[ \\epsilon_{i|\\hat{y}} \\overset{\\mathrm{i.i.d~}}{\\sim} \\mathcal{N}\\left(0, \\sigma^2_{\\epsilon}\\right) \\] The mathematical expression says the residuals conditioned on \\(\\hat{y}\\) (having the same fitted value) are independent and identically normally distributed with a mean of 0 and some variance (\\(\\sigma^2_{\\epsilon}\\)). (See here for a reminder about the distributional assumptions for regression models.) 30.2 Expressing the Assumptions uising Matrix Algebra We can express these same assumptions using matrix notation. First recall that the residuals from the model can be expressed as a vector: \\[ \\boldsymbol{\\epsilon} = \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2\\\\ \\epsilon_3\\\\ \\vdots \\\\ \\epsilon_n\\end{bmatrix} \\] Note: Here we use \\(\\boldsymbol{\\epsilon}\\) rather than e to indicate that the vector is for the residuals in the population. The assumption that the average residual (at each \\(\\hat{y}\\)) is zero can be written using expectations as: \\[ \\begin{split} E(\\boldsymbol{\\epsilon}) = \\mathbf{0} \\\\[2em] E\\begin{pmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2\\\\ \\epsilon_3\\\\ \\vdots \\\\ \\epsilon_n\\end{bmatrix}\\end{pmatrix} = \\begin{bmatrix}0 \\\\ 0\\\\ 0\\\\ \\vdots \\\\ 0\\end{bmatrix} \\end{split} \\] The constant variance assumption says that the variance of the residuals (at each \\(\\hat{y}\\)) is the same, \\(\\sigma^2_{\\epsilon}\\). Remember that this is the residual variance for the model, which we estimate as: \\[ \\hat\\sigma^2_{\\epsilon} = \\frac{\\mathbf{e}^\\intercal\\mathbf{e}}{n - \\mathrm{tr}(\\mathbf{H})} \\] The independence assumption implies that the covariance between two residuals is 0. Namely, \\[ \\mathrm{Cov}(\\epsilon_i, \\epsilon_j) = 0 \\] for all \\(i\\neq j\\). 30.3 Variance-Covariance Matrix of the Residuals If we combine the assumption of independence with the assumption of constant variance, we can write out the variance-covariance matrix of the residuals. \\[ \\boldsymbol{\\Sigma_\\epsilon} = \\begin{bmatrix}\\sigma^2_{\\epsilon} &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; \\sigma^2_{\\epsilon} &amp; 0 &amp; \\ldots &amp; 0\\\\ 0 &amp; 0 &amp; \\sigma^2_{\\epsilon} &amp; \\ldots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; \\sigma^2_{\\epsilon}\\end{bmatrix} \\] Notice that the variance-covariance matrix of the residuals is a diagonal, scalar matrix. The variances are given along the main diagonal and the covariances are the off-diagonal elements. This matrix indicates that the variances are all the same value and the covariances between residuals is 0 (i.e., independence). This matrix can also be expressed as the product of \\(\\sigma^2_{\\epsilon}\\) and an \\(n \\times n\\) identity matrix: \\[ \\begin{split} \\boldsymbol{\\Sigma_\\epsilon} &amp;= \\begin{bmatrix}\\sigma^2_{\\epsilon} &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; \\sigma^2_{\\epsilon} &amp; 0 &amp; \\ldots &amp; 0\\\\ 0 &amp; 0 &amp; \\sigma^2_{\\epsilon} &amp; \\ldots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; \\sigma^2_{\\epsilon}\\end{bmatrix} \\\\[2ex] &amp;= \\sigma^2_{\\epsilon} \\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; \\ldots &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; \\ldots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 1\\end{bmatrix}\\\\[2ex] &amp;= \\sigma^2_{\\epsilon}~ \\mathbf{I} \\end{split} \\] Using our toy example, the variance-covariance matrix of the residuals can be computed as: # Create vector of outcomes y = c(3.0, 3.9, 2.9, 2.7, 3.7, 2.4) # Create design matrix X = matrix( data = c( rep(1, 6), 560, 780, 620, 600, 720, 380, 11, 10, 19, 7, 18, 13 ), ncol = 3 ) # Compute residual variance b = solve(t(X) %*% X) %*% t(X) %*% y e = y - X %*% b ss_resid = t(e) %*% e var_e = ss_resid / df_resid # Compute variance-covariance matrix of residuals as.numeric(var_e) * diag(6) [,1] [,2] [,3] [,4] [,5] [,6] [1,] 0.0912 0.0000 0.0000 0.0000 0.0000 0.0000 [2,] 0.0000 0.0912 0.0000 0.0000 0.0000 0.0000 [3,] 0.0000 0.0000 0.0912 0.0000 0.0000 0.0000 [4,] 0.0000 0.0000 0.0000 0.0912 0.0000 0.0000 [5,] 0.0000 0.0000 0.0000 0.0000 0.0912 0.0000 [6,] 0.0000 0.0000 0.0000 0.0000 0.0000 0.0912 30.4 Regression Model: Revisited Using matrix notation, we compactly specify the regression model as: \\[ \\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\] where \\(\\boldsymbol{\\epsilon}_{\\vert\\hat{y}}\\) is a vector of independent random variables (conditioned on some predicted value) that is normally distributed with: \\(E(\\boldsymbol{\\epsilon})=0\\) and \\(\\boldsymbol{\\Sigma_\\epsilon}= \\sigma^2_{\\epsilon}\\mathbf{I}\\). "],["references.html", "References", " References "]]

<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Statistical Appplication of Vectors | Matrix Algebra for Educational Scientists" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Matrix algebra content for QME students" />
<meta name="github-repo" content="zief0002/matrix-algebra" />

<meta name="author" content="Michael Rodriguez &amp; Andrew Zieffler" />

<meta name="date" content="2021-05-05" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Matrix algebra content for QME students">

<title>Statistical Appplication of Vectors | Matrix Algebra for Educational Scientists</title>

<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap/js/bootstrap.min.js"></script>
<script src="libs/bootstrap/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation/tabsets.js"></script>
<script src="libs/kePrint/kePrint.js"></script>
<link href="libs/lightable/lightable.css" rel="stylesheet" />

<script type="text/javascript">
<!--
    function toggle_visibility(id) {
       var e = document.getElementById(id);
       if(e.style.display == 'block')
          e.style.display = 'none';
       else
          e.style.display = 'block';
    }
//-->
</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  { color: #cccccc; background-color: #303030; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ffcfaf; } /* Alert */
code span.an { color: #7f9f7f; font-weight: bold; } /* Annotation */
code span.at { } /* Attribute */
code span.bn { color: #dca3a3; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #f0dfaf; } /* ControlFlow */
code span.ch { color: #dca3a3; } /* Char */
code span.cn { color: #dca3a3; font-weight: bold; } /* Constant */
code span.co { color: #7f9f7f; } /* Comment */
code span.cv { color: #7f9f7f; font-weight: bold; } /* CommentVar */
code span.do { color: #7f9f7f; } /* Documentation */
code span.dt { color: #dfdfbf; } /* DataType */
code span.dv { color: #dcdccc; } /* DecVal */
code span.er { color: #c3bf9f; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #c0bed1; } /* Float */
code span.fu { color: #efef8f; } /* Function */
code span.im { } /* Import */
code span.in { color: #7f9f7f; font-weight: bold; } /* Information */
code span.kw { color: #f0dfaf; } /* Keyword */
code span.op { color: #f0efd0; } /* Operator */
code span.ot { color: #efef8f; } /* Other */
code span.pp { color: #ffcfaf; font-weight: bold; } /* Preprocessor */
code span.sc { color: #dca3a3; } /* SpecialChar */
code span.ss { color: #cc9393; } /* SpecialString */
code span.st { color: #cc9393; } /* String */
code span.va { } /* Variable */
code span.vs { color: #cc9393; } /* VerbatimString */
code span.wa { color: #7f9f7f; font-weight: bold; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="style/style.css" type="text/css" />
<link rel="stylesheet" href="style/table-styles.css" type="text/css" />
<link rel="stylesheet" href="style/syntax.css" type="text/css" />
<link rel="stylesheet" href="style/navbar.css" type="text/css" />
<link rel="stylesheet" href="style/notes.css" type="text/css" />
<link rel="stylesheet" href="style/buttons.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#foreword">Foreword</a></li>
<li><a href="intro.html#intro">Introduction</a></li>
<li><a href="datastructures.html#datastructures">Data Structures</a></li>
<li><a href="vec.html#vec">Vectors</a></li>
<li><a href="vecop.html#vecop">Vector Operations</a></li>
<li><a href="vector-geometry-angles-projection-and-decomposition.html#vector-geometry-angles-projection-and-decomposition">Vector Geometry: Angles, Projection, and Decomposition</a></li>
<li><a href="statistical-appplication-of-vectors.html#statistical-appplication-of-vectors">Statistical Appplication of Vectors</a></li>
<li><a href="matrices-1.html#matrices-1">Matrices</a></li>
<li><a href="matrix-operations.html#matrix-operations">Matrix Operations</a></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="statistical-appplication-of-vectors" class="section level1">
<h1>Statistical Appplication of Vectors</h1>
<p>In this chapter, we will provide examples of how vectors define the underlying geometry of various statistical summaries (standard deviation and correlation coefficient), including linear models. We will provide an example using a single predictor, but again, the ideas can be extended to models that include multiple predictors.</p>
<div id="deviation-scores-and-the-standard-deviation" class="section level2">
<h2>Deviation Scores and the Standard Deviation</h2>
<p>Consider the following vector scores (<strong>x</strong>) and the vector of the mean deviation scores (<span class="math inline">\(\mathbf{d_x}\)</span>):</p>
<p><span class="math display">\[
\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n\end{bmatrix} \qquad \mathbf{d_x} = \begin{bmatrix} x_1-\bar{x} \\ x_2-\bar{x} \\ x_3-\bar{x} \\ \vdots \\ x_n-\bar{x}\end{bmatrix}
\]</span></p>
<p>Remember that the length of a vector is the square root of the dot product of the vector with itself. If we were to compute the length of the deviation score vector, the length would be the square root of the sum of squared deviations:</p>
<p><span class="math display">\[
\sqrt{\mathbf{d_x}\bullet \mathbf{d_x}} = \sqrt{(x_1 - \bar{x})^2 + (x_2 - \bar{x})^2 + (x_3 - \bar{x})^2 + \ldots + (x_n - \bar{x})^2}
\]</span></p>
<p>If we divided this result by <span class="math inline">\(\sqrt{n}\)</span>, this would be equivalent to the standard deviation of the scores in <strong>x</strong> (<span class="math inline">\(s_\mathbf{x}\)</span>).</p>
<p><span class="math display">\[
s_\mathbf{x} = \frac{\lvert\lvert\mathbf{d_x}\rvert\rvert}{\sqrt{n}}
\]</span></p>
<p>So there is a direct relation between the length of a deviation vector and the standard deviation, namely,</p>
<p><span class="math display">\[
\lvert\lvert\mathbf{d_x}\rvert\rvert = \sqrt{n} (s_\mathbf{x})
\]</span></p>
<p>Consider the following vectors of data representing SAT scores (<strong>x</strong>) and GPA values (<strong>y</strong>) for <span class="math inline">\(n=10\)</span> student:</p>
<p><span class="math display">\[
\mathbf{x} = \begin{bmatrix}760 \\ 710 \\ 680 \\ 730 \\ 420 \\ 410 \\ 620 \\ 630 \\ 720 \\ 670\end{bmatrix} \qquad \mathbf{y} = \begin{bmatrix} 3.8 \\ 2.4 \\ 2.6 \\ 3.1 \\ 1.9 \\ 1.7 \\ 2.5 \\ 2.4 \\ 3.5 \\ 3.1\end{bmatrix}
\]</span></p>
<p>We compute the mean scores for each vector as <span class="math inline">\(\bar{x}=635\)</span> and <span class="math inline">\(\bar{y}=2.7\)</span>, respectively. By subtracting the mean (which is a scalar) from each vector of data, we can create deviation vectors:</p>
<p><span class="math display">\[
\mathbf{d_x} = \mathbf{x} - 635 = \begin{bmatrix}125 \\ 75 \\ 45 \\ 95 \\ -215 \\ -225 \\ -15 \\ -5 \\ 85 \\ 35\end{bmatrix} \qquad \mathbf{d_y} =\mathbf{y}-2.7 = \begin{bmatrix}1.1 \\ -0.3 \\ -0.1 \\ 0.4 \\ -0.8 \\ -1.0 \\ -0.2 \\ -0.3 \\ 0.8 \\ 0.4\end{bmatrix}
\]</span>
The length of the deviation vectors are computed as <span class="math inline">\(\sqrt{\mathbf{d}\bullet\mathbf{d}}\)</span>, namely,</p>
<p><span class="math display">\[
\begin{split}
\lvert\lvert\mathbf{d_x}\rvert\rvert &amp;= \sqrt{\mathbf{d_x}\bullet\mathbf{d_x}} \\[2em]
\lvert\lvert\mathbf{d_x}\rvert\rvert &amp;= \sqrt{\mathbf{d_y}\bullet\mathbf{d_y}}
\end{split}
\]</span></p>
<p>And using the values in the deviation vectors,</p>
<p><span class="math display">\[
\begin{split}
\lvert\lvert\mathbf{d_x}\rvert\rvert &amp;= \sqrt{137850} = 371.28 \\[2em]
\lvert\lvert\mathbf{d_x}\rvert\rvert &amp;= \sqrt{4.04} = 2.01
\end{split}
\]</span></p>
<p>Finally, we can compute the standard deviations for <strong>x</strong> and <strong>y</strong> by dividing these lengths by <span class="math inline">\(\sqrt{n}\)</span>.</p>
<p><span class="math display">\[
\begin{split}
s_\mathbf{x} &amp;= \frac{371.28}{\sqrt{10}} = 117.41 \\[2em]
s_\mathbf{y} &amp;= \frac{4.04}{\sqrt{10}} = 0.64
\end{split}
\]</span></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="statistical-appplication-of-vectors.html#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Original vectors</span></span>
<span id="cb67-2"><a href="statistical-appplication-of-vectors.html#cb67-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="fu">c</span>(<span class="dv">760</span>, <span class="dv">710</span>, <span class="dv">680</span>, <span class="dv">730</span>, <span class="dv">420</span>, <span class="dv">410</span>, <span class="dv">620</span>, <span class="dv">630</span>, <span class="dv">720</span>, <span class="dv">670</span>), <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb67-3"><a href="statistical-appplication-of-vectors.html#cb67-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="fu">c</span>(<span class="fl">3.8</span>, <span class="fl">2.4</span>, <span class="fl">2.6</span>, <span class="fl">3.1</span>, <span class="fl">1.9</span>, <span class="fl">1.7</span>, <span class="fl">2.5</span>, <span class="fl">2.4</span>, <span class="fl">3.5</span>, <span class="fl">3.1</span>), <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb67-4"><a href="statistical-appplication-of-vectors.html#cb67-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-5"><a href="statistical-appplication-of-vectors.html#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute deviation vectors</span></span>
<span id="cb67-6"><a href="statistical-appplication-of-vectors.html#cb67-6" aria-hidden="true" tabindex="-1"></a>d_x <span class="ot">=</span> x <span class="sc">-</span> <span class="fu">mean</span>(x)</span>
<span id="cb67-7"><a href="statistical-appplication-of-vectors.html#cb67-7" aria-hidden="true" tabindex="-1"></a>d_x</span></code></pre></div>
<pre><code>      [,1]
 [1,]  125
 [2,]   75
 [3,]   45
 [4,]   95
 [5,] -215
 [6,] -225
 [7,]  -15
 [8,]   -5
 [9,]   85
[10,]   35</code></pre>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="statistical-appplication-of-vectors.html#cb69-1" aria-hidden="true" tabindex="-1"></a>d_y <span class="ot">=</span> y <span class="sc">-</span> <span class="fu">mean</span>(y)</span>
<span id="cb69-2"><a href="statistical-appplication-of-vectors.html#cb69-2" aria-hidden="true" tabindex="-1"></a>d_y</span></code></pre></div>
<pre><code>      [,1]
 [1,]  1.1
 [2,] -0.3
 [3,] -0.1
 [4,]  0.4
 [5,] -0.8
 [6,] -1.0
 [7,] -0.2
 [8,] -0.3
 [9,]  0.8
[10,]  0.4</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="statistical-appplication-of-vectors.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute lengths of x deviation vector</span></span>
<span id="cb71-2"><a href="statistical-appplication-of-vectors.html#cb71-2" aria-hidden="true" tabindex="-1"></a>l_x <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(d_x <span class="sc">*</span> d_x))</span>
<span id="cb71-3"><a href="statistical-appplication-of-vectors.html#cb71-3" aria-hidden="true" tabindex="-1"></a>l_x</span></code></pre></div>
<pre><code>[1] 371.2816</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="statistical-appplication-of-vectors.html#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute lengths of y deviation vector</span></span>
<span id="cb73-2"><a href="statistical-appplication-of-vectors.html#cb73-2" aria-hidden="true" tabindex="-1"></a>l_y <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(d_y <span class="sc">*</span> d_y))</span>
<span id="cb73-3"><a href="statistical-appplication-of-vectors.html#cb73-3" aria-hidden="true" tabindex="-1"></a>l_y</span></code></pre></div>
<pre><code>[1] 2.009975</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="statistical-appplication-of-vectors.html#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute sd of x</span></span>
<span id="cb75-2"><a href="statistical-appplication-of-vectors.html#cb75-2" aria-hidden="true" tabindex="-1"></a>s_x <span class="ot">=</span> l_x <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">10</span>)</span>
<span id="cb75-3"><a href="statistical-appplication-of-vectors.html#cb75-3" aria-hidden="true" tabindex="-1"></a>s_x</span></code></pre></div>
<pre><code>[1] 117.4095</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="statistical-appplication-of-vectors.html#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute sd of y</span></span>
<span id="cb77-2"><a href="statistical-appplication-of-vectors.html#cb77-2" aria-hidden="true" tabindex="-1"></a>s_y <span class="ot">=</span> l_y <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">10</span>)</span>
<span id="cb77-3"><a href="statistical-appplication-of-vectors.html#cb77-3" aria-hidden="true" tabindex="-1"></a>s_y</span></code></pre></div>
<pre><code>[1] 0.6356099</code></pre>
<p>Note that if we are using <span class="math inline">\(s_\mathbf{x}\)</span> as an estimate for the population parameter <span class="math inline">\(\sigma_\mathbf{x}\)</span>, that is <strong>x</strong> is a sample of student scores, then we need to divide the length of vector <strong>x</strong> by <span class="math inline">\(n-1\)</span> rather than <span class="math inline">\(n\)</span>. In that case,</p>
<p><span class="math display">\[
\begin{split}
\hat\sigma_\mathbf{x} &amp;= \frac{371.28}{\sqrt{9}} = 123.76 \\[2em]
\hat\sigma_\mathbf{y} &amp;= \frac{4.04}{\sqrt{9}} = 0.67
\end{split}
\]</span></p>
<p>This is the denominator that is used in the <code>sd()</code> function in R.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="statistical-appplication-of-vectors.html#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute sd of x</span></span>
<span id="cb79-2"><a href="statistical-appplication-of-vectors.html#cb79-2" aria-hidden="true" tabindex="-1"></a>sigma_x <span class="ot">=</span> l_x <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">9</span>)</span>
<span id="cb79-3"><a href="statistical-appplication-of-vectors.html#cb79-3" aria-hidden="true" tabindex="-1"></a>sigma_x</span></code></pre></div>
<pre><code>[1] 123.7605</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="statistical-appplication-of-vectors.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute sd of y</span></span>
<span id="cb81-2"><a href="statistical-appplication-of-vectors.html#cb81-2" aria-hidden="true" tabindex="-1"></a>sigma_y <span class="ot">=</span> l_y <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">9</span>)</span>
<span id="cb81-3"><a href="statistical-appplication-of-vectors.html#cb81-3" aria-hidden="true" tabindex="-1"></a>sigma_y</span></code></pre></div>
<pre><code>[1] 0.6699917</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="statistical-appplication-of-vectors.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check results</span></span>
<span id="cb83-2"><a href="statistical-appplication-of-vectors.html#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(x)</span></code></pre></div>
<pre><code>[1] 123.7605</code></pre>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="statistical-appplication-of-vectors.html#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(y)</span></code></pre></div>
<pre><code>[1] 0.6699917</code></pre>
<p><br /></p>
</div>
<div id="vector-correlation-and-separation" class="section level2">
<h2>Vector Correlation and Separation</h2>
<p>The correlation between two vectors can also be expressed in terms of deviation scores:</p>
<p><span class="math display">\[
\begin{split}
r_{xy} &amp;= \frac{\mathrm{Cov}_{\mathbf{xy}}}{s_\mathbf{x} s_\mathbf{y}} \\[2em]
&amp;= \frac{\frac{\sum (\mathbf{x}-\bar{x})(\mathbf{y}-\bar{y})}{n}}{s_\mathbf{x} s_\mathbf{y}}
\end{split}
\]</span></p>
<p>The expression <span class="math inline">\(\sum (x-\bar{x})(y-\bar{y})\)</span> is equivalent to <span class="math inline">\(\mathbf{x}\bullet\mathbf{y}\)</span>. Furthermore, we can re-write the standard deviations using our previous relationship with length of the deviation vectors. This implies,</p>
<p><span class="math display">\[
\begin{split}
r_{xy} &amp;= \frac{\frac{\mathbf{d_x}\bullet\mathbf{d_y}}{n}}{\frac{\lvert\lvert\mathbf{d_x}\rvert\rvert}{\sqrt{n}}\frac{\lvert\lvert\mathbf{d_y}\rvert\rvert}{\sqrt{n}}} \\[2em]
&amp;= \frac{\frac{\mathbf{d_x}\bullet\mathbf{d_y}}{n}}{\frac{\lvert\lvert\mathbf{d_x}\rvert\rvert~\lvert\lvert\mathbf{d_y}\rvert\rvert}{n}} \\[2em]
&amp;= \frac{\mathbf{d_x}\bullet\mathbf{d_y}}{\lvert\lvert\mathbf{d_x}\rvert\rvert~\lvert\lvert\mathbf{d_y}\rvert\rvert}
\end{split}
\]</span>
That is, the correlation between <strong>x</strong> and <strong>y</strong> is equal to the ratio between the dot product of the deviation vectors and the product of their lengths.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> In our example,</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="statistical-appplication-of-vectors.html#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute correlation</span></span>
<span id="cb87-2"><a href="statistical-appplication-of-vectors.html#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(d_x <span class="sc">*</span> d_y) <span class="sc">/</span> (l_x <span class="sc">*</span> l_y)</span></code></pre></div>
<pre><code>[1] 0.8468822</code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="statistical-appplication-of-vectors.html#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check with correlation function</span></span>
<span id="cb89-2"><a href="statistical-appplication-of-vectors.html#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(x, y)</span></code></pre></div>
<pre><code>          [,1]
[1,] 0.8468822</code></pre>
<p>Recall that the angle between two vectors <strong>a</strong> and <strong>b</strong>, denoted <span class="math inline">\(\theta_{\mathbf{ab}}\)</span>, is given by,</p>
<p><span class="math display">\[
\cos (\theta_{\mathbf{ab}}) = \frac{\mathbf{a}\bullet\mathbf{b}}{\lvert\lvert\mathbf{a}\rvert\rvert\times\lvert\lvert\mathbf{b}\rvert\rvert}
\]</span></p>
<p>Thus to compute the angle between the two deviation vectors (<span class="math inline">\(\theta\)</span>), this is:</p>
<p><span class="math display">\[
\cos (\theta) = \frac{\mathbf{d_x}\bullet\mathbf{d_y}}{\lvert\lvert\mathbf{d_x}\rvert\rvert\times\lvert\lvert\mathbf{d_y}\rvert\rvert}
\]</span></p>
<p>This is the same as the formula for the correlation! In other words,</p>
<p><span class="math display">\[
r_{\mathbf{xy}} = \cos (\theta)
\]</span></p>
<p>The correlation between two vectors <strong>x</strong> and <strong>y</strong> is equal to the cosine of the angle between their deviation vectors. Using this equality, we can find the angle between the deviation vectors for our example SAT scores and GPA values. Since <span class="math inline">\(r_{\mathbf{xy}}=0.847\)</span>, that implies <span class="math inline">\(\theta\approx32^\circ\)</span>.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="statistical-appplication-of-vectors.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute theta (in radians)</span></span>
<span id="cb91-2"><a href="statistical-appplication-of-vectors.html#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="fu">acos</span>(<span class="fl">0.847</span>)</span></code></pre></div>
<pre><code>[1] 0.5604801</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="statistical-appplication-of-vectors.html#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute theta (in degrees)</span></span>
<span id="cb93-2"><a href="statistical-appplication-of-vectors.html#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="fu">acos</span>(<span class="fl">0.847</span>) <span class="sc">*</span> <span class="dv">180</span> <span class="sc">/</span> pi</span></code></pre></div>
<pre><code>[1] 32.11314</code></pre>
<p>This correlation corresponds to an angle of approximately <span class="math inline">\(32^\circ\)</span> of separation between the deviation score vectors for SAT scores and GPA values in the 10-dimensional space defined by our 10 students.</p>
<div class="fyi">
<p>We can use the fact that <span class="math inline">\(r_{\mathbf{xy}} = \cos (\theta)\)</span> to make some connections between the value of the correlation coefficient and the angle between the deviation vectors.</p>
<ul>
<li>Perfect positive correlation (<span class="math inline">\(r_{\mathbf{xy}}=1\)</span>) indicates that deviation vectors are collinear. In this case <span class="math inline">\(\cos(\theta)=1\)</span> which implies that <span class="math inline">\(\theta=0^\circ\)</span>.</li>
<li>Perfect negative correlation (<span class="math inline">\(r_{\mathbf{xy}}=-1\)</span>) indicates that deviation vectors are in opposite directions. In this case <span class="math inline">\(\cos(\theta)=-1\)</span> which implies that <span class="math inline">\(\theta=180^\circ\)</span>.</li>
<li>Perfect lack of correlation (<span class="math inline">\(r_{\mathbf{xy}}=0\)</span>) indicates that deviation vectors are orthogonal. In this case <span class="math inline">\(\cos(\theta)=0\)</span> which implies that <span class="math inline">\(\theta=90^\circ\)</span>.</li>
</ul>
</div>
<p><br /></p>
</div>
<div id="orthogonal-decomposition-and-bivariate-regression" class="section level2">
<h2>Orthogonal Decomposition and Bivariate Regression</h2>
<p>Recall that the simple regression model seeks to explain variation in an outcome variable <em>Y</em> using a predictor variable <em>X</em>. Mathematically, the model fitted is expressed as:</p>
<p><span class="math display">\[
\mathbf{y} = b_0 + b_1(\mathbf{x}) + \mathbf{e}
\]</span></p>
<p>where <strong>y</strong> is a vector of fitted values, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are scalars produced from the OLS estimation, <strong>x</strong> is a vector of the predictor values, and <strong>e</strong> is a vector of residuals. Recall that when we have mean centered the outcome and predictor, the intercept (<span class="math inline">\(b_0\)</span>) drops out of this equation. we can express this as:</p>
<p><span class="math display">\[
\mathbf{y} - \bar{y} = b_1(\mathbf{x} - \bar{x}) + \mathbf{e}
\]</span></p>
<p>The outcome and predictors are now expressed as deviation vectors, say</p>
<p><span class="math display">\[
\mathbf{d}_\mathbf{y} = b_1(\mathbf{d}_\mathbf{x} ) + \mathbf{e}
\]</span>
where <span class="math inline">\(\hat{\mathbf{d}}_\mathbf{y}=b_1(\mathbf{d}_\mathbf{x} )\)</span>. That is, we can partition the deviation vector of <strong>y</strong> into two components:</p>
<p><span class="math display">\[
\mathbf{d}_\mathbf{y} = \hat{\mathbf{d}}_\mathbf{y} + \mathbf{e}
\]</span></p>
<p>Ordinary least squares (OLS) estimation determines the value of <span class="math inline">\(b_1\)</span> by minimizing the sum of squared residuals, that is, it minimizes the quantity <span class="math inline">\(\lvert\lvert\mathbf{e}\rvert\rvert^2\)</span>. Geometrically, minimizing <span class="math inline">\(\lvert\lvert\mathbf{e}\rvert\rvert^2\)</span> is determining the orthogonal projection of <span class="math inline">\(\mathbf{d}_\mathbf{y}\)</span> onto <span class="math inline">\(\mathbf{d}_\mathbf{x}\)</span>.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> This projection is the first component of the partitioning of the deviation vector described previously, <span class="math inline">\(\hat{\mathbf{d}}_\mathbf{y}=b_1(\mathbf{d}_\mathbf{x})\)</span>, which is collinear with <span class="math inline">\(\mathbf{d}_\mathbf{x}\)</span> since <span class="math inline">\(b_1(\mathbf{d}_\mathbf{x})\)</span> is a scalar multiple of <span class="math inline">\(\mathbf{d}_\mathbf{x}\)</span> This is shown in Figure <a href="statistical-appplication-of-vectors.html#fig:fig06-01">19</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:fig06-01"></span>
<img src="figs/fig-06-01.png" alt="The two orthogonal projections from the deviation vector of **y** form the basis for the model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the **e** vector." width="60%" />
<p class="caption">
Figure 19: The two orthogonal projections from the deviation vector of <strong>y</strong> form the basis for the model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the <strong>e</strong> vector.
</p>
</div>
<p>The ‘triangle’ formed by the vectors <span class="math inline">\(\hat{\mathbf{d}}_\mathbf{y}\)</span>, <strong>e</strong>, and <span class="math inline">\(\mathbf{d}_\mathbf{y}\)</span> is referred to as the <em>model triangle</em>. Figure <a href="statistical-appplication-of-vectors.html#fig:model-triangle">20</a> shows the model triangle. (Remember <strong>e</strong> can be moved to the right-side of the triangle since location is not a vector property.)</p>
<div class="figure" style="text-align: center"><span id="fig:model-triangle"></span>
<img src="figs/fig-06-model-triangle.png" alt="The model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the **e** vector." width="60%" />
<p class="caption">
Figure 20: The model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the <strong>e</strong> vector.
</p>
</div>
<p>The geometry of this triangle is the same as the geometry visualizing the sum of these vectors, namely</p>
<p><span class="math display">\[
\mathbf{d}_\mathbf{y} = \hat{\mathbf{d}}_\mathbf{y} + \mathbf{e}
\]</span></p>
<p>Namely that the vectors that create the legs of the model triangle can be added together to create the <span class="math inline">\(\mathbf{d}_\mathbf{y}\)</span> hypotenuse vector. The <strong>e</strong> vector is also the second orthogonal projection vector, <span class="math inline">\(\mathbf{e} = \mathbf{p}_{\mathbf{d}_\mathbf{y}\perp\mathbf{o}}\)</span>. By definition, this means that the vector of residuals (<strong>e</strong>) is orthogonal to the vector of fitted values (<span class="math inline">\(\hat{\mathbf{d}}_\mathbf{y}\)</span>), which means, the correlation between those two vectors is zero.</p>
<p><span class="math display">\[
r_{\mathbf{e},\hat{\mathbf{d}}_\mathbf{y}} = 0
\]</span></p>
<p>It also means that the model triangle is a right triangle, whose side lengths are governed by the Pythagorean Theorem.</p>
<p><span class="math display">\[
\lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert^2 = \lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert^2 + \lvert\lvert\mathbf{e}\rvert\rvert^2
\]</span></p>
<p>Expressing these lengths using the deviations we get</p>
<p><span class="math display">\[
\sum_{i=1}^n (y_i - \bar{y})^2 = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^n (y_i - \hat{y}_i)^2
\]</span></p>
<p>This is the partitioning that describes the ANOVA decomposition:</p>
<p><span class="math display">\[
\mathrm{Total~SS} = \mathrm{Model~SS} + \mathrm{Residual~SS}
\]</span></p>
<p>Each of these sum of squares is the squared length of one of the vectors in the model triangle.</p>
<p><span class="math display">\[
\begin{split}
\mathrm{Total~SS} &amp;= \lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert^2 \\[2em]
\mathrm{Model~SS} &amp;= \lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert^2 \\[2em]
\mathrm{Residual~SS} &amp;= \lvert\lvert\mathbf{e}\rvert\rvert^2
\end{split}
\]</span></p>
<p>Relatedly, the lengths of the vectors making up the model triangle are the square roots of the these sum of square terms:</p>
<p><span class="math display">\[
\begin{split}
\lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert &amp;= \sqrt{\mathrm{Total~SS}} \\[2em]
\lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert &amp;= \sqrt{\mathrm{Model~SS}} \\[2em]
\lvert\lvert\mathbf{e}\rvert\rvert &amp;= \sqrt{\mathrm{Residual~SS}}
\end{split}
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-25"></span>
<img src="figs/fig-06-02.png" alt="The side lengths of the model triangle (yellow) correspond to the square roots of the sum of square terms used in the ANOVA decomposition." width="60%" />
<p class="caption">
Figure 21: The side lengths of the model triangle (yellow) correspond to the square roots of the sum of square terms used in the ANOVA decomposition.
</p>
</div>
<p>Lastly, since the model-level <span class="math inline">\(R^2\)</span> value is defined as <span class="math inline">\(R^2 = \frac{\mathrm{Model~SS}}{\mathrm{Total~SS}}\)</span> and there is only a single predictor in the model then,</p>
<p><span class="math display">\[
\begin{split}
r_{\mathbf{xy}} &amp;= \sqrt{R^2} \\[2em]
&amp;= \sqrt{\frac{\mathrm{Model~SS}}{\mathrm{Total~SS}}} \\[2em]
&amp;= \frac{\sqrt{\mathrm{Model~SS}}}{\sqrt{\mathrm{Total~SS}}} \\[2em]
&amp;= \frac{\lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert}{\lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert}
\end{split}
\]</span></p>
<p>That is, the correlation coefficient between <strong>x</strong> and <strong>y</strong> is equivalent to the ratio of the lengths between the orthogonal projection vector collinear with the deviation vector of <strong>x</strong> and the deviation vector of <strong>y</strong>. Note this is also exactly how we compute cosine of <span class="math inline">\(\theta\)</span>.</p>
<p><br /></p>
<div id="back-to-the-sat-and-gpa-example" class="section level3">
<h3>Back to the SAT and GPA Example</h3>
<p>Here we return to our GPA and SAT data to provide an example of these computations. Suppose we want to predict SAT (<strong>y</strong>) from GPA (<strong>x</strong>), employing deviation scores. We begin by computing the length of both deviation vectors:</p>
<p><span class="math display">\[
\begin{split}
\lvert\lvert\mathbf{d}_\mathbf{x}\rvert\rvert &amp;= \sqrt{137850} = 371.28\\[2em]
\lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert &amp;= \sqrt{4.04} = 2.01
\end{split}
\]</span></p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="statistical-appplication-of-vectors.html#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute length of deviation vector d_x</span></span>
<span id="cb95-2"><a href="statistical-appplication-of-vectors.html#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(l_x <span class="sc">*</span> l_x))</span></code></pre></div>
<pre><code>[1] 371.2816</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="statistical-appplication-of-vectors.html#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute length of deviation vector d_y</span></span>
<span id="cb97-2"><a href="statistical-appplication-of-vectors.html#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(l_y <span class="sc">*</span> l_y))</span></code></pre></div>
<pre><code>[1] 2.009975</code></pre>
<p>We can also compute the correlation coefficient between SAT scores and GPAs by finding the cosine of the angle between the two vectors, <span class="math inline">\(r = 0.847\)</span>. Taking the arc-cosine, we find that <span class="math inline">\(\theta = 32.12^\circ\)</span>.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="statistical-appplication-of-vectors.html#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute correlation</span></span>
<span id="cb99-2"><a href="statistical-appplication-of-vectors.html#cb99-2" aria-hidden="true" tabindex="-1"></a>r <span class="ot">=</span> <span class="fu">sum</span>(d_x <span class="sc">*</span> d_y) <span class="sc">/</span> (l_x <span class="sc">*</span> l_y)</span>
<span id="cb99-3"><a href="statistical-appplication-of-vectors.html#cb99-3" aria-hidden="true" tabindex="-1"></a>r</span></code></pre></div>
<pre><code>[1] 0.8468822</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-28"></span>
<img src="figs/fig-06-02.png" alt="The model triangle (yellow) for our regression of GPA (**y**) onto SAT scores (**x**)." width="60%" />
<p class="caption">
Figure 22: The model triangle (yellow) for our regression of GPA (<strong>y</strong>) onto SAT scores (<strong>x</strong>).
</p>
</div>
<p>We can now use the definition of cosine and sine to compute the lengths of the two orthogonal projections from <span class="math inline">\(\mathbf{d}_\mathbf{y}\)</span>. The length of the projection onto <span class="math inline">\(\mathbf{d}_\mathbf{x}\)</span> is calculated as:</p>
<p><span class="math display">\[
\begin{split}
\cos(\theta) &amp;= \frac{\lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert}{\lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert} \\[2em]
0.847 &amp;= \frac{\lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert}{2.01} \\[2em]
\lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert &amp;= 1.70
\end{split}
\]</span></p>
<p>The length of the second projection onto <strong>o</strong> is calculated as:</p>
<p><span class="math display">\[
\begin{split}
\sin(\theta) &amp;= \frac{\lvert\lvert\mathbf{e}\rvert\rvert}{\lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert} \\[2em]
0.531 &amp;= \frac{\lvert\lvert\mathbf{e}\rvert\rvert}{2.01} \\[2em]
\lvert\lvert\mathbf{e}\rvert\rvert &amp;= 1.07
\end{split}
\]</span></p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="statistical-appplication-of-vectors.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute length of projection on d_x</span></span>
<span id="cb101-2"><a href="statistical-appplication-of-vectors.html#cb101-2" aria-hidden="true" tabindex="-1"></a>r <span class="sc">*</span> l_y</span></code></pre></div>
<pre><code>[1] 1.702212</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="statistical-appplication-of-vectors.html#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute length of projection on o</span></span>
<span id="cb103-2"><a href="statistical-appplication-of-vectors.html#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sin</span>(<span class="fu">acos</span>(r)) <span class="sc">*</span> l_y</span></code></pre></div>
<pre><code>[1] 1.068866</code></pre>
<p>Figure <a href="statistical-appplication-of-vectors.html#fig:fig06-04">23</a> shows the model triangle for the regression of GPA (<strong>y</strong>) onto SAT scores (<strong>x</strong>) with all of the computed side lengths.</p>
<div class="figure" style="text-align: center"><span id="fig:fig06-04"></span>
<img src="figs/fig-06-04.png" alt="The model triangle (yellow) for our regression of GPA (**y**) onto SAT scores (**x**)." width="60%" />
<p class="caption">
Figure 23: The model triangle (yellow) for our regression of GPA (<strong>y</strong>) onto SAT scores (<strong>x</strong>).
</p>
</div>
<p>Finally, we can use these lengths, along with the length of <span class="math inline">\(\mathbf{d}_\mathbf{y}\)</span> to write out the ANOVA decomposition; the partitioning of the sum of squares.</p>
<p><span class="math display">\[
\begin{split}
\mathrm{Total~SS} &amp;= \lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert^2 = 2.01^2 = 4.04\\[2em]
\mathrm{Model~SS} &amp;= \lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert^2 = 1.70^2 = 2.90\\[2em]
\mathrm{Residual~SS} &amp;= \lvert\lvert\mathbf{e}\rvert\rvert^2 = 1.07^2 = 1.14
\end{split}
\]</span></p>
<p>These sums of squares are additive within rounding:</p>
<p><span class="math display">\[
\begin{split}
\mathrm{Total~SS} &amp;= \mathrm{Model~SS} + \mathrm{Residual~SS} \\[2em]
4.04 &amp;= 2.90 + 1.14
\end{split}
\]</span></p>
<p>Using these values we can also compute the model-level <span class="math inline">\(R^2\)</span>.</p>
<p><span class="math display">\[
R^2 = \frac{\mathrm{Model~SS}}{\mathrm{Total~SS}} =  \frac{2.90}{4.04} = 0.718
\]</span></p>
<p>That is, differences in SAT scores explain 71.8% of the variation in GPAs.</p>
<p><br /></p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>Most formulas for the correlation will use <span class="math inline">\(n-1\)</span> rather than <span class="math inline">\(n\)</span>, but here it doesn’t matter as the <span class="math inline">\(n\)</span>s drop out when we reduce this.<a href="statistical-appplication-of-vectors.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>Remember from geometry that the shortest distance from a point (at the end of the <span class="math inline">\(\mathbf{d}_\mathbf{y}\)</span>) to a line (spanned by <span class="math inline">\(\mathbf{d}_\mathbf{x}\)</span>) is the perpendicular line segment between them.<a href="statistical-appplication-of-vectors.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="vector-geometry-angles-projection-and-decomposition.html"><button class="btn btn-default">Previous</button></a>
<a href="matrices-1.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>

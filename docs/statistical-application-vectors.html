<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Statistical Application: Vectors | Matrix Algebra for Educational Scientists</title>
  <meta name="description" content="Matrix algebra content for QME students" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Statistical Application: Vectors | Matrix Algebra for Educational Scientists" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Matrix algebra content for QME students" />
  <meta name="github-repo" content="zief0002/matrix-algebra" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Statistical Application: Vectors | Matrix Algebra for Educational Scientists" />
  
  <meta name="twitter:description" content="Matrix algebra content for QME students" />
  

<meta name="author" content="Michael Rodriguez &amp; Andrew Zieffler" />


<meta name="date" content="2022-08-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="vector-geometry-angles-projection-and-decomposition.html"/>
<link rel="next" href="matrices-1.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/kePrint/kePrint.js"></script>
<link href="libs/lightable/lightable.css" rel="stylesheet" />

<script type="text/javascript">
<!--
    function toggle_visibility(id) {
       var e = document.getElementById(id);
       if(e.style.display == 'block')
          e.style.display = 'none';
       else
          e.style.display = 'block';
    }
//-->
</script>




<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style/style-gitbook.css" type="text/css" />
<link rel="stylesheet" href="style/table-styles.css" type="text/css" />
<link rel="stylesheet" href="style/syntax.css" type="text/css" />
<link rel="stylesheet" href="style/notes.css" type="text/css" />
<link rel="stylesheet" href="style/toc.css" type="text/css" />
<link rel="stylesheet" href="style/buttons.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Foreword</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#colophon"><i class="fa fa-check"></i>Colophon</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="datastructures.html"><a href="datastructures.html"><i class="fa fa-check"></i><b>2</b> Data Structures</a>
<ul>
<li class="chapter" data-level="2.1" data-path="datastructures.html"><a href="datastructures.html#scalars"><i class="fa fa-check"></i><b>2.1</b> Scalars</a></li>
<li class="chapter" data-level="2.2" data-path="datastructures.html"><a href="datastructures.html#vectors"><i class="fa fa-check"></i><b>2.2</b> Vectors</a></li>
<li class="chapter" data-level="2.3" data-path="datastructures.html"><a href="datastructures.html#matrices"><i class="fa fa-check"></i><b>2.3</b> Matrices</a></li>
<li class="chapter" data-level="2.4" data-path="datastructures.html"><a href="datastructures.html#tensors"><i class="fa fa-check"></i><b>2.4</b> Tensors</a></li>
<li class="chapter" data-level="2.5" data-path="datastructures.html"><a href="datastructures.html#a-word-about-notation"><i class="fa fa-check"></i><b>2.5</b> A Word about Notation</a></li>
<li class="chapter" data-level="2.6" data-path="datastructures.html"><a href="datastructures.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>Vectors and Vector Operations</b></span></li>
<li class="chapter" data-level="3" data-path="vec.html"><a href="vec.html"><i class="fa fa-check"></i><b>3</b> Vectors</a>
<ul>
<li class="chapter" data-level="3.1" data-path="vec.html"><a href="vec.html#what-is-a-vector"><i class="fa fa-check"></i><b>3.1</b> What is a Vector?</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="vec.html"><a href="vec.html#transposition"><i class="fa fa-check"></i><b>3.1.1</b> Transposition</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="vec.html"><a href="vec.html#general-form-of-a-vector"><i class="fa fa-check"></i><b>3.2</b> General Form of a Vector</a></li>
<li class="chapter" data-level="3.3" data-path="vec.html"><a href="vec.html#the-geometry-of-vectors"><i class="fa fa-check"></i><b>3.3</b> The Geometry of Vectors</a></li>
<li class="chapter" data-level="3.4" data-path="vec.html"><a href="vec.html#vector-properties"><i class="fa fa-check"></i><b>3.4</b> Vector Properties</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="vec.html"><a href="vec.html#vector-length"><i class="fa fa-check"></i><b>3.4.1</b> Vector Length</a></li>
<li class="chapter" data-level="3.4.2" data-path="vec.html"><a href="vec.html#vector-direction"><i class="fa fa-check"></i><b>3.4.2</b> Vector Direction</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="vec.html"><a href="vec.html#vector-equality"><i class="fa fa-check"></i><b>3.5</b> Vector Equality</a></li>
<li class="chapter" data-level="3.6" data-path="vec.html"><a href="vec.html#special-vectors"><i class="fa fa-check"></i><b>3.6</b> Special Vectors</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="vec.html"><a href="vec.html#zero-vector"><i class="fa fa-check"></i><b>3.6.1</b> Zero Vector</a></li>
<li class="chapter" data-level="3.6.2" data-path="vec.html"><a href="vec.html#ones-vector"><i class="fa fa-check"></i><b>3.6.2</b> Ones Vector</a></li>
<li class="chapter" data-level="3.6.3" data-path="vec.html"><a href="vec.html#unit-vector"><i class="fa fa-check"></i><b>3.6.3</b> Unit Vector</a></li>
<li class="chapter" data-level="3.6.4" data-path="vec.html"><a href="vec.html#elementary-vectors"><i class="fa fa-check"></i><b>3.6.4</b> Elementary Vectors</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="vec.html"><a href="vec.html#exercises-1"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="vecop.html"><a href="vecop.html"><i class="fa fa-check"></i><b>4</b> Vector Operations</a>
<ul>
<li class="chapter" data-level="4.1" data-path="vecop.html"><a href="vecop.html#vector-addition-and-subtraction"><i class="fa fa-check"></i><b>4.1</b> Vector Addition and Subtraction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="vecop.html"><a href="vecop.html#geometry-of-adding-vectors"><i class="fa fa-check"></i><b>4.1.1</b> Geometry of Adding Vectors</a></li>
<li class="chapter" data-level="4.1.2" data-path="vecop.html"><a href="vecop.html#vector-subtraction"><i class="fa fa-check"></i><b>4.1.2</b> Vector Subtraction</a></li>
<li class="chapter" data-level="4.1.3" data-path="vecop.html"><a href="vecop.html#properties-of-vector-addition-and-subtraction"><i class="fa fa-check"></i><b>4.1.3</b> Properties of Vector Addition and Subtraction</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="vecop.html"><a href="vecop.html#vectorscalar-multiplication"><i class="fa fa-check"></i><b>4.2</b> Vector–Scalar Multiplication</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="vecop.html"><a href="vecop.html#division-by-a-scalar"><i class="fa fa-check"></i><b>4.2.1</b> Division by a Scalar</a></li>
<li class="chapter" data-level="4.2.2" data-path="vecop.html"><a href="vecop.html#geometry-of-vectorscalar-multiplication"><i class="fa fa-check"></i><b>4.2.2</b> Geometry of Vector–Scalar Multiplication</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="vecop.html"><a href="vecop.html#vectorvector-multiplication-dot-product"><i class="fa fa-check"></i><b>4.3</b> Vector–Vector Multiplication: Dot Product</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="vecop.html"><a href="vecop.html#re-visiting-vector-length"><i class="fa fa-check"></i><b>4.3.1</b> Re-visiting Vector Length</a></li>
<li class="chapter" data-level="4.3.2" data-path="vecop.html"><a href="vecop.html#dot-products-using-the-special-vectors"><i class="fa fa-check"></i><b>4.3.2</b> Dot Products Using the Special Vectors</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="vecop.html"><a href="vecop.html#vector-operations-using-r"><i class="fa fa-check"></i><b>4.4</b> Vector Operations Using R</a></li>
<li class="chapter" data-level="4.5" data-path="vecop.html"><a href="vecop.html#exercises-2"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="vector-geometry-angles-projection-and-decomposition.html"><a href="vector-geometry-angles-projection-and-decomposition.html"><i class="fa fa-check"></i><b>5</b> Vector Geometry: Angles, Projection, and Decomposition</a>
<ul>
<li class="chapter" data-level="5.1" data-path="vector-geometry-angles-projection-and-decomposition.html"><a href="vector-geometry-angles-projection-and-decomposition.html#angle-between-vectors"><i class="fa fa-check"></i><b>5.1</b> Angle Between Vectors</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="vector-geometry-angles-projection-and-decomposition.html"><a href="vector-geometry-angles-projection-and-decomposition.html#orthogonal-vectors"><i class="fa fa-check"></i><b>5.1.1</b> Orthogonal Vectors</a></li>
<li class="chapter" data-level="5.1.2" data-path="vector-geometry-angles-projection-and-decomposition.html"><a href="vector-geometry-angles-projection-and-decomposition.html#collinear-vectors"><i class="fa fa-check"></i><b>5.1.2</b> Collinear Vectors</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="vector-geometry-angles-projection-and-decomposition.html"><a href="vector-geometry-angles-projection-and-decomposition.html#orthogonal-projection"><i class="fa fa-check"></i><b>5.2</b> Orthogonal Projection</a></li>
<li class="chapter" data-level="5.3" data-path="vector-geometry-angles-projection-and-decomposition.html"><a href="vector-geometry-angles-projection-and-decomposition.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>5.3</b> Orthogonal Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistical-application-vectors.html"><a href="statistical-application-vectors.html"><i class="fa fa-check"></i><b>6</b> Statistical Application: Vectors</a>
<ul>
<li class="chapter" data-level="6.1" data-path="statistical-application-vectors.html"><a href="statistical-application-vectors.html#deviation-scores-and-the-standard-deviation"><i class="fa fa-check"></i><b>6.1</b> Deviation Scores and the Standard Deviation</a></li>
<li class="chapter" data-level="6.2" data-path="statistical-application-vectors.html"><a href="statistical-application-vectors.html#vector-correlation-and-separation"><i class="fa fa-check"></i><b>6.2</b> Vector Correlation and Separation</a></li>
<li class="chapter" data-level="6.3" data-path="statistical-application-vectors.html"><a href="statistical-application-vectors.html#orthogonal-decomposition-and-bivariate-regression"><i class="fa fa-check"></i><b>6.3</b> Orthogonal Decomposition and Bivariate Regression</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="statistical-application-vectors.html"><a href="statistical-application-vectors.html#back-to-the-sat-and-gpa-example"><i class="fa fa-check"></i><b>6.3.1</b> Back to the SAT and GPA Example</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Matrices and Matrix Operations</b></span></li>
<li class="chapter" data-level="7" data-path="matrices-1.html"><a href="matrices-1.html"><i class="fa fa-check"></i><b>7</b> Matrices</a>
<ul>
<li class="chapter" data-level="7.0.1" data-path="matrices-1.html"><a href="matrices-1.html#dimensions-of-a-matrix"><i class="fa fa-check"></i><b>7.0.1</b> Dimensions of a Matrix</a></li>
<li class="chapter" data-level="7.0.2" data-path="matrices-1.html"><a href="matrices-1.html#indexing-individual-elements"><i class="fa fa-check"></i><b>7.0.2</b> Indexing Individual Elements</a></li>
<li class="chapter" data-level="7.0.3" data-path="matrices-1.html"><a href="matrices-1.html#geometry-of-matrices"><i class="fa fa-check"></i><b>7.0.3</b> Geometry of Matrices</a></li>
<li class="chapter" data-level="7.0.4" data-path="matrices-1.html"><a href="matrices-1.html#matrices-in-r"><i class="fa fa-check"></i><b>7.0.4</b> Matrices in R</a></li>
<li class="chapter" data-level="7.1" data-path="matrices-1.html"><a href="matrices-1.html#matrix-equality"><i class="fa fa-check"></i><b>7.1</b> Matrix Equality</a></li>
<li class="chapter" data-level="7.2" data-path="matrices-1.html"><a href="matrices-1.html#exercises-3"><i class="fa fa-check"></i><b>7.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="matrix-addition-and-subtraction.html"><a href="matrix-addition-and-subtraction.html"><i class="fa fa-check"></i><b>8</b> Matrix Addition and Subtraction</a>
<ul>
<li class="chapter" data-level="8.1" data-path="matrix-addition-and-subtraction.html"><a href="matrix-addition-and-subtraction.html#properties-of-matrix-addition"><i class="fa fa-check"></i><b>8.1</b> Properties of Matrix Addition</a></li>
<li class="chapter" data-level="8.2" data-path="matrix-addition-and-subtraction.html"><a href="matrix-addition-and-subtraction.html#statistical-application-deviation-matrix"><i class="fa fa-check"></i><b>8.2</b> Statistical Application: Deviation Matrix</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="matrix-multiplication.html"><a href="matrix-multiplication.html"><i class="fa fa-check"></i><b>9</b> Matrix Multiplication</a>
<ul>
<li class="chapter" data-level="9.1" data-path="matrix-multiplication.html"><a href="matrix-multiplication.html#scalarmatrix-multiplication"><i class="fa fa-check"></i><b>9.1</b> Scalar–Matrix Multiplication</a></li>
<li class="chapter" data-level="9.2" data-path="matrix-multiplication.html"><a href="matrix-multiplication.html#matrix-multiplication-1"><i class="fa fa-check"></i><b>9.2</b> Matrix Multiplication</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="matrix-multiplication.html"><a href="matrix-multiplication.html#conformability"><i class="fa fa-check"></i><b>9.2.1</b> Conformability</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="matrix-multiplication.html"><a href="matrix-multiplication.html#properties-of-matrix-multiplication"><i class="fa fa-check"></i><b>9.3</b> Properties of Matrix Multiplication</a></li>
<li class="chapter" data-level="9.4" data-path="matrix-multiplication.html"><a href="matrix-multiplication.html#revisiting-dot-products"><i class="fa fa-check"></i><b>9.4</b> Revisiting Dot Products</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="matrix-transposition.html"><a href="matrix-transposition.html"><i class="fa fa-check"></i><b>10</b> Matrix Transposition</a>
<ul>
<li class="chapter" data-level="10.1" data-path="matrix-transposition.html"><a href="matrix-transposition.html#exercises-4"><i class="fa fa-check"></i><b>10.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="exercises-matrix-operations.html"><a href="exercises-matrix-operations.html"><i class="fa fa-check"></i><b>11</b> Exercises: Matrix Operations</a></li>
<li class="part"><span><b>Square Matrices and Their Properties</b></span></li>
<li class="chapter" data-level="12" data-path="square-matrices-and-friends.html"><a href="square-matrices-and-friends.html"><i class="fa fa-check"></i><b>12</b> Square Matrices and Friends</a>
<ul>
<li class="chapter" data-level="12.1" data-path="square-matrices-and-friends.html"><a href="square-matrices-and-friends.html#square-matrices"><i class="fa fa-check"></i><b>12.1</b> Square Matrices</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="square-matrices-and-friends.html"><a href="square-matrices-and-friends.html#main-diagonal"><i class="fa fa-check"></i><b>12.1.1</b> Main Diagonal</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="square-matrices-and-friends.html"><a href="square-matrices-and-friends.html#diagonal-matrices"><i class="fa fa-check"></i><b>12.2</b> Diagonal Matrices</a></li>
<li class="chapter" data-level="12.3" data-path="square-matrices-and-friends.html"><a href="square-matrices-and-friends.html#scalar-matrices"><i class="fa fa-check"></i><b>12.3</b> Scalar Matrices</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="square-matrices-and-friends.html"><a href="square-matrices-and-friends.html#identity-matrices"><i class="fa fa-check"></i><b>12.3.1</b> Identity Matrices</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="square-matrices-and-friends.html"><a href="square-matrices-and-friends.html#symmetric-matrices"><i class="fa fa-check"></i><b>12.4</b> Symmetric Matrices</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="square-matrices-and-friends.html"><a href="square-matrices-and-friends.html#skew-symmetric-matrices"><i class="fa fa-check"></i><b>12.4.1</b> Skew Symmetric Matrices</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="square-matrices-and-friends.html"><a href="square-matrices-and-friends.html#triangular-matrices"><i class="fa fa-check"></i><b>12.5</b> Triangular Matrices</a></li>
<li class="chapter" data-level="12.6" data-path="square-matrices-and-friends.html"><a href="square-matrices-and-friends.html#exercises-5"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="properties-of-square-matrices.html"><a href="properties-of-square-matrices.html"><i class="fa fa-check"></i><b>13</b> Properties of Square Matrices</a>
<ul>
<li class="chapter" data-level="13.1" data-path="properties-of-square-matrices.html"><a href="properties-of-square-matrices.html#matrix-trace"><i class="fa fa-check"></i><b>13.1</b> Matrix Trace</a></li>
<li class="chapter" data-level="13.2" data-path="properties-of-square-matrices.html"><a href="properties-of-square-matrices.html#determinant"><i class="fa fa-check"></i><b>13.2</b> Determinant</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="properties-of-square-matrices.html"><a href="properties-of-square-matrices.html#determinant-for-a-1-times-1-matrix."><i class="fa fa-check"></i><b>13.2.1</b> Determinant for a <span class="math inline">\(1 \times 1\)</span> matrix.</a></li>
<li class="chapter" data-level="13.2.2" data-path="properties-of-square-matrices.html"><a href="properties-of-square-matrices.html#determinant-for-a-2-times-2-matrix."><i class="fa fa-check"></i><b>13.2.2</b> Determinant for a <span class="math inline">\(2 \times 2\)</span> matrix.</a></li>
<li class="chapter" data-level="13.2.3" data-path="properties-of-square-matrices.html"><a href="properties-of-square-matrices.html#determinant-for-a-3-times-3-matrix."><i class="fa fa-check"></i><b>13.2.3</b> Determinant for a <span class="math inline">\(3 \times 3\)</span> matrix.</a></li>
<li class="chapter" data-level="13.2.4" data-path="properties-of-square-matrices.html"><a href="properties-of-square-matrices.html#using-r-to-find-the-determinant"><i class="fa fa-check"></i><b>13.2.4</b> Using R to Find the Determinant</a></li>
<li class="chapter" data-level="13.2.5" data-path="properties-of-square-matrices.html"><a href="properties-of-square-matrices.html#properties-of-the-determinant"><i class="fa fa-check"></i><b>13.2.5</b> Properties of the Determinant</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="properties-of-square-matrices.html"><a href="properties-of-square-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>13.3</b> Matrix Inverse</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="properties-of-square-matrices.html"><a href="properties-of-square-matrices.html#find-the-inverse-using-r"><i class="fa fa-check"></i><b>13.3.1</b> Find the Inverse using R</a></li>
<li class="chapter" data-level="13.3.2" data-path="properties-of-square-matrices.html"><a href="properties-of-square-matrices.html#properties-of-the-inverse"><i class="fa fa-check"></i><b>13.3.2</b> Properties of the Inverse</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="properties-of-square-matrices.html"><a href="properties-of-square-matrices.html#exercises-6"><i class="fa fa-check"></i><b>13.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>14</b> Eigenvalues and Eigenvectors</a>
<ul>
<li class="chapter" data-level="14.1" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#eigenvalues"><i class="fa fa-check"></i><b>14.1</b> Eigenvalues</a></li>
<li class="chapter" data-level="14.2" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#eigenvectors"><i class="fa fa-check"></i><b>14.2</b> Eigenvectors</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#finding-the-elements-of-v"><i class="fa fa-check"></i><b>14.2.1</b> Finding the Elements of v</a></li>
<li class="chapter" data-level="14.2.2" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#normalized-eigenvectors"><i class="fa fa-check"></i><b>14.2.2</b> Normalized Eigenvectors</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#using-r-to-find-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>14.3</b> Using R to Find Eigenvalues and Eigenvectors</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="statistical-application-sscp-variancecovariance-and-correlation-matrices.html"><a href="statistical-application-sscp-variancecovariance-and-correlation-matrices.html"><i class="fa fa-check"></i><b>15</b> Statistical Application: SSCP, Variance–Covariance, and Correlation Matrices</a>
<ul>
<li class="chapter" data-level="15.1" data-path="statistical-application-sscp-variancecovariance-and-correlation-matrices.html"><a href="statistical-application-sscp-variancecovariance-and-correlation-matrices.html#deviation-scores"><i class="fa fa-check"></i><b>15.1</b> Deviation Scores</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="statistical-application-sscp-variancecovariance-and-correlation-matrices.html"><a href="statistical-application-sscp-variancecovariance-and-correlation-matrices.html#creating-the-mean-matrix"><i class="fa fa-check"></i><b>15.1.1</b> Creating the Mean Matrix</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="statistical-application-sscp-variancecovariance-and-correlation-matrices.html"><a href="statistical-application-sscp-variancecovariance-and-correlation-matrices.html#sscp-matrix"><i class="fa fa-check"></i><b>15.2</b> SSCP Matrix</a></li>
<li class="chapter" data-level="15.3" data-path="statistical-application-sscp-variancecovariance-and-correlation-matrices.html"><a href="statistical-application-sscp-variancecovariance-and-correlation-matrices.html#correlation-matrix"><i class="fa fa-check"></i><b>15.3</b> Correlation Matrix</a></li>
<li class="chapter" data-level="15.4" data-path="statistical-application-sscp-variancecovariance-and-correlation-matrices.html"><a href="statistical-application-sscp-variancecovariance-and-correlation-matrices.html#standardized-scores"><i class="fa fa-check"></i><b>15.4</b> Standardized Scores</a></li>
</ul></li>
<li class="part"><span><b>Matrix Transformations</b></span></li>
<li class="chapter" data-level="16" data-path="basis-vectors-and-matrices.html"><a href="basis-vectors-and-matrices.html"><i class="fa fa-check"></i><b>16</b> Basis Vectors and Matrices</a>
<ul>
<li class="chapter" data-level="16.1" data-path="basis-vectors-and-matrices.html"><a href="basis-vectors-and-matrices.html#an-example"><i class="fa fa-check"></i><b>16.1</b> An Example</a></li>
<li class="chapter" data-level="16.2" data-path="basis-vectors-and-matrices.html"><a href="basis-vectors-and-matrices.html#changing-basis-vectors"><i class="fa fa-check"></i><b>16.2</b> Changing Basis Vectors</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="basis-vectors-and-matrices.html"><a href="basis-vectors-and-matrices.html#non-orthonormal-basis"><i class="fa fa-check"></i><b>16.2.1</b> Non-Orthonormal Basis</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="basis-vectors-and-matrices.html"><a href="basis-vectors-and-matrices.html#converting-non-standard-coordinates-to-the-standard-basis"><i class="fa fa-check"></i><b>16.3</b> Converting Non-Standard Coordinates to the Standard Basis</a></li>
<li class="chapter" data-level="16.4" data-path="basis-vectors-and-matrices.html"><a href="basis-vectors-and-matrices.html#final-thoughts"><i class="fa fa-check"></i><b>16.4</b> Final Thoughts</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="quadratic-form-of-a-matrix.html"><a href="quadratic-form-of-a-matrix.html"><i class="fa fa-check"></i><b>17</b> Quadratic Form of a Matrix</a>
<ul>
<li class="chapter" data-level="17.1" data-path="quadratic-form-of-a-matrix.html"><a href="quadratic-form-of-a-matrix.html#linear-forms"><i class="fa fa-check"></i><b>17.1</b> Linear Forms</a></li>
<li class="chapter" data-level="17.2" data-path="quadratic-form-of-a-matrix.html"><a href="quadratic-form-of-a-matrix.html#bilinear-form"><i class="fa fa-check"></i><b>17.2</b> Bilinear Form</a></li>
<li class="chapter" data-level="17.3" data-path="quadratic-form-of-a-matrix.html"><a href="quadratic-form-of-a-matrix.html#quadratic-form"><i class="fa fa-check"></i><b>17.3</b> Quadratic Form</a></li>
<li class="chapter" data-level="17.4" data-path="quadratic-form-of-a-matrix.html"><a href="quadratic-form-of-a-matrix.html#example-quadratic-form"><i class="fa fa-check"></i><b>17.4</b> Example: Quadratic Form</a></li>
<li class="chapter" data-level="17.5" data-path="quadratic-form-of-a-matrix.html"><a href="quadratic-form-of-a-matrix.html#positive-definite-matrices"><i class="fa fa-check"></i><b>17.5</b> Positive Definite Matrices</a></li>
</ul></li>
<li class="part"><span><b>Systems of Equations</b></span></li>
<li class="chapter" data-level="18" data-path="systems-of-equations.html"><a href="systems-of-equations.html"><i class="fa fa-check"></i><b>18</b> Systems of Equations</a>
<ul>
<li class="chapter" data-level="18.1" data-path="systems-of-equations.html"><a href="systems-of-equations.html#solving-systems-of-equations-with-matrix-algebra"><i class="fa fa-check"></i><b>18.1</b> Solving Systems of Equations with Matrix Algebra</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="systems-of-equations.html"><a href="systems-of-equations.html#solving-this-system-of-equations-using-r"><i class="fa fa-check"></i><b>18.1.1</b> Solving this System of Equations Using R</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="systems-of-equations.html"><a href="systems-of-equations.html#linear-dependence-and-independence"><i class="fa fa-check"></i><b>18.2</b> Linear Dependence and Independence</a></li>
<li class="chapter" data-level="18.3" data-path="systems-of-equations.html"><a href="systems-of-equations.html#singularity"><i class="fa fa-check"></i><b>18.3</b> Singularity</a></li>
<li class="chapter" data-level="18.4" data-path="systems-of-equations.html"><a href="systems-of-equations.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>18.4</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="18.5" data-path="systems-of-equations.html"><a href="systems-of-equations.html#rank-and-the-solution-of-systems-of-equations"><i class="fa fa-check"></i><b>18.5</b> Rank and the Solution of Systems of Equations</a></li>
<li class="chapter" data-level="18.6" data-path="systems-of-equations.html"><a href="systems-of-equations.html#exercises-7"><i class="fa fa-check"></i><b>18.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="statistical-appplication-estimating-regression-coefficients.html"><a href="statistical-appplication-estimating-regression-coefficients.html"><i class="fa fa-check"></i><b>19</b> Statistical Appplication: Estimating Regression Coefficients</a>
<ul>
<li class="chapter" data-level="19.1" data-path="statistical-appplication-estimating-regression-coefficients.html"><a href="statistical-appplication-estimating-regression-coefficients.html#regression-model"><i class="fa fa-check"></i><b>19.1</b> Regression Model</a></li>
<li class="chapter" data-level="19.2" data-path="statistical-appplication-estimating-regression-coefficients.html"><a href="statistical-appplication-estimating-regression-coefficients.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>19.2</b> Estimating the Regression Coefficients</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="statistical-appplication-estimating-regression-coefficients.html"><a href="statistical-appplication-estimating-regression-coefficients.html#example-using-data"><i class="fa fa-check"></i><b>19.2.1</b> Example Using Data</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="statistical-appplication-estimating-regression-coefficients.html"><a href="statistical-appplication-estimating-regression-coefficients.html#the-mathbfxintercalmathbfx-matrix"><i class="fa fa-check"></i><b>19.3</b> The <span class="math inline">\(\mathbf{X}^\intercal\mathbf{X}\)</span> Matrix</a></li>
</ul></li>
<li class="part"><span><b>Matrix Decomposition</b></span></li>
<li class="chapter" data-level="20" data-path="introduction-to-matrix-decompostion.html"><a href="introduction-to-matrix-decompostion.html"><i class="fa fa-check"></i><b>20</b> Introduction to Matrix Decompostion</a></li>
<li class="chapter" data-level="21" data-path="lu-decompostion.html"><a href="lu-decompostion.html"><i class="fa fa-check"></i><b>21</b> LU Decompostion</a>
<ul>
<li class="chapter" data-level="21.1" data-path="lu-decompostion.html"><a href="lu-decompostion.html#an-example-of-lu-decomposition"><i class="fa fa-check"></i><b>21.1</b> An Example of LU Decomposition</a></li>
<li class="chapter" data-level="21.2" data-path="lu-decompostion.html"><a href="lu-decompostion.html#solving-systems-of-equations-with-the-lu-decomposition"><i class="fa fa-check"></i><b>21.2</b> Solving Systems of Equations with the LU Decomposition</a>
<ul>
<li class="chapter" data-level="21.2.1" data-path="lu-decompostion.html"><a href="lu-decompostion.html#step-1-solve-for-z"><i class="fa fa-check"></i><b>21.2.1</b> Step 1: Solve for Z</a></li>
<li class="chapter" data-level="21.2.2" data-path="lu-decompostion.html"><a href="lu-decompostion.html#step-2-solve-for-x"><i class="fa fa-check"></i><b>21.2.2</b> Step 2: Solve for X</a></li>
<li class="chapter" data-level="21.2.3" data-path="lu-decompostion.html"><a href="lu-decompostion.html#using-r-to-solve-the-two-equations"><i class="fa fa-check"></i><b>21.2.3</b> Using R to Solve the Two Equations</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="lu-decompostion.html"><a href="lu-decompostion.html#application-of-lu-decomposition-in-computing"><i class="fa fa-check"></i><b>21.3</b> Application of LU Decomposition in Computing</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="statistical-application-estimating-regression-coefficients-with-lu-decomposition.html"><a href="statistical-application-estimating-regression-coefficients-with-lu-decomposition.html"><i class="fa fa-check"></i><b>22</b> Statistical Application: Estimating Regression Coefficients with LU Decomposition</a>
<ul>
<li class="chapter" data-level="22.0.1" data-path="statistical-application-estimating-regression-coefficients-with-lu-decomposition.html"><a href="statistical-application-estimating-regression-coefficients-with-lu-decomposition.html#estimating-regression-coefficients-using-lu-decomposition"><i class="fa fa-check"></i><b>22.0.1</b> Estimating Regression Coefficients Using LU Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="cholesky-decompostion.html"><a href="cholesky-decompostion.html"><i class="fa fa-check"></i><b>23</b> Cholesky Decompostion</a>
<ul>
<li class="chapter" data-level="23.1" data-path="cholesky-decompostion.html"><a href="cholesky-decompostion.html#example-of-cholesky-decomposition"><i class="fa fa-check"></i><b>23.1</b> Example of Cholesky Decomposition</a></li>
<li class="chapter" data-level="23.2" data-path="cholesky-decompostion.html"><a href="cholesky-decompostion.html#cholesky-decomposition-using-r"><i class="fa fa-check"></i><b>23.2</b> Cholesky Decomposition using R</a></li>
<li class="chapter" data-level="23.3" data-path="cholesky-decompostion.html"><a href="cholesky-decompostion.html#statistical-application-estimating-regression-coefficents-with-cholesky-decomposition"><i class="fa fa-check"></i><b>23.3</b> Statistical Application: Estimating Regression Coefficents with Cholesky Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="qr-decompostion.html"><a href="qr-decompostion.html"><i class="fa fa-check"></i><b>24</b> QR Decompostion</a>
<ul>
<li class="chapter" data-level="24.1" data-path="qr-decompostion.html"><a href="qr-decompostion.html#qr-decomposition-using-r"><i class="fa fa-check"></i><b>24.1</b> QR Decomposition using R</a></li>
<li class="chapter" data-level="24.2" data-path="qr-decompostion.html"><a href="qr-decompostion.html#statistical-application-estimating-regression-coefficents-with-qr-decomposition"><i class="fa fa-check"></i><b>24.2</b> Statistical Application: Estimating Regression Coefficents with QR Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="spectral-decompostion.html"><a href="spectral-decompostion.html"><i class="fa fa-check"></i><b>25</b> Spectral Decompostion</a>
<ul>
<li class="chapter" data-level="25.0.1" data-path="spectral-decompostion.html"><a href="spectral-decompostion.html#an-example-1"><i class="fa fa-check"></i><b>25.0.1</b> An Example</a></li>
<li class="chapter" data-level="25.1" data-path="spectral-decompostion.html"><a href="spectral-decompostion.html#solving-systems-of-equations-with-spectral-decomposition"><i class="fa fa-check"></i><b>25.1</b> Solving Systems of Equations with Spectral Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="singular-value-decompostion.html"><a href="singular-value-decompostion.html"><i class="fa fa-check"></i><b>26</b> Singular Value Decompostion</a>
<ul>
<li class="chapter" data-level="26.1" data-path="singular-value-decompostion.html"><a href="singular-value-decompostion.html#svd-using-r"><i class="fa fa-check"></i><b>26.1</b> SVD Using R</a></li>
</ul></li>
<li class="part"><span><b>Regression Applications</b></span></li>
<li class="chapter" data-level="27" data-path="important-matrices-in-regression.html"><a href="important-matrices-in-regression.html"><i class="fa fa-check"></i><b>27</b> Important Matrices in Regression</a>
<ul>
<li class="chapter" data-level="27.1" data-path="important-matrices-in-regression.html"><a href="important-matrices-in-regression.html#design-matrix"><i class="fa fa-check"></i><b>27.1</b> Design Matrix</a></li>
<li class="chapter" data-level="27.2" data-path="important-matrices-in-regression.html"><a href="important-matrices-in-regression.html#vector-of-fitted-values"><i class="fa fa-check"></i><b>27.2</b> Vector of Fitted Values</a></li>
<li class="chapter" data-level="27.3" data-path="important-matrices-in-regression.html"><a href="important-matrices-in-regression.html#the-h-matrix-hat-matrix"><i class="fa fa-check"></i><b>27.3</b> The H-Matrix (Hat Matrix)</a></li>
<li class="chapter" data-level="27.4" data-path="important-matrices-in-regression.html"><a href="important-matrices-in-regression.html#vector-of-residuals"><i class="fa fa-check"></i><b>27.4</b> Vector of Residuals</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="sums-of-squares-in-regression.html"><a href="sums-of-squares-in-regression.html"><i class="fa fa-check"></i><b>28</b> Sums of Squares in Regression</a>
<ul>
<li class="chapter" data-level="28.1" data-path="sums-of-squares-in-regression.html"><a href="sums-of-squares-in-regression.html#sums-of-squares"><i class="fa fa-check"></i><b>28.1</b> Sums of Squares</a>
<ul>
<li class="chapter" data-level="28.1.1" data-path="sums-of-squares-in-regression.html"><a href="sums-of-squares-in-regression.html#mean-centering-the-outcome"><i class="fa fa-check"></i><b>28.1.1</b> Mean Centering the Outcome</a></li>
</ul></li>
<li class="chapter" data-level="28.2" data-path="sums-of-squares-in-regression.html"><a href="sums-of-squares-in-regression.html#sums-of-squares-as-functions-of-the-data"><i class="fa fa-check"></i><b>28.2</b> Sums of Squares as Functions of the Data</a>
<ul>
<li class="chapter" data-level="28.2.1" data-path="sums-of-squares-in-regression.html"><a href="sums-of-squares-in-regression.html#example-using-r"><i class="fa fa-check"></i><b>28.2.1</b> Example Using R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="29" data-path="standard-errors-and-variance-estimates.html"><a href="standard-errors-and-variance-estimates.html"><i class="fa fa-check"></i><b>29</b> Standard Errors and Variance Estimates</a>
<ul>
<li class="chapter" data-level="29.1" data-path="standard-errors-and-variance-estimates.html"><a href="standard-errors-and-variance-estimates.html#residual-variance-of-the-model"><i class="fa fa-check"></i><b>29.1</b> Residual Variance of the Model</a></li>
<li class="chapter" data-level="29.2" data-path="standard-errors-and-variance-estimates.html"><a href="standard-errors-and-variance-estimates.html#coefficient-variances-and-covariances"><i class="fa fa-check"></i><b>29.2</b> Coefficient Variances and Covariances</a></li>
<li class="chapter" data-level="29.3" data-path="standard-errors-and-variance-estimates.html"><a href="standard-errors-and-variance-estimates.html#standard-errors-for-the-coefficients"><i class="fa fa-check"></i><b>29.3</b> Standard Errors for the Coefficients</a></li>
<li class="chapter" data-level="29.4" data-path="standard-errors-and-variance-estimates.html"><a href="standard-errors-and-variance-estimates.html#correlation-between-the-coefficients"><i class="fa fa-check"></i><b>29.4</b> Correlation Between the Coefficients</a></li>
<li class="chapter" data-level="29.5" data-path="standard-errors-and-variance-estimates.html"><a href="standard-errors-and-variance-estimates.html#inference-model-level"><i class="fa fa-check"></i><b>29.5</b> Inference: Model-Level</a></li>
<li class="chapter" data-level="29.6" data-path="standard-errors-and-variance-estimates.html"><a href="standard-errors-and-variance-estimates.html#inference-coefficient-level"><i class="fa fa-check"></i><b>29.6</b> Inference: Coefficient-Level</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="assumptions-of-the-regression-model.html"><a href="assumptions-of-the-regression-model.html"><i class="fa fa-check"></i><b>30</b> Assumptions of the Regression Model</a>
<ul>
<li class="chapter" data-level="30.1" data-path="assumptions-of-the-regression-model.html"><a href="assumptions-of-the-regression-model.html#regression-assumptions"><i class="fa fa-check"></i><b>30.1</b> Regression Assumptions</a></li>
<li class="chapter" data-level="30.2" data-path="assumptions-of-the-regression-model.html"><a href="assumptions-of-the-regression-model.html#expressing-the-assumptions-uising-matrix-algebra"><i class="fa fa-check"></i><b>30.2</b> Expressing the Assumptions uising Matrix Algebra</a></li>
<li class="chapter" data-level="30.3" data-path="assumptions-of-the-regression-model.html"><a href="assumptions-of-the-regression-model.html#variance-covariance-matrix-of-the-residuals"><i class="fa fa-check"></i><b>30.3</b> Variance-Covariance Matrix of the Residuals</a></li>
<li class="chapter" data-level="30.4" data-path="assumptions-of-the-regression-model.html"><a href="assumptions-of-the-regression-model.html#regression-model-revisited"><i class="fa fa-check"></i><b>30.4</b> Regression Model: Revisited</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Matrix Algebra for Educational Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<link href="style/style-gitbook.css" rel="stylesheet">
<div class="hero-image-container"> 
  <img class= "hero-image" src="figs/hero-image-2.png">
</div>
<div id="statistical-application-vectors" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Statistical Application: Vectors<a href="statistical-application-vectors.html#statistical-application-vectors" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this chapter, we will provide examples of how vectors define the underlying geometry of various statistical summaries (standard deviation and correlation coefficient), including linear models. We will provide an example using a single predictor, but again, the ideas can be extended to models that include multiple predictors.</p>
<div id="deviation-scores-and-the-standard-deviation" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Deviation Scores and the Standard Deviation<a href="statistical-application-vectors.html#deviation-scores-and-the-standard-deviation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider the following vector scores (<strong>x</strong>) and the vector of the mean deviation scores (<span class="math inline">\(\mathbf{d_x}\)</span>):</p>
<p><span class="math display">\[
\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n\end{bmatrix} \qquad \mathbf{d_x} = \begin{bmatrix} x_1-\bar{x} \\ x_2-\bar{x} \\ x_3-\bar{x} \\ \vdots \\ x_n-\bar{x}\end{bmatrix}
\]</span></p>
<p>Remember that the length of a vector is the square root of the dot product of the vector with itself. If we were to compute the length of the deviation score vector, the length would be the square root of the sum of squared deviations:</p>
<p><span class="math display">\[
\sqrt{\mathbf{d_x}\bullet \mathbf{d_x}} = \sqrt{(x_1 - \bar{x})^2 + (x_2 - \bar{x})^2 + (x_3 - \bar{x})^2 + \ldots + (x_n - \bar{x})^2}
\]</span></p>
<p>If we divided this result by <span class="math inline">\(\sqrt{n}\)</span>, this would be equivalent to the standard deviation of the scores in <strong>x</strong> (<span class="math inline">\(s_\mathbf{x}\)</span>).</p>
<p><span class="math display">\[
s_\mathbf{x} = \frac{\lvert\lvert\mathbf{d_x}\rvert\rvert}{\sqrt{n}}
\]</span></p>
<p>So there is a direct relation between the length of a deviation vector and the standard deviation, namely,</p>
<p><span class="math display">\[
\lvert\lvert\mathbf{d_x}\rvert\rvert = \sqrt{n} (s_\mathbf{x})
\]</span></p>
<p>Consider the following vectors of data representing SAT scores (<strong>x</strong>) and GPA values (<strong>y</strong>) for <span class="math inline">\(n=10\)</span> student:</p>
<p><span class="math display">\[
\mathbf{x} = \begin{bmatrix}760 \\ 710 \\ 680 \\ 730 \\ 420 \\ 410 \\ 620 \\ 630 \\ 720 \\ 670\end{bmatrix} \qquad \mathbf{y} = \begin{bmatrix} 3.8 \\ 2.4 \\ 2.6 \\ 3.1 \\ 1.9 \\ 1.7 \\ 2.5 \\ 2.4 \\ 3.5 \\ 3.1\end{bmatrix}
\]</span></p>
<p>We compute the mean scores for each vector as <span class="math inline">\(\bar{x}=635\)</span> and <span class="math inline">\(\bar{y}=2.7\)</span>, respectively. By subtracting the mean (which is a scalar) from each vector of data, we can create deviation vectors:</p>
<p><span class="math display">\[
\mathbf{d_x} = \mathbf{x} - 635 = \begin{bmatrix}125 \\ 75 \\ 45 \\ 95 \\ -215 \\ -225 \\ -15 \\ -5 \\ 85 \\ 35\end{bmatrix} \qquad \mathbf{d_y} =\mathbf{y}-2.7 = \begin{bmatrix}1.1 \\ -0.3 \\ -0.1 \\ 0.4 \\ -0.8 \\ -1.0 \\ -0.2 \\ -0.3 \\ 0.8 \\ 0.4\end{bmatrix}
\]</span>
The length of the deviation vectors are computed as <span class="math inline">\(\sqrt{\mathbf{d}\bullet\mathbf{d}}\)</span>, namely,</p>
<p><span class="math display">\[
\begin{split}
\lvert\lvert\mathbf{d_x}\rvert\rvert &amp;= \sqrt{\mathbf{d_x}\bullet\mathbf{d_x}} \\[2em]
\lvert\lvert\mathbf{d_x}\rvert\rvert &amp;= \sqrt{\mathbf{d_y}\bullet\mathbf{d_y}}
\end{split}
\]</span></p>
<p>And using the values in the deviation vectors,</p>
<p><span class="math display">\[
\begin{split}
\lvert\lvert\mathbf{d_x}\rvert\rvert &amp;= \sqrt{137850} = 371.28 \\[2em]
\lvert\lvert\mathbf{d_x}\rvert\rvert &amp;= \sqrt{4.04} = 2.01
\end{split}
\]</span></p>
<p>Finally, we can compute the standard deviations for <strong>x</strong> and <strong>y</strong> by dividing these lengths by <span class="math inline">\(\sqrt{n}\)</span>.</p>
<p><span class="math display">\[
\begin{split}
s_\mathbf{x} &amp;= \frac{371.28}{\sqrt{10}} = 117.41 \\[2em]
s_\mathbf{y} &amp;= \frac{4.04}{\sqrt{10}} = 0.64
\end{split}
\]</span></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="statistical-application-vectors.html#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Original vectors</span></span>
<span id="cb67-2"><a href="statistical-application-vectors.html#cb67-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="fu">c</span>(<span class="dv">760</span>, <span class="dv">710</span>, <span class="dv">680</span>, <span class="dv">730</span>, <span class="dv">420</span>, <span class="dv">410</span>, <span class="dv">620</span>, <span class="dv">630</span>, <span class="dv">720</span>, <span class="dv">670</span>), <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb67-3"><a href="statistical-application-vectors.html#cb67-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="fu">c</span>(<span class="fl">3.8</span>, <span class="fl">2.4</span>, <span class="fl">2.6</span>, <span class="fl">3.1</span>, <span class="fl">1.9</span>, <span class="fl">1.7</span>, <span class="fl">2.5</span>, <span class="fl">2.4</span>, <span class="fl">3.5</span>, <span class="fl">3.1</span>), <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb67-4"><a href="statistical-application-vectors.html#cb67-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-5"><a href="statistical-application-vectors.html#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute deviation vectors</span></span>
<span id="cb67-6"><a href="statistical-application-vectors.html#cb67-6" aria-hidden="true" tabindex="-1"></a>d_x <span class="ot">=</span> x <span class="sc">-</span> <span class="fu">mean</span>(x)</span>
<span id="cb67-7"><a href="statistical-application-vectors.html#cb67-7" aria-hidden="true" tabindex="-1"></a>d_x</span></code></pre></div>
<pre><code>      [,1]
 [1,]  125
 [2,]   75
 [3,]   45
 [4,]   95
 [5,] -215
 [6,] -225
 [7,]  -15
 [8,]   -5
 [9,]   85
[10,]   35</code></pre>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="statistical-application-vectors.html#cb69-1" aria-hidden="true" tabindex="-1"></a>d_y <span class="ot">=</span> y <span class="sc">-</span> <span class="fu">mean</span>(y)</span>
<span id="cb69-2"><a href="statistical-application-vectors.html#cb69-2" aria-hidden="true" tabindex="-1"></a>d_y</span></code></pre></div>
<pre><code>      [,1]
 [1,]  1.1
 [2,] -0.3
 [3,] -0.1
 [4,]  0.4
 [5,] -0.8
 [6,] -1.0
 [7,] -0.2
 [8,] -0.3
 [9,]  0.8
[10,]  0.4</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="statistical-application-vectors.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute lengths of x deviation vector</span></span>
<span id="cb71-2"><a href="statistical-application-vectors.html#cb71-2" aria-hidden="true" tabindex="-1"></a>l_x <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(d_x <span class="sc">*</span> d_x))</span>
<span id="cb71-3"><a href="statistical-application-vectors.html#cb71-3" aria-hidden="true" tabindex="-1"></a>l_x</span></code></pre></div>
<pre><code>[1] 371.2816</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="statistical-application-vectors.html#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute lengths of y deviation vector</span></span>
<span id="cb73-2"><a href="statistical-application-vectors.html#cb73-2" aria-hidden="true" tabindex="-1"></a>l_y <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(d_y <span class="sc">*</span> d_y))</span>
<span id="cb73-3"><a href="statistical-application-vectors.html#cb73-3" aria-hidden="true" tabindex="-1"></a>l_y</span></code></pre></div>
<pre><code>[1] 2.009975</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="statistical-application-vectors.html#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute sd of x</span></span>
<span id="cb75-2"><a href="statistical-application-vectors.html#cb75-2" aria-hidden="true" tabindex="-1"></a>s_x <span class="ot">=</span> l_x <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">10</span>)</span>
<span id="cb75-3"><a href="statistical-application-vectors.html#cb75-3" aria-hidden="true" tabindex="-1"></a>s_x</span></code></pre></div>
<pre><code>[1] 117.4095</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="statistical-application-vectors.html#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute sd of y</span></span>
<span id="cb77-2"><a href="statistical-application-vectors.html#cb77-2" aria-hidden="true" tabindex="-1"></a>s_y <span class="ot">=</span> l_y <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">10</span>)</span>
<span id="cb77-3"><a href="statistical-application-vectors.html#cb77-3" aria-hidden="true" tabindex="-1"></a>s_y</span></code></pre></div>
<pre><code>[1] 0.6356099</code></pre>
<p>Note that if we are using <span class="math inline">\(s_\mathbf{x}\)</span> as an estimate for the population parameter <span class="math inline">\(\sigma_\mathbf{x}\)</span>, that is <strong>x</strong> is a sample of student scores, then we need to divide the length of vector <strong>x</strong> by <span class="math inline">\(n-1\)</span> rather than <span class="math inline">\(n\)</span>. In that case,</p>
<p><span class="math display">\[
\begin{split}
\hat\sigma_\mathbf{x} &amp;= \frac{371.28}{\sqrt{9}} = 123.76 \\[2em]
\hat\sigma_\mathbf{y} &amp;= \frac{4.04}{\sqrt{9}} = 0.67
\end{split}
\]</span></p>
<p>This is the denominator that is used in the <code>sd()</code> function in R.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="statistical-application-vectors.html#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute sd of x</span></span>
<span id="cb79-2"><a href="statistical-application-vectors.html#cb79-2" aria-hidden="true" tabindex="-1"></a>sigma_x <span class="ot">=</span> l_x <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">9</span>)</span>
<span id="cb79-3"><a href="statistical-application-vectors.html#cb79-3" aria-hidden="true" tabindex="-1"></a>sigma_x</span></code></pre></div>
<pre><code>[1] 123.7605</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="statistical-application-vectors.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute sd of y</span></span>
<span id="cb81-2"><a href="statistical-application-vectors.html#cb81-2" aria-hidden="true" tabindex="-1"></a>sigma_y <span class="ot">=</span> l_y <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">9</span>)</span>
<span id="cb81-3"><a href="statistical-application-vectors.html#cb81-3" aria-hidden="true" tabindex="-1"></a>sigma_y</span></code></pre></div>
<pre><code>[1] 0.6699917</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="statistical-application-vectors.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check results</span></span>
<span id="cb83-2"><a href="statistical-application-vectors.html#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(x)</span></code></pre></div>
<pre><code>[1] 123.7605</code></pre>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="statistical-application-vectors.html#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(y)</span></code></pre></div>
<pre><code>[1] 0.6699917</code></pre>
<p><br /></p>
</div>
<div id="vector-correlation-and-separation" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Vector Correlation and Separation<a href="statistical-application-vectors.html#vector-correlation-and-separation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The correlation between two vectors can also be expressed in terms of deviation scores:</p>
<p><span class="math display">\[
\begin{split}
r_{xy} &amp;= \frac{\mathrm{Cov}_{\mathbf{xy}}}{s_\mathbf{x} s_\mathbf{y}} \\[2em]
&amp;= \frac{\frac{\sum (\mathbf{x}-\bar{x})(\mathbf{y}-\bar{y})}{n}}{s_\mathbf{x} s_\mathbf{y}}
\end{split}
\]</span></p>
<p>The expression <span class="math inline">\(\sum (x-\bar{x})(y-\bar{y})\)</span> is equivalent to <span class="math inline">\(\mathbf{x}\bullet\mathbf{y}\)</span>. Furthermore, we can re-write the standard deviations using our previous relationship with length of the deviation vectors. This implies,</p>
<p><span class="math display">\[
\begin{split}
r_{xy} &amp;= \frac{\frac{\mathbf{d_x}\bullet\mathbf{d_y}}{n}}{\frac{\lvert\lvert\mathbf{d_x}\rvert\rvert}{\sqrt{n}}\frac{\lvert\lvert\mathbf{d_y}\rvert\rvert}{\sqrt{n}}} \\[2em]
&amp;= \frac{\frac{\mathbf{d_x}\bullet\mathbf{d_y}}{n}}{\frac{\lvert\lvert\mathbf{d_x}\rvert\rvert~\lvert\lvert\mathbf{d_y}\rvert\rvert}{n}} \\[2em]
&amp;= \frac{\mathbf{d_x}\bullet\mathbf{d_y}}{\lvert\lvert\mathbf{d_x}\rvert\rvert~\lvert\lvert\mathbf{d_y}\rvert\rvert}
\end{split}
\]</span>
That is, the correlation between <strong>x</strong> and <strong>y</strong> is equal to the ratio between the dot product of the deviation vectors and the product of their lengths.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> In our example,</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="statistical-application-vectors.html#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute correlation</span></span>
<span id="cb87-2"><a href="statistical-application-vectors.html#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(d_x <span class="sc">*</span> d_y) <span class="sc">/</span> (l_x <span class="sc">*</span> l_y)</span></code></pre></div>
<pre><code>[1] 0.8468822</code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="statistical-application-vectors.html#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check with correlation function</span></span>
<span id="cb89-2"><a href="statistical-application-vectors.html#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(x, y)</span></code></pre></div>
<pre><code>          [,1]
[1,] 0.8468822</code></pre>
<p>Recall that the angle between two vectors <strong>a</strong> and <strong>b</strong>, denoted <span class="math inline">\(\theta_{\mathbf{ab}}\)</span>, is given by,</p>
<p><span class="math display">\[
\cos (\theta_{\mathbf{ab}}) = \frac{\mathbf{a}\bullet\mathbf{b}}{\lvert\lvert\mathbf{a}\rvert\rvert\times\lvert\lvert\mathbf{b}\rvert\rvert}
\]</span></p>
<p>Thus to compute the angle between the two deviation vectors (<span class="math inline">\(\theta\)</span>), this is:</p>
<p><span class="math display">\[
\cos (\theta) = \frac{\mathbf{d_x}\bullet\mathbf{d_y}}{\lvert\lvert\mathbf{d_x}\rvert\rvert\times\lvert\lvert\mathbf{d_y}\rvert\rvert}
\]</span></p>
<p>This is the same as the formula for the correlation! In other words,</p>
<p><span class="math display">\[
r_{\mathbf{xy}} = \cos (\theta)
\]</span></p>
<p>The correlation between two vectors <strong>x</strong> and <strong>y</strong> is equal to the cosine of the angle between their deviation vectors. Using this equality, we can find the angle between the deviation vectors for our example SAT scores and GPA values. Since <span class="math inline">\(r_{\mathbf{xy}}=0.847\)</span>, that implies <span class="math inline">\(\theta\approx32^\circ\)</span>.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="statistical-application-vectors.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute theta (in radians)</span></span>
<span id="cb91-2"><a href="statistical-application-vectors.html#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="fu">acos</span>(<span class="fl">0.847</span>)</span></code></pre></div>
<pre><code>[1] 0.5604801</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="statistical-application-vectors.html#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute theta (in degrees)</span></span>
<span id="cb93-2"><a href="statistical-application-vectors.html#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="fu">acos</span>(<span class="fl">0.847</span>) <span class="sc">*</span> <span class="dv">180</span> <span class="sc">/</span> pi</span></code></pre></div>
<pre><code>[1] 32.11314</code></pre>
<p>This correlation corresponds to an angle of approximately <span class="math inline">\(32^\circ\)</span> of separation between the deviation score vectors for SAT scores and GPA values in the 10-dimensional space defined by our 10 students.</p>
<div class="fyi">
<p>We can use the fact that <span class="math inline">\(r_{\mathbf{xy}} = \cos (\theta)\)</span> to make some connections between the value of the correlation coefficient and the angle between the deviation vectors.</p>
<ul>
<li>Perfect positive correlation (<span class="math inline">\(r_{\mathbf{xy}}=1\)</span>) indicates that deviation vectors are collinear. In this case <span class="math inline">\(\cos(\theta)=1\)</span> which implies that <span class="math inline">\(\theta=0^\circ\)</span>.</li>
<li>Perfect negative correlation (<span class="math inline">\(r_{\mathbf{xy}}=-1\)</span>) indicates that deviation vectors are in opposite directions. In this case <span class="math inline">\(\cos(\theta)=-1\)</span> which implies that <span class="math inline">\(\theta=180^\circ\)</span>.</li>
<li>Perfect lack of correlation (<span class="math inline">\(r_{\mathbf{xy}}=0\)</span>) indicates that deviation vectors are orthogonal. In this case <span class="math inline">\(\cos(\theta)=0\)</span> which implies that <span class="math inline">\(\theta=90^\circ\)</span>.</li>
</ul>
</div>
<p><br /></p>
</div>
<div id="orthogonal-decomposition-and-bivariate-regression" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Orthogonal Decomposition and Bivariate Regression<a href="statistical-application-vectors.html#orthogonal-decomposition-and-bivariate-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall that the simple regression model seeks to explain variation in an outcome variable <em>Y</em> using a predictor variable <em>X</em>. Mathematically, the model fitted is expressed as:</p>
<p><span class="math display">\[
\mathbf{y} = b_0 + b_1(\mathbf{x}) + \mathbf{e}
\]</span></p>
<p>where <strong>y</strong> is a vector of fitted values, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are scalars produced from the OLS estimation, <strong>x</strong> is a vector of the predictor values, and <strong>e</strong> is a vector of residuals. Recall that when we have mean centered the outcome and predictor, the intercept (<span class="math inline">\(b_0\)</span>) drops out of this equation. we can express this as:</p>
<p><span class="math display">\[
\mathbf{y} - \bar{y} = b_1(\mathbf{x} - \bar{x}) + \mathbf{e}
\]</span></p>
<p>The outcome and predictors are now expressed as deviation vectors, say</p>
<p><span class="math display">\[
\mathbf{d}_\mathbf{y} = b_1(\mathbf{d}_\mathbf{x} ) + \mathbf{e}
\]</span>
where <span class="math inline">\(\hat{\mathbf{d}}_\mathbf{y}=b_1(\mathbf{d}_\mathbf{x} )\)</span>. That is, we can partition the deviation vector of <strong>y</strong> into two components:</p>
<p><span class="math display">\[
\mathbf{d}_\mathbf{y} = \hat{\mathbf{d}}_\mathbf{y} + \mathbf{e}
\]</span></p>
<p>Ordinary least squares (OLS) estimation determines the value of <span class="math inline">\(b_1\)</span> by minimizing the sum of squared residuals, that is, it minimizes the quantity <span class="math inline">\(\lvert\lvert\mathbf{e}\rvert\rvert^2\)</span>. Geometrically, minimizing <span class="math inline">\(\lvert\lvert\mathbf{e}\rvert\rvert^2\)</span> is determining the orthogonal projection of <span class="math inline">\(\mathbf{d}_\mathbf{y}\)</span> onto <span class="math inline">\(\mathbf{d}_\mathbf{x}\)</span>.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> This projection is the first component of the partitioning of the deviation vector described previously, <span class="math inline">\(\hat{\mathbf{d}}_\mathbf{y}=b_1(\mathbf{d}_\mathbf{x})\)</span>, which is collinear with <span class="math inline">\(\mathbf{d}_\mathbf{x}\)</span> since <span class="math inline">\(b_1(\mathbf{d}_\mathbf{x})\)</span> is a scalar multiple of <span class="math inline">\(\mathbf{d}_\mathbf{x}\)</span> This is shown in Figure <a href="statistical-application-vectors.html#fig:fig06-01">6.1</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fig06-01"></span>
<img src="figs/fig-06-01.png" alt="The two orthogonal projections from the deviation vector of **y** form the basis for the model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the **e** vector." width="60%" />
<p class="caption">
Figure 6.1: The two orthogonal projections from the deviation vector of <strong>y</strong> form the basis for the model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the <strong>e</strong> vector.
</p>
</div>
<p>The ‘triangle’ formed by the vectors <span class="math inline">\(\hat{\mathbf{d}}_\mathbf{y}\)</span>, <strong>e</strong>, and <span class="math inline">\(\mathbf{d}_\mathbf{y}\)</span> is referred to as the <em>model triangle</em>. Figure <a href="statistical-application-vectors.html#fig:model-triangle">6.2</a> shows the model triangle. (Remember <strong>e</strong> can be moved to the right-side of the triangle since location is not a vector property.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:model-triangle"></span>
<img src="figs/fig-06-model-triangle.png" alt="The model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the **e** vector." width="60%" />
<p class="caption">
Figure 6.2: The model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the <strong>e</strong> vector.
</p>
</div>
<p>The geometry of this triangle is the same as the geometry visualizing the sum of these vectors, namely</p>
<p><span class="math display">\[
\mathbf{d}_\mathbf{y} = \hat{\mathbf{d}}_\mathbf{y} + \mathbf{e}
\]</span></p>
<p>Namely that the vectors that create the legs of the model triangle can be added together to create the <span class="math inline">\(\mathbf{d}_\mathbf{y}\)</span> hypotenuse vector. The <strong>e</strong> vector is also the second orthogonal projection vector, <span class="math inline">\(\mathbf{e} = \mathbf{p}_{\mathbf{d}_\mathbf{y}\perp\mathbf{o}}\)</span>. By definition, this means that the vector of residuals (<strong>e</strong>) is orthogonal to the vector of fitted values (<span class="math inline">\(\hat{\mathbf{d}}_\mathbf{y}\)</span>), which means, the correlation between those two vectors is zero.</p>
<p><span class="math display">\[
r_{\mathbf{e},\hat{\mathbf{d}}_\mathbf{y}} = 0
\]</span></p>
<p>It also means that the model triangle is a right triangle, whose side lengths are governed by the Pythagorean Theorem.</p>
<p><span class="math display">\[
\lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert^2 = \lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert^2 + \lvert\lvert\mathbf{e}\rvert\rvert^2
\]</span></p>
<p>Expressing these lengths using the deviations we get</p>
<p><span class="math display">\[
\sum_{i=1}^n (y_i - \bar{y})^2 = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^n (y_i - \hat{y}_i)^2
\]</span></p>
<p>This is the partitioning that describes the ANOVA decomposition:</p>
<p><span class="math display">\[
\mathrm{Total~SS} = \mathrm{Model~SS} + \mathrm{Residual~SS}
\]</span></p>
<p>Each of these sum of squares is the squared length of one of the vectors in the model triangle.</p>
<p><span class="math display">\[
\begin{split}
\mathrm{Total~SS} &amp;= \lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert^2 \\[2em]
\mathrm{Model~SS} &amp;= \lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert^2 \\[2em]
\mathrm{Residual~SS} &amp;= \lvert\lvert\mathbf{e}\rvert\rvert^2
\end{split}
\]</span></p>
<p>Relatedly, the lengths of the vectors making up the model triangle are the square roots of the these sum of square terms:</p>
<p><span class="math display">\[
\begin{split}
\lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert &amp;= \sqrt{\mathrm{Total~SS}} \\[2em]
\lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert &amp;= \sqrt{\mathrm{Model~SS}} \\[2em]
\lvert\lvert\mathbf{e}\rvert\rvert &amp;= \sqrt{\mathrm{Residual~SS}}
\end{split}
\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-25"></span>
<img src="figs/fig-06-02.png" alt="The side lengths of the model triangle (yellow) correspond to the square roots of the sum of square terms used in the ANOVA decomposition." width="60%" />
<p class="caption">
Figure 6.3: The side lengths of the model triangle (yellow) correspond to the square roots of the sum of square terms used in the ANOVA decomposition.
</p>
</div>
<p>Lastly, since the model-level <span class="math inline">\(R^2\)</span> value is defined as <span class="math inline">\(R^2 = \frac{\mathrm{Model~SS}}{\mathrm{Total~SS}}\)</span> and there is only a single predictor in the model then,</p>
<p><span class="math display">\[
\begin{split}
r_{\mathbf{xy}} &amp;= \sqrt{R^2} \\[2em]
&amp;= \sqrt{\frac{\mathrm{Model~SS}}{\mathrm{Total~SS}}} \\[2em]
&amp;= \frac{\sqrt{\mathrm{Model~SS}}}{\sqrt{\mathrm{Total~SS}}} \\[2em]
&amp;= \frac{\lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert}{\lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert}
\end{split}
\]</span></p>
<p>That is, the correlation coefficient between <strong>x</strong> and <strong>y</strong> is equivalent to the ratio of the lengths between the orthogonal projection vector collinear with the deviation vector of <strong>x</strong> and the deviation vector of <strong>y</strong>. Note this is also exactly how we compute cosine of <span class="math inline">\(\theta\)</span>.</p>
<p><br /></p>
<div id="back-to-the-sat-and-gpa-example" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Back to the SAT and GPA Example<a href="statistical-application-vectors.html#back-to-the-sat-and-gpa-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here we return to our GPA and SAT data to provide an example of these computations. Suppose we want to predict SAT (<strong>y</strong>) from GPA (<strong>x</strong>), employing deviation scores. We begin by computing the length of both deviation vectors:</p>
<p><span class="math display">\[
\begin{split}
\lvert\lvert\mathbf{d}_\mathbf{x}\rvert\rvert &amp;= \sqrt{137850} = 371.28\\[2em]
\lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert &amp;= \sqrt{4.04} = 2.01
\end{split}
\]</span></p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="statistical-application-vectors.html#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute length of deviation vector d_x</span></span>
<span id="cb95-2"><a href="statistical-application-vectors.html#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(l_x <span class="sc">*</span> l_x))</span></code></pre></div>
<pre><code>[1] 371.2816</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="statistical-application-vectors.html#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute length of deviation vector d_y</span></span>
<span id="cb97-2"><a href="statistical-application-vectors.html#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(l_y <span class="sc">*</span> l_y))</span></code></pre></div>
<pre><code>[1] 2.009975</code></pre>
<p>We can also compute the correlation coefficient between SAT scores and GPAs by finding the cosine of the angle between the two vectors, <span class="math inline">\(r = 0.847\)</span>. Taking the arc-cosine, we find that <span class="math inline">\(\theta = 32.12^\circ\)</span>.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="statistical-application-vectors.html#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute correlation</span></span>
<span id="cb99-2"><a href="statistical-application-vectors.html#cb99-2" aria-hidden="true" tabindex="-1"></a>r <span class="ot">=</span> <span class="fu">sum</span>(d_x <span class="sc">*</span> d_y) <span class="sc">/</span> (l_x <span class="sc">*</span> l_y)</span>
<span id="cb99-3"><a href="statistical-application-vectors.html#cb99-3" aria-hidden="true" tabindex="-1"></a>r</span></code></pre></div>
<pre><code>[1] 0.8468822</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-28"></span>
<img src="figs/fig-06-02.png" alt="The model triangle (yellow) for our regression of GPA (**y**) onto SAT scores (**x**)." width="60%" />
<p class="caption">
Figure 6.4: The model triangle (yellow) for our regression of GPA (<strong>y</strong>) onto SAT scores (<strong>x</strong>).
</p>
</div>
<p>We can now use the definition of cosine and sine to compute the lengths of the two orthogonal projections from <span class="math inline">\(\mathbf{d}_\mathbf{y}\)</span>. The length of the projection onto <span class="math inline">\(\mathbf{d}_\mathbf{x}\)</span> is calculated as:</p>
<p><span class="math display">\[
\begin{split}
\cos(\theta) &amp;= \frac{\lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert}{\lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert} \\[2em]
0.847 &amp;= \frac{\lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert}{2.01} \\[2em]
\lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert &amp;= 1.70
\end{split}
\]</span></p>
<p>The length of the second projection onto <strong>o</strong> is calculated as:</p>
<p><span class="math display">\[
\begin{split}
\sin(\theta) &amp;= \frac{\lvert\lvert\mathbf{e}\rvert\rvert}{\lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert} \\[2em]
0.531 &amp;= \frac{\lvert\lvert\mathbf{e}\rvert\rvert}{2.01} \\[2em]
\lvert\lvert\mathbf{e}\rvert\rvert &amp;= 1.07
\end{split}
\]</span></p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="statistical-application-vectors.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute length of projection on d_x</span></span>
<span id="cb101-2"><a href="statistical-application-vectors.html#cb101-2" aria-hidden="true" tabindex="-1"></a>r <span class="sc">*</span> l_y</span></code></pre></div>
<pre><code>[1] 1.702212</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="statistical-application-vectors.html#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute length of projection on o</span></span>
<span id="cb103-2"><a href="statistical-application-vectors.html#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sin</span>(<span class="fu">acos</span>(r)) <span class="sc">*</span> l_y</span></code></pre></div>
<pre><code>[1] 1.068866</code></pre>
<p>Figure <a href="statistical-application-vectors.html#fig:fig06-04">6.5</a> shows the model triangle for the regression of GPA (<strong>y</strong>) onto SAT scores (<strong>x</strong>) with all of the computed side lengths.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fig06-04"></span>
<img src="figs/fig-06-04.png" alt="The model triangle (yellow) for our regression of GPA (**y**) onto SAT scores (**x**)." width="60%" />
<p class="caption">
Figure 6.5: The model triangle (yellow) for our regression of GPA (<strong>y</strong>) onto SAT scores (<strong>x</strong>).
</p>
</div>
<p>Finally, we can use these lengths, along with the length of <span class="math inline">\(\mathbf{d}_\mathbf{y}\)</span> to write out the ANOVA decomposition; the partitioning of the sum of squares.</p>
<p><span class="math display">\[
\begin{split}
\mathrm{Total~SS} &amp;= \lvert\lvert\mathbf{d}_\mathbf{y}\rvert\rvert^2 = 2.01^2 = 4.04\\[2em]
\mathrm{Model~SS} &amp;= \lvert\lvert\hat{\mathbf{d}}_\mathbf{y}\rvert\rvert^2 = 1.70^2 = 2.90\\[2em]
\mathrm{Residual~SS} &amp;= \lvert\lvert\mathbf{e}\rvert\rvert^2 = 1.07^2 = 1.14
\end{split}
\]</span></p>
<p>These sums of squares are additive within rounding:</p>
<p><span class="math display">\[
\begin{split}
\mathrm{Total~SS} &amp;= \mathrm{Model~SS} + \mathrm{Residual~SS} \\[2em]
4.04 &amp;= 2.90 + 1.14
\end{split}
\]</span></p>
<p>Using these values we can also compute the model-level <span class="math inline">\(R^2\)</span>.</p>
<p><span class="math display">\[
R^2 = \frac{\mathrm{Model~SS}}{\mathrm{Total~SS}} =  \frac{2.90}{4.04} = 0.718
\]</span></p>
<p>That is, differences in SAT scores explain 71.8% of the variation in GPAs.</p>
<p><br /></p>

</div>
</div>
</div>



<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>Most formulas for the correlation will use <span class="math inline">\(n-1\)</span> rather than <span class="math inline">\(n\)</span>, but here it doesn’t matter as the <span class="math inline">\(n\)</span>s drop out when we reduce this.<a href="statistical-application-vectors.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>Remember from geometry that the shortest distance from a point (at the end of the <span class="math inline">\(\mathbf{d}_\mathbf{y}\)</span>) to a line (spanned by <span class="math inline">\(\mathbf{d}_\mathbf{x}\)</span>) is the perpendicular line segment between them.<a href="statistical-application-vectors.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="vector-geometry-angles-projection-and-decomposition.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="matrices-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
